{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "import pandas as pd\n",
    "import numpy as np\n",
    "import sys\n",
    "import torch\n",
    "import torch.nn as nn\n",
    "import torch.nn.functional as F\n",
    "\n",
    "sys.path.append('../../../')\n",
    "\n",
    "from configs.data_configs.rosbank import data_configs\n",
    "from configs.model_configs.gen.rosbank import model_configs\n",
    "from src.data_load.dataloader import create_data_loaders, create_test_loader\n",
    "\n",
    "from src.models.TimeGan import TG\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "data_conf = data_configs()\n",
    "model_conf = model_configs()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Data shapes: train 8467, val 946, test 0\n"
     ]
    }
   ],
   "source": [
    "train_loader, val_loader = create_data_loaders(data_conf, supervised=False)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [],
   "source": [
    "for batch in train_loader:\n",
    "    break"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [],
   "source": [
    "tg = TG(model_conf=model_conf, data_conf=data_conf)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "tensor(420.7841, grad_fn=<AddBackward0>)\n"
     ]
    }
   ],
   "source": [
    "mse = tg.train_discriminator(batch[0])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "metadata": {},
   "outputs": [],
   "source": [
    "out = tg.generate(batch[0])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [],
   "source": [
    "from src.models.TimeGan import random_generator\n",
    "\n",
    "x, time_steps = tg.processor(batch[0])\n",
    "bs, l, d = x.size()\n",
    "Z = random_generator(bs, d, [l]*bs, l)\n",
    "gen_latens = tg.supervisor(tg.generator(Z))\n",
    "y_fake = tg.discriminator(gen_latens)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [
    {
     "ename": "NameError",
     "evalue": "name 'y_fake' is not defined",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mNameError\u001b[0m                                 Traceback (most recent call last)",
      "\u001b[1;32m/home/event_seq/experiments/rosbank/notebooks/timegan.ipynb Cell 8\u001b[0m line \u001b[0;36m1\n\u001b[0;32m----> <a href='vscode-notebook-cell://attached-container%2B7b22636f6e7461696e65724e616d65223a222f76765f6576656e74227d@ssh-remote%2Bmidas.skoltech.ru/home/event_seq/experiments/rosbank/notebooks/timegan.ipynb#X20sdnNjb2RlLXJlbW90ZQ%3D%3D?line=0'>1</a>\u001b[0m F\u001b[39m.\u001b[39mcross_entropy(y_fake\u001b[39m.\u001b[39mpermute(\u001b[39m0\u001b[39m,\u001b[39m2\u001b[39m,\u001b[39m1\u001b[39m), torch\u001b[39m.\u001b[39mones_like(y_fake)\u001b[39m.\u001b[39msqueeze(\u001b[39m-\u001b[39m\u001b[39m1\u001b[39m)\u001b[39m.\u001b[39mlo, reduction\u001b[39m=\u001b[39m\u001b[39m'\u001b[39m\u001b[39mnone\u001b[39m\u001b[39m'\u001b[39m)\u001b[39m.\u001b[39msize()\n",
      "\u001b[0;31mNameError\u001b[0m: name 'y_fake' is not defined"
     ]
    }
   ],
   "source": [
    "F.cross_entropy(y_fake.permute(0,2,1), torch.ones_like(y_fake).squeeze(-1).lo, reduction='none').size()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "(torch.Size([128, 2, 200]), torch.Size([128, 200]))"
      ]
     },
     "execution_count": 11,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "y_fake.permute(0,2,1).size(), torch.ones_like(y_fake)[:,:,0].size()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 22,
   "metadata": {},
   "outputs": [],
   "source": [
    "opt = torch.optim.Adam(\n",
    "            list(tg.encoder.parameters()) + list(tg.decoder.parameters()), model_conf.lr, weight_decay=model_conf.weight_decay\n",
    "        )"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "import torch.nn.init as init\n",
    "\n",
    "\n",
    "def random_generator(batch_size, z_dim, T_mb, max_seq_len):\n",
    "  \"\"\"Random vector generation.\n",
    "  \n",
    "  Args:\n",
    "    - batch_size: size of the random vector\n",
    "    - z_dim: dimension of random vector\n",
    "    - T_mb: time information for the random vector\n",
    "    - max_seq_len: maximum sequence length\n",
    "    \n",
    "  Returns:\n",
    "    - Z_mb: generated random vector\n",
    "  \"\"\"\n",
    "  Z_mb = list()\n",
    "  for i in range(batch_size):\n",
    "    temp = np.zeros([max_seq_len, z_dim])\n",
    "    temp_Z = np.random.uniform(0., 1, [T_mb[i], z_dim])\n",
    "    temp[:T_mb[i],:] = temp_Z\n",
    "    Z_mb.append(temp)\n",
    "  return torch.tensor(np.stack(Z_mb)).float()\n",
    "\n",
    "def _weights_init(m):\n",
    "    classname = m.__class__.__name__\n",
    "    if isinstance(m, nn.Linear):\n",
    "        init.xavier_uniform_(m.weight)\n",
    "        m.bias.data.fill_(0)\n",
    "    elif classname.find('Conv') != -1:\n",
    "        m.weight.data.normal_(0.0, 0.02)\n",
    "    elif classname.find('Norm') != -1:\n",
    "        m.weight.data.normal_(1.0, 0.02)\n",
    "        m.bias.data.fill_(0)\n",
    "    elif classname.find(\"GRU\") != -1:\n",
    "      for name,param in m.named_parameters():\n",
    "        if 'weight_ih' in name:\n",
    "          init.xavier_uniform_(param.data)\n",
    "        elif 'weight_hh' in name:\n",
    "          init.orthogonal_(param.data)\n",
    "        elif 'bias' in name:\n",
    "          param.data.fill_(0)\n",
    "\n",
    "class Encoder(nn.Module):\n",
    "    \"\"\"Embedding network between original feature space to latent space.\n",
    "\n",
    "        Args:\n",
    "          - input: input time-series features. (L, N, X) = (24, ?, 6)\n",
    "          - h3: (num_layers, N, H). [3, ?, 24]\n",
    "\n",
    "        Returns:\n",
    "          - H: embeddings\n",
    "    \"\"\"\n",
    "    def __init__(self, input_size, hidden_rnn, num_layers):\n",
    "        super(Encoder, self).__init__()\n",
    "        self.rnn = nn.GRU(input_size=input_size, hidden_size=hidden_rnn, num_layers=num_layers, batch_first=True)\n",
    "        self.fc = nn.Linear(hidden_rnn, hidden_rnn)\n",
    "        self.sigmoid = nn.Sigmoid()\n",
    "        # в оригинале стремная инициализация весов\n",
    "        self.apply(_weights_init)\n",
    "\n",
    "    def forward(self, x, sigmoid=True):\n",
    "        e_outputs, _ = self.rnn(x)\n",
    "        H = self.fc(e_outputs)\n",
    "        if sigmoid:\n",
    "            H = self.sigmoid(H)\n",
    "        return H\n",
    "\n",
    "class Recovery(nn.Module):\n",
    "    \"\"\"Recovery network from latent space to original space.\n",
    "\n",
    "    Args:\n",
    "      - H: latent representation\n",
    "      - T: input time information\n",
    "\n",
    "    Returns:\n",
    "      - X_tilde: recovered data\n",
    "    \"\"\"\n",
    "    def __init__(self, input_size, hidden_rnn, num_layers):\n",
    "        super(Recovery, self).__init__()\n",
    "        self.rnn = nn.GRU(input_size=hidden_rnn, hidden_size=input_size, num_layers=num_layers, batch_first=True)\n",
    "        \n",
    "        self.fc = nn.Linear(input_size, input_size)\n",
    "        self.sigmoid = nn.Sigmoid()\n",
    "        self.apply(_weights_init)\n",
    "\n",
    "    def forward(self, input, sigmoid=True):\n",
    "        r_outputs, _ = self.rnn(input)\n",
    "        X_tilde = self.fc(r_outputs)\n",
    "        if sigmoid:\n",
    "            X_tilde = self.sigmoid(X_tilde)\n",
    "        return X_tilde\n",
    "\n",
    "class Generator(nn.Module):\n",
    "    \"\"\"Generator function: Generate time-series data in latent space.\n",
    "\n",
    "    Args:\n",
    "      - Z: random variables\n",
    "      - T: input time information\n",
    "\n",
    "    Returns:\n",
    "      - E: generated embedding\n",
    "    \"\"\"\n",
    "    def __init__(self, input_size, hidden_rnn, num_layers):\n",
    "        super(Generator, self).__init__()\n",
    "        self.rnn = nn.GRU(input_size=input_size, hidden_size=hidden_rnn, num_layers=num_layers, batch_first=True)\n",
    "        self.fc = nn.Linear(hidden_rnn, hidden_rnn)\n",
    "        self.sigmoid = nn.Sigmoid()\n",
    "        self.apply(_weights_init)\n",
    "\n",
    "    def forward(self, input, sigmoid=True):\n",
    "        g_outputs, _ = self.rnn(input)\n",
    "        E = self.fc(g_outputs)\n",
    "        if sigmoid:\n",
    "            E = self.sigmoid(E)\n",
    "        return E\n",
    "\n",
    "\n",
    "class Supervisor(nn.Module):\n",
    "    \"\"\"Generate next sequence using the previous sequence.\n",
    "\n",
    "    Args:\n",
    "      - H: latent representation\n",
    "      - T: input time information\n",
    "\n",
    "    Returns:\n",
    "      - S: generated sequence based on the latent representations generated by the generator\n",
    "    \"\"\"\n",
    "    def __init__(self, hidden_rnn, num_layers):\n",
    "        super(Supervisor, self).__init__()\n",
    "        self.rnn = nn.GRU(input_size=hidden_rnn, hidden_size=hidden_rnn, num_layers=num_layers, batch_first=True)\n",
    "        self.fc = nn.Linear(hidden_rnn, hidden_rnn)\n",
    "        self.sigmoid = nn.Sigmoid()\n",
    "        self.apply(_weights_init)\n",
    "\n",
    "    def forward(self, input, sigmoid=True):\n",
    "        s_outputs, _ = self.rnn(input)\n",
    "        S = self.fc(s_outputs)\n",
    "        if sigmoid:\n",
    "            S = self.sigmoid(S)\n",
    "        return S\n",
    "\n",
    "\n",
    "class Discriminator(nn.Module):\n",
    "    \"\"\"Discriminate the original and synthetic time-series data.\n",
    "\n",
    "    Args:\n",
    "      - H: latent representation\n",
    "      - T: input time information\n",
    "\n",
    "    Returns:\n",
    "      - Y_hat: classification results between original and synthetic time-series\n",
    "    \"\"\"\n",
    "    def __init__(self, hidden_rnn, num_layers):\n",
    "        super(Discriminator, self).__init__()\n",
    "        self.rnn = nn.GRU(input_size=hidden_rnn, hidden_size=hidden_rnn, num_layers=num_layers, batch_first=True)\n",
    "        self.fc = nn.Linear(hidden_rnn, 1)\n",
    "        self.apply(_weights_init)\n",
    "\n",
    "    def forward(self, input):\n",
    "        d_outputs, _ = self.rnn(input)\n",
    "        Y_hat = self.fc(d_outputs)\n",
    "        return Y_hat\n",
    "\n",
    "\n",
    "class TG(nn.Module):\n",
    "\n",
    "    def __init__(self, input_size, hidden_rnn, num_layers):\n",
    "        super().__init__()\n",
    "        self.encoder = Encoder(input_size=input_size, hidden_rnn=hidden_rnn, num_layers=num_layers)\n",
    "        self.decoder = Recovery(input_size=input_size, hidden_rnn=hidden_rnn, num_layers=num_layers)\n",
    "        self.supervisor = Supervisor(hidden_rnn=hidden_rnn, num_layers=num_layers)\n",
    "        self.generator = Generator(input_size=input_size, hidden_rnn=hidden_rnn, num_layers=num_layers)\n",
    "        self.discriminator = Discriminator(hidden_rnn=hidden_rnn, num_layers=num_layers)\n",
    "\n",
    "        self.gamma = 1\n",
    "    def train_embedder(self, x):\n",
    "        latens = self.encoder(x)\n",
    "        decoded = self.decoder(latens)\n",
    "        global_hidden = latens[:, -1, :]\n",
    "        mse = F.mse_loss(decoded, x, reduction='none').sum(dim=[1,2]).mean()\n",
    "        \n",
    "        return global_hidden, mse\n",
    "\n",
    "    def train_generator(self, x):\n",
    "        bs, l, d = x.size()\n",
    "        Z = random_generator(bs, d, [l]*bs, l)\n",
    "        gen_E = self.generator(Z)\n",
    "        \n",
    "        gen_latens = self.supervisor(gen_E)\n",
    "        latens = self.encoder(x)\n",
    "\n",
    "        mse = F.mse_loss(latens, gen_latens, reduction='none').sum(dim=[1,2]).mean()\n",
    "        return mse\n",
    "\n",
    "    def train_joint(self, x):\n",
    "        bs, l, d = x.size()\n",
    "        Z = random_generator(bs, d, [l]*bs, l)\n",
    "        gen_latens = self.supervisor(self.generator(Z))\n",
    "        latens = self.encoder(x)\n",
    "        gen_decoded = self.decoder(gen_latens)\n",
    "\n",
    "        y_fake = self.discriminator(gen_latens)\n",
    "        g_loss_u = F.cross_entropy(torch.ones_like(y_fake), y_fake)\n",
    "        g_loss_s = F.mse_loss(latens, gen_latens, reduction='none').sum(dim=[1,2]).mean()\n",
    "\n",
    "        var_loss = torch.abs(torch.sqrt(torch.var(gen_decoded, dim=0) + 1e-6) - torch.sqrt(torch.var(x, dim=0) + 1e-6)).mean()\n",
    "        mean_loss = torch.abs(torch.mean(gen_decoded, dim=0) - torch.mean(x, dim=0)).mean()\n",
    "        g_loss_v = var_loss + mean_loss\n",
    "\n",
    "        e_loss = F.mse_loss(latens, gen_latens, reduction='none').sum(dim=[1,2]).mean() \n",
    "        e_loss = 10 * torch.sqrt(e_loss)\n",
    "        e_loss = e_loss + 0.1 * g_loss_s\n",
    "\n",
    "        return g_loss_u, g_loss_s, g_loss_v, e_loss\n",
    "\n",
    "    def train_discriminator(self, x):\n",
    "        bs, l, d = x.size()\n",
    "        Z = random_generator(bs, d, [l]*bs, l)\n",
    "        e_gen = self.generator(Z)\n",
    "        gen_latens = self.supervisor(e_gen)\n",
    "        latens = self.encoder(x)\n",
    "\n",
    "        y_fake = self.discriminator(gen_latens)\n",
    "        y_fake_e = self.discriminator(e_gen)\n",
    "        y_real = self.discriminator(latens)\n",
    "        D_loss_fake = F.cross_entropy(torch.zeros_like(y_fake), y_fake)\n",
    "        D_loss_real = F.cross_entropy(torch.ones_like(y_fake), y_real)\n",
    "        D_loss_fake_e = F.cross_entropy(torch.zeros_like(y_fake_e), y_fake_e)\n",
    "        D_loss = D_loss_real + D_loss_fake + self.gamma * D_loss_fake_e\n",
    "\n",
    "        print(D_loss)\n",
    "        if D_loss.item() <= 0.15:\n",
    "          D_loss = torch.tensor(0)\n",
    "        \n",
    "        return D_loss\n",
    "    \n",
    "    def reconstruct(self, x):\n",
    "        latens = self.encoder(x)\n",
    "        decoded = self.decoder(latens)\n",
    "        return decoded\n",
    "    \n",
    "    def generate(self, x):\n",
    "        bs, l, d = x.size()\n",
    "        Z = random_generator(bs, d, [l]*bs, l)\n",
    "        gen_latens = self.supervisor(self.generator(Z))\n",
    "        gen_decoded = self.decoder(gen_latens)\n",
    "        return gen_decoded\n",
    "    \n",
    "        #train onyl when > 0.15\n",
    "  #       ###  # Discriminator loss\n",
    "  # D_loss_real = tf.losses.sigmoid_cross_entropy(tf.ones_like(Y_real), Y_real)\n",
    "  # D_loss_fake = tf.losses.sigmoid_cross_entropy(tf.zeros_like(Y_fake), Y_fake)\n",
    "  # D_loss_fake_e = tf.losses.sigmoid_cross_entropy(tf.zeros_like(Y_fake_e), Y_fake_e)\n",
    "  # D_loss = D_loss_real + D_loss_fake + gamma * D_loss_fake_e"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [],
   "source": [
    "x = torch.rand(3, 5, 7) \n",
    "TG = TG(input_size=7, hidden_rnn=11, num_layers=2)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "tensor(-0.2320, grad_fn=<AddBackward0>)\n"
     ]
    }
   ],
   "source": [
    "out = TG.train_discriminator(x)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 21,
   "metadata": {},
   "outputs": [
    {
     "ename": "AttributeError",
     "evalue": "type object 'TG' has no attribute 'encoder'",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mAttributeError\u001b[0m                            Traceback (most recent call last)",
      "\u001b[1;32m/home/event_seq/experiments/rosbank/notebooks/timegan.ipynb Cell 15\u001b[0m line \u001b[0;36m2\n\u001b[1;32m      <a href='vscode-notebook-cell://attached-container%2B7b22636f6e7461696e65724e616d65223a222f76765f6576656e74227d@ssh-remote%2Bmidas.skoltech.ru/home/event_seq/experiments/rosbank/notebooks/timegan.ipynb#W4sdnNjb2RlLXJlbW90ZQ%3D%3D?line=0'>1</a>\u001b[0m opt \u001b[39m=\u001b[39m torch\u001b[39m.\u001b[39moptim\u001b[39m.\u001b[39mAdam(\n\u001b[0;32m----> <a href='vscode-notebook-cell://attached-container%2B7b22636f6e7461696e65724e616d65223a222f76765f6576656e74227d@ssh-remote%2Bmidas.skoltech.ru/home/event_seq/experiments/rosbank/notebooks/timegan.ipynb#W4sdnNjb2RlLXJlbW90ZQ%3D%3D?line=1'>2</a>\u001b[0m             \u001b[39mlist\u001b[39m(TG\u001b[39m.\u001b[39;49mencoder\u001b[39m.\u001b[39mparameters()) \u001b[39m+\u001b[39m \u001b[39mlist\u001b[39m(TG\u001b[39m.\u001b[39mdecoder\u001b[39m.\u001b[39mparameters()), model_conf\u001b[39m.\u001b[39mlr, weight_decay\u001b[39m=\u001b[39mmodel_conf\u001b[39m.\u001b[39mweight_decay\n\u001b[1;32m      <a href='vscode-notebook-cell://attached-container%2B7b22636f6e7461696e65724e616d65223a222f76765f6576656e74227d@ssh-remote%2Bmidas.skoltech.ru/home/event_seq/experiments/rosbank/notebooks/timegan.ipynb#W4sdnNjb2RlLXJlbW90ZQ%3D%3D?line=2'>3</a>\u001b[0m         )\n",
      "\u001b[0;31mAttributeError\u001b[0m: type object 'TG' has no attribute 'encoder'"
     ]
    }
   ],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": 66,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "(torch.Size([3, 5, 7]), torch.Size([3, 5, 7]))"
      ]
     },
     "execution_count": 66,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "d_scores, gen_latens, decoded, x1 = net(x)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 72,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "tensor(14.1805, grad_fn=<MeanBackward0>)"
      ]
     },
     "execution_count": 72,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "F.mse_loss(x1, decoded, reduction='none').sum(dim=[1,2]).mean()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.10.12"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
