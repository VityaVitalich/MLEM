{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "import pandas as pd\n",
    "import numpy as np\n",
    "import sys\n",
    "import torch\n",
    "import torch.nn as nn\n",
    "import torch.nn.functional as F\n",
    "\n",
    "sys.path.append('../../../')\n",
    "\n",
    "from configs.data_configs.rosbank import data_configs\n",
    "from configs.model_configs.gen.rosbank import model_configs\n",
    "from src.data_load.dataloader import create_data_loaders, create_test_loader\n",
    "\n",
    "from src.models.TimeGan import TG\n",
    "\n",
    "import torch\n",
    "import numpy as np\n",
    "import torch.nn as nn\n",
    "import torch.nn.init as init\n",
    "import torch.nn.functional as F"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def random_generator(batch_size, z_dim, T_mb, max_seq_len):\n",
    "    \"\"\"Random vector generation.\n",
    "\n",
    "    Args:\n",
    "      - batch_size: size of the random vector\n",
    "      - z_dim: dimension of random vector\n",
    "      - T_mb: time information for the random vector\n",
    "      - max_seq_len: maximum sequence length\n",
    "\n",
    "    Returns:\n",
    "      - Z_mb: generated random vector\n",
    "    \"\"\"\n",
    "    Z_mb = list()\n",
    "    for i in range(batch_size):\n",
    "        temp = np.zeros([max_seq_len, z_dim])\n",
    "        temp_Z = np.random.uniform(0.0, 1, [T_mb[i], z_dim])\n",
    "        temp[: T_mb[i], :] = temp_Z\n",
    "        Z_mb.append(temp)\n",
    "    return torch.tensor(np.stack(Z_mb)).float()\n",
    "\n",
    "\n",
    "def _weights_init(m):\n",
    "    classname = m.__class__.__name__\n",
    "    if isinstance(m, nn.Linear):\n",
    "        init.xavier_uniform_(m.weight)\n",
    "        m.bias.data.fill_(0)\n",
    "    elif classname.find(\"Conv\") != -1:\n",
    "        m.weight.data.normal_(0.0, 0.02)\n",
    "    elif classname.find(\"Norm\") != -1:\n",
    "        m.weight.data.normal_(1.0, 0.02)\n",
    "        m.bias.data.fill_(0)\n",
    "    elif classname.find(\"GRU\") != -1:\n",
    "        for name, param in m.named_parameters():\n",
    "            if \"weight_ih\" in name:\n",
    "                init.xavier_uniform_(param.data)\n",
    "            elif \"weight_hh\" in name:\n",
    "                init.orthogonal_(param.data)\n",
    "            elif \"bias\" in name:\n",
    "                param.data.fill_(0)\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "class Encoder(nn.Module):\n",
    "    \"\"\"Embedding network between original feature space to latent space.\n",
    "\n",
    "    Args:\n",
    "      - input: input time-series features. (L, N, X) = (24, ?, 6)\n",
    "      - h3: (num_layers, N, H). [3, ?, 24]\n",
    "\n",
    "    Returns:\n",
    "      - H: embeddings\n",
    "    \"\"\"\n",
    "\n",
    "    def __init__(self, input_size, d_model, num_layers, num_heads):\n",
    "        super(Encoder, self).__init__()\n",
    "\n",
    "\n",
    "        self.encoder_proj = nn.Linear(\n",
    "            input_size, d_model\n",
    "        )\n",
    "        encoder_layer = nn.TransformerEncoderLayer(\n",
    "            d_model=d_model,\n",
    "            nhead=num_heads,\n",
    "            batch_first=True,\n",
    "        )\n",
    "\n",
    "        self.encoder = nn.TransformerEncoder(\n",
    "            encoder_layer,\n",
    "            num_layers,\n",
    "            norm=nn.LayerNorm(),\n",
    "            enable_nested_tensor=True,\n",
    "            norm_first=True,\n",
    "        )\n",
    "        self.fc = nn.Linear(d_model, d_model)\n",
    "        # в оригинале стремная инициализация весов\n",
    "        self.apply(_weights_init)\n",
    "\n",
    "    def forward(self, x):\n",
    "        e_outputs, = self.encoder(self.encoder_proj(x))\n",
    "        H = self.fc(e_outputs)\n",
    "        return H\n",
    "\n",
    "\n",
    "class Recovery(nn.Module):\n",
    "    \"\"\"Recovery network from latent space to original space.\n",
    "\n",
    "    Args:\n",
    "      - H: latent representation\n",
    "      - T: input time information\n",
    "\n",
    "    Returns:\n",
    "      - X_tilde: recovered data\n",
    "    \"\"\"\n",
    "\n",
    "    def __init__(self, input_size, hidden_rnn, num_layers):\n",
    "        super(Recovery, self).__init__()\n",
    "        self.rnn = nn.GRU(\n",
    "            input_size=hidden_rnn,\n",
    "            hidden_size=input_size,\n",
    "            num_layers=num_layers,\n",
    "            batch_first=True,\n",
    "        )\n",
    "\n",
    "        self.fc = nn.Linear(input_size, input_size)\n",
    "        self.sigmoid = nn.Sigmoid()\n",
    "        self.apply(_weights_init)\n",
    "\n",
    "    def forward(self, input, sigmoid=True):\n",
    "        r_outputs, _ = self.rnn(input)\n",
    "        X_tilde = self.fc(r_outputs)\n",
    "        if sigmoid:\n",
    "            X_tilde = self.sigmoid(X_tilde)\n",
    "        return X_tilde\n",
    "\n",
    "\n",
    "class Generator(nn.Module):\n",
    "    \"\"\"Generator function: Generate time-series data in latent space.\n",
    "\n",
    "    Args:\n",
    "      - Z: random variables\n",
    "      - T: input time information\n",
    "\n",
    "    Returns:\n",
    "      - E: generated embedding\n",
    "    \"\"\"\n",
    "\n",
    "    def __init__(self, input_size, hidden_rnn, num_layers):\n",
    "        super(Generator, self).__init__()\n",
    "        self.rnn = nn.GRU(\n",
    "            input_size=input_size,\n",
    "            hidden_size=hidden_rnn,\n",
    "            num_layers=num_layers,\n",
    "            batch_first=True,\n",
    "        )\n",
    "        self.fc = nn.Linear(hidden_rnn, hidden_rnn)\n",
    "        self.sigmoid = nn.Sigmoid()\n",
    "        self.apply(_weights_init)\n",
    "\n",
    "    def forward(self, input, sigmoid=True):\n",
    "        g_outputs, _ = self.rnn(input)\n",
    "        E = self.fc(g_outputs)\n",
    "        if sigmoid:\n",
    "            E = self.sigmoid(E)\n",
    "        return E\n",
    "\n",
    "\n",
    "class Supervisor(nn.Module):\n",
    "    \"\"\"Generate next sequence using the previous sequence.\n",
    "\n",
    "    Args:\n",
    "      - H: latent representation\n",
    "      - T: input time information\n",
    "\n",
    "    Returns:\n",
    "      - S: generated sequence based on the latent representations generated by the generator\n",
    "    \"\"\"\n",
    "\n",
    "    def __init__(self, hidden_rnn, num_layers):\n",
    "        super(Supervisor, self).__init__()\n",
    "        self.rnn = nn.GRU(\n",
    "            input_size=hidden_rnn,\n",
    "            hidden_size=hidden_rnn,\n",
    "            num_layers=num_layers,\n",
    "            batch_first=True,\n",
    "        )\n",
    "        self.fc = nn.Linear(hidden_rnn, hidden_rnn)\n",
    "        self.sigmoid = nn.Sigmoid()\n",
    "        self.apply(_weights_init)\n",
    "\n",
    "    def forward(self, input, sigmoid=True):\n",
    "        s_outputs, _ = self.rnn(input)\n",
    "        S = self.fc(s_outputs)\n",
    "        if sigmoid:\n",
    "            S = self.sigmoid(S)\n",
    "        return S\n",
    "\n",
    "\n",
    "class Discriminator(nn.Module):\n",
    "    \"\"\"Discriminate the original and synthetic time-series data.\n",
    "\n",
    "    Args:\n",
    "      - H: latent representation\n",
    "      - T: input time information\n",
    "\n",
    "    Returns:\n",
    "      - Y_hat: classification results between original and synthetic time-series\n",
    "    \"\"\"\n",
    "\n",
    "    def __init__(self, hidden_rnn, num_layers):\n",
    "        super(Discriminator, self).__init__()\n",
    "        self.rnn = nn.GRU(\n",
    "            input_size=hidden_rnn,\n",
    "            hidden_size=hidden_rnn,\n",
    "            num_layers=num_layers,\n",
    "            batch_first=True,\n",
    "        )\n",
    "        self.fc = nn.Linear(hidden_rnn, 2)\n",
    "        self.apply(_weights_init)\n",
    "\n",
    "    def forward(self, input):\n",
    "        d_outputs, _ = self.rnn(input)\n",
    "        Y_hat = self.fc(d_outputs)\n",
    "        return Y_hat\n",
    "\n",
    "\n",
    "class TG(nn.Module):\n",
    "    def __init__(self, data_conf, model_conf):\n",
    "        super().__init__()\n",
    "        self.model_conf = model_conf\n",
    "        self.data_conf = data_conf\n",
    "\n",
    "        ### PROCESSORS ###\n",
    "        self.processor = prp.FeatureProcessor(\n",
    "            model_conf=model_conf, data_conf=data_conf\n",
    "        )\n",
    "\n",
    "        ### INPUT SIZE ###\n",
    "        all_emb_size = self.model_conf.features_emb_dim * len(\n",
    "            self.data_conf.features.embeddings\n",
    "        )\n",
    "\n",
    "        self.all_numeric_size = (\n",
    "            len(self.data_conf.features.numeric_values)\n",
    "            * self.model_conf.numeric_emb_size\n",
    "        )\n",
    "\n",
    "        self.input_dim = all_emb_size + self.all_numeric_size\n",
    "\n",
    "        # Pos Encoding\n",
    "        self.enc_pos_encoding = nn.Embedding(\n",
    "                self.data_conf.test.max_seq_len + 1, self.model_conf.encoder_hidden\n",
    "            )\n",
    "        self.cls_token = nn.Parameter(torch.rand(self.model_conf.encoder_hidden))\n",
    "\n",
    "        self.encoder = Encoder(\n",
    "            input_size=self.input_dim,\n",
    "            hidden_rnn=self.model_conf.timegan.rnn_hidden,\n",
    "            num_layers=self.model_conf.timegan.num_layers,\n",
    "        )\n",
    "        self.decoder = Recovery(\n",
    "            input_size=self.input_dim,\n",
    "            hidden_rnn=self.model_conf.timegan.rnn_hidden,\n",
    "            num_layers=self.model_conf.timegan.num_layers,\n",
    "        )\n",
    "        self.supervisor = Supervisor(\n",
    "            hidden_rnn=self.model_conf.timegan.rnn_hidden,\n",
    "            num_layers=self.model_conf.timegan.num_layers,\n",
    "        )\n",
    "        self.generator = Generator(\n",
    "            input_size=self.input_dim,\n",
    "            hidden_rnn=self.model_conf.timegan.rnn_hidden,\n",
    "            num_layers=self.model_conf.timegan.num_layers,\n",
    "        )\n",
    "        self.discriminator = Discriminator(\n",
    "            hidden_rnn=self.model_conf.timegan.rnn_hidden,\n",
    "            num_layers=self.model_conf.timegan.num_layers,\n",
    "        )\n",
    "\n",
    "        self.gamma = self.model_conf.timegan.gamma\n",
    "\n",
    "        # Predictors\n",
    "        self.embedding_predictor = EmbeddingPredictor(\n",
    "            model_conf=self.model_conf, data_conf=self.data_conf\n",
    "        )\n",
    "        self.numeric_projector = NumericalFeatureProjector(\n",
    "            model_conf=self.model_conf, data_conf=self.data_conf\n",
    "        )\n",
    "        self.mse_fn = torch.nn.MSELoss(reduction=\"none\")\n",
    "        # self.ce_fn = torch.nn.CrossEntropyLoss(\n",
    "        #     reduction=\"mean\", ignore_index=0, label_smoothing=0.15\n",
    "        # )\n",
    "\n",
    "    def numerical_loss(self, pred, input_batch):\n",
    "        # MSE\n",
    "        total_mse_loss = 0\n",
    "        for key, values in input_batch.payload.items():\n",
    "            if key in self.processor.numeric_names:\n",
    "                gt_val = values.float()\n",
    "                gt_val = values.float()\n",
    "                pred_val = pred[key].squeeze(-1)\n",
    "\n",
    "                mse_loss = self.mse_fn(\n",
    "                    gt_val,\n",
    "                    pred_val,\n",
    "                )\n",
    "                mask = gt_val != 0\n",
    "                masked_mse = mse_loss * mask\n",
    "                total_mse_loss += (\n",
    "                    masked_mse.sum(dim=1)  # / (mask != 0).sum(dim=1)\n",
    "                ).mean()\n",
    "\n",
    "        return total_mse_loss\n",
    "\n",
    "    def e_loss(self, decoded, padded_batch):\n",
    "        emb_dist = self.embedding_predictor(decoded)\n",
    "        cross_entropy_losses = self.embedding_predictor.loss(emb_dist, padded_batch)\n",
    "        total_ce_loss = torch.sum(\n",
    "            torch.cat([value.unsqueeze(0) for _, value in cross_entropy_losses.items()])\n",
    "        )\n",
    "\n",
    "        mse_loss = self.numerical_loss(self.numeric_projector(decoded), padded_batch)\n",
    "\n",
    "        return total_ce_loss + mse_loss\n",
    "\n",
    "    def train_embedder(self, padded_batch):\n",
    "        x, time_steps = self.processor(padded_batch)\n",
    "        latens = self.encoder(x)\n",
    "        decoded = self.decoder(latens)\n",
    "\n",
    "        lens = padded_batch.seq_lens - 1\n",
    "        global_hidden = latens[:, lens, :].diagonal().T\n",
    "\n",
    "        total_loss = self.e_loss(decoded, padded_batch)\n",
    "\n",
    "        return global_hidden, total_loss\n",
    "\n",
    "    def train_generator(self, padded_batch):\n",
    "        x, time_steps = self.processor(padded_batch)\n",
    "        bs, l, d = x.size()\n",
    "        Z = random_generator(bs, d, [l] * bs, l)\n",
    "        gen_E = self.generator(Z.to(self.model_conf.device))\n",
    "\n",
    "        gen_latens = self.supervisor(gen_E)\n",
    "        latens = self.encoder(x)\n",
    "\n",
    "        mse = F.mse_loss(latens, gen_latens, reduction=\"none\").sum(dim=[1, 2]).mean()\n",
    "        return mse\n",
    "\n",
    "    def train_joint(self, padded_batch):\n",
    "        x, time_steps = self.processor(padded_batch)\n",
    "        bs, l, d = x.size()\n",
    "        Z = random_generator(bs, d, [l] * bs, l)\n",
    "        gen_latens = self.supervisor(self.generator(Z.to(self.model_conf.device)))\n",
    "        latens = self.encoder(x)\n",
    "        decoded = self.decoder(latens)\n",
    "        gen_decoded = self.decoder(gen_latens)\n",
    "\n",
    "        y_fake = self.discriminator(gen_latens)\n",
    "        g_loss_u = (\n",
    "            F.cross_entropy(\n",
    "                y_fake.permute(0, 2, 1),\n",
    "                torch.ones_like(y_fake)[:, :, 0].long(),\n",
    "                reduction=\"none\",\n",
    "            )\n",
    "            .sum(dim=1)\n",
    "            .mean()\n",
    "        )\n",
    "        g_loss_s = (\n",
    "            F.mse_loss(latens, gen_latens, reduction=\"none\").sum(dim=[1, 2]).mean()\n",
    "        )\n",
    "\n",
    "        var_loss = torch.abs(\n",
    "            torch.sqrt(torch.var(gen_decoded, dim=0) + 1e-6)\n",
    "            - torch.sqrt(torch.var(x.detach(), dim=0) + 1e-6)\n",
    "        ).mean()\n",
    "        mean_loss = torch.abs(\n",
    "            torch.mean(gen_decoded, dim=0) - torch.mean(x.detach(), dim=0)\n",
    "        ).mean()\n",
    "        g_loss_v = var_loss + mean_loss\n",
    "\n",
    "        e_loss = self.e_loss(decoded, padded_batch)\n",
    "        e_loss = 10 * torch.sqrt(e_loss)\n",
    "        e_loss = e_loss + 0.1 * g_loss_s\n",
    "\n",
    "        return g_loss_u, g_loss_s, g_loss_v, e_loss\n",
    "\n",
    "    def train_discriminator(self, padded_batch):\n",
    "        x, time_steps = self.processor(padded_batch)\n",
    "        bs, l, d = x.size()\n",
    "        Z = random_generator(bs, d, [l] * bs, l)\n",
    "        e_gen = self.generator(Z.to(self.model_conf.device))\n",
    "        gen_latens = self.supervisor(e_gen)\n",
    "        latens = self.encoder(x)\n",
    "\n",
    "        y_fake = self.discriminator(gen_latens)\n",
    "        y_fake_e = self.discriminator(e_gen)\n",
    "        y_real = self.discriminator(latens)\n",
    "        D_loss_fake = (\n",
    "            F.cross_entropy(\n",
    "                y_fake.permute(0, 2, 1),\n",
    "                torch.zeros_like(y_fake)[:, :, 0].long(),\n",
    "                reduction=\"none\",\n",
    "            )\n",
    "            .sum(dim=1)\n",
    "            .mean()\n",
    "        )\n",
    "        D_loss_real = (\n",
    "            F.cross_entropy(\n",
    "                y_real.permute(0, 2, 1),\n",
    "                torch.ones_like(y_real)[:, :, 0].long(),\n",
    "                reduction=\"none\",\n",
    "            )\n",
    "            .sum(dim=1)\n",
    "            .mean()\n",
    "        )\n",
    "        D_loss_fake_e = (\n",
    "            F.cross_entropy(\n",
    "                y_fake_e.permute(0, 2, 1),\n",
    "                torch.zeros_like(y_fake_e)[:, :, 0].long(),\n",
    "                reduction=\"none\",\n",
    "            )\n",
    "            .sum(dim=1)\n",
    "            .mean()\n",
    "        )\n",
    "        D_loss = D_loss_real + D_loss_fake + self.gamma * D_loss_fake_e\n",
    "\n",
    "        if D_loss.item() <= 0.15:\n",
    "            D_loss = torch.tensor(0)\n",
    "\n",
    "        return D_loss\n",
    "\n",
    "    def reconstruct(self, padded_batch):\n",
    "        x, time_steps = self.processor(padded_batch)\n",
    "        latens = self.encoder(x)\n",
    "        decoded = self.decoder(latens)\n",
    "\n",
    "        pred = self.embedding_predictor(decoded)\n",
    "        pred.update(self.numeric_projector(decoded))\n",
    "\n",
    "        out = {\"pred\": pred, \"time_steps\": time_steps}\n",
    "        return out\n",
    "\n",
    "    def generate(self, padded_batch):\n",
    "        x, time_steps = self.processor(padded_batch)\n",
    "        bs, l, d = x.size()\n",
    "        Z = random_generator(bs, d, [l] * bs, l)\n",
    "        gen_latens = self.supervisor(self.generator(Z.to(self.model_conf.device)))\n",
    "        gen_decoded = self.decoder(gen_latens)\n",
    "\n",
    "        pred = self.embedding_predictor(gen_decoded)\n",
    "        pred.update(self.numeric_projector(gen_decoded))\n",
    "\n",
    "        out = {\"pred\": pred, \"time_steps\": time_steps}\n",
    "        return out\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "data_conf = data_configs()\n",
    "model_conf = model_configs()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Data shapes: train 8467, val 946, test 0\n"
     ]
    }
   ],
   "source": [
    "train_loader, val_loader = create_data_loaders(data_conf, supervised=False)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [],
   "source": [
    "for batch in train_loader:\n",
    "    break"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [],
   "source": [
    "tg = TG(model_conf=model_conf, data_conf=data_conf)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "tensor(420.7841, grad_fn=<AddBackward0>)\n"
     ]
    }
   ],
   "source": [
    "mse = tg.train_discriminator(batch[0])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "metadata": {},
   "outputs": [],
   "source": [
    "out = tg.generate(batch[0])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [],
   "source": [
    "from src.models.TimeGan import random_generator\n",
    "\n",
    "x, time_steps = tg.processor(batch[0])\n",
    "bs, l, d = x.size()\n",
    "Z = random_generator(bs, d, [l]*bs, l)\n",
    "gen_latens = tg.supervisor(tg.generator(Z))\n",
    "y_fake = tg.discriminator(gen_latens)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [
    {
     "ename": "NameError",
     "evalue": "name 'y_fake' is not defined",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mNameError\u001b[0m                                 Traceback (most recent call last)",
      "\u001b[1;32m/home/event_seq/experiments/rosbank/notebooks/timegan.ipynb Cell 8\u001b[0m line \u001b[0;36m1\n\u001b[0;32m----> <a href='vscode-notebook-cell://attached-container%2B7b22636f6e7461696e65724e616d65223a222f76765f6576656e74227d@ssh-remote%2Bmidas.skoltech.ru/home/event_seq/experiments/rosbank/notebooks/timegan.ipynb#X20sdnNjb2RlLXJlbW90ZQ%3D%3D?line=0'>1</a>\u001b[0m F\u001b[39m.\u001b[39mcross_entropy(y_fake\u001b[39m.\u001b[39mpermute(\u001b[39m0\u001b[39m,\u001b[39m2\u001b[39m,\u001b[39m1\u001b[39m), torch\u001b[39m.\u001b[39mones_like(y_fake)\u001b[39m.\u001b[39msqueeze(\u001b[39m-\u001b[39m\u001b[39m1\u001b[39m)\u001b[39m.\u001b[39mlo, reduction\u001b[39m=\u001b[39m\u001b[39m'\u001b[39m\u001b[39mnone\u001b[39m\u001b[39m'\u001b[39m)\u001b[39m.\u001b[39msize()\n",
      "\u001b[0;31mNameError\u001b[0m: name 'y_fake' is not defined"
     ]
    }
   ],
   "source": [
    "F.cross_entropy(y_fake.permute(0,2,1), torch.ones_like(y_fake).squeeze(-1).lo, reduction='none').size()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "(torch.Size([128, 2, 200]), torch.Size([128, 200]))"
      ]
     },
     "execution_count": 11,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "y_fake.permute(0,2,1).size(), torch.ones_like(y_fake)[:,:,0].size()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 22,
   "metadata": {},
   "outputs": [],
   "source": [
    "opt = torch.optim.Adam(\n",
    "            list(tg.encoder.parameters()) + list(tg.decoder.parameters()), model_conf.lr, weight_decay=model_conf.weight_decay\n",
    "        )"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [],
   "source": [
    "x = torch.rand(3, 5, 7) \n",
    "TG = TG(input_size=7, hidden_rnn=11, num_layers=2)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "tensor(-0.2320, grad_fn=<AddBackward0>)\n"
     ]
    }
   ],
   "source": [
    "out = TG.train_discriminator(x)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 21,
   "metadata": {},
   "outputs": [
    {
     "ename": "AttributeError",
     "evalue": "type object 'TG' has no attribute 'encoder'",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mAttributeError\u001b[0m                            Traceback (most recent call last)",
      "\u001b[1;32m/home/event_seq/experiments/rosbank/notebooks/timegan.ipynb Cell 15\u001b[0m line \u001b[0;36m2\n\u001b[1;32m      <a href='vscode-notebook-cell://attached-container%2B7b22636f6e7461696e65724e616d65223a222f76765f6576656e74227d@ssh-remote%2Bmidas.skoltech.ru/home/event_seq/experiments/rosbank/notebooks/timegan.ipynb#W4sdnNjb2RlLXJlbW90ZQ%3D%3D?line=0'>1</a>\u001b[0m opt \u001b[39m=\u001b[39m torch\u001b[39m.\u001b[39moptim\u001b[39m.\u001b[39mAdam(\n\u001b[0;32m----> <a href='vscode-notebook-cell://attached-container%2B7b22636f6e7461696e65724e616d65223a222f76765f6576656e74227d@ssh-remote%2Bmidas.skoltech.ru/home/event_seq/experiments/rosbank/notebooks/timegan.ipynb#W4sdnNjb2RlLXJlbW90ZQ%3D%3D?line=1'>2</a>\u001b[0m             \u001b[39mlist\u001b[39m(TG\u001b[39m.\u001b[39;49mencoder\u001b[39m.\u001b[39mparameters()) \u001b[39m+\u001b[39m \u001b[39mlist\u001b[39m(TG\u001b[39m.\u001b[39mdecoder\u001b[39m.\u001b[39mparameters()), model_conf\u001b[39m.\u001b[39mlr, weight_decay\u001b[39m=\u001b[39mmodel_conf\u001b[39m.\u001b[39mweight_decay\n\u001b[1;32m      <a href='vscode-notebook-cell://attached-container%2B7b22636f6e7461696e65724e616d65223a222f76765f6576656e74227d@ssh-remote%2Bmidas.skoltech.ru/home/event_seq/experiments/rosbank/notebooks/timegan.ipynb#W4sdnNjb2RlLXJlbW90ZQ%3D%3D?line=2'>3</a>\u001b[0m         )\n",
      "\u001b[0;31mAttributeError\u001b[0m: type object 'TG' has no attribute 'encoder'"
     ]
    }
   ],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": 66,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "(torch.Size([3, 5, 7]), torch.Size([3, 5, 7]))"
      ]
     },
     "execution_count": 66,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "d_scores, gen_latens, decoded, x1 = net(x)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 72,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "tensor(14.1805, grad_fn=<MeanBackward0>)"
      ]
     },
     "execution_count": 72,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "F.mse_loss(x1, decoded, reduction='none').sum(dim=[1,2]).mean()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.10.12"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
