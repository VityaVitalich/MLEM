{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "import torch.nn as nn\n",
    "import torch\n",
    "import sys\n",
    "\n",
    "sys.path.append('../../../')\n",
    "\n",
    "from configs.data_configs.rosbank import data_configs\n",
    "from configs.model_configs.gen.rosbank import model_configs\n",
    "from src.data_load.dataloader import create_data_loaders, create_test_loader\n",
    "\n",
    "import src.models.preprocessors as prp\n",
    "\n",
    "from src.models.gen_models import NumericalFeatureProjector, EmbeddingPredictor\n",
    "#from src.models.timevae import TimeVAE"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {},
   "outputs": [],
   "source": [
    "# due to flatten constant len of input needed\n",
    "\n",
    "def sample_z(mean, logstd, k_iwae):\n",
    "    epsilon = torch.randn(k_iwae, mean.shape[0], mean.shape[1]).to(\n",
    "        logstd.device\n",
    "    )\n",
    "    z = epsilon * torch.exp(0.5 * logstd) + mean  # modified\n",
    "    z = z.view(-1, mean.shape[1])\n",
    "    return z\n",
    "\n",
    "def get_normal_kl(mean_1, log_std_1, mean_2=None, log_std_2=None):\n",
    "    \"\"\"\n",
    "    This function should return the value of KL(p1 || p2),\n",
    "    where p1 = Normal(mean_1, exp(log_std_1)), p2 = Normal(mean_2, exp(log_std_2) ** 2).\n",
    "    If mean_2 and log_std_2 are None values, we will use standard normal distribution.\n",
    "    Note that we consider the case of diagonal covariance matrix.\n",
    "    \"\"\"\n",
    "    if mean_2 is None:\n",
    "        mean_2 = torch.zeros_like(mean_1).to(mean_1.device)\n",
    "    if log_std_2 is None:\n",
    "        log_std_2 = torch.zeros_like(log_std_1).to(mean_1.device)\n",
    "    # ====\n",
    "    # https://stats.stackexchange.com/questions/7440/kl-divergence-between-two-univariate-gaussians\n",
    "    # https://stats.stackexchange.com/questions/60680/kl-divergence-between-two-multivariate-gaussians\n",
    "\n",
    "    sigma_1 = torch.exp(log_std_1)\n",
    "    sigma_2 = torch.exp(log_std_2)\n",
    "\n",
    "    out = torch.log(sigma_2 / sigma_1)\n",
    "    out += (sigma_1**2 + (mean_1 - mean_2) ** 2) / (2 * (sigma_2**2))\n",
    "    out -= 1 / 2\n",
    "\n",
    "    return out\n",
    "\n",
    "\n",
    "\n",
    "class TimeVAE(nn.Module):\n",
    "\n",
    "    def __init__(self, model_conf, data_conf):\n",
    "        super().__init__()\n",
    "        self.model_conf = model_conf\n",
    "        self.data_conf = data_conf\n",
    "\n",
    "        self.processor = prp.FeatureProcessor(\n",
    "            model_conf=model_conf, data_conf=data_conf\n",
    "        )\n",
    "        self.time_encoder = prp.TimeEncoder(\n",
    "            model_conf=self.model_conf, data_conf=self.data_conf\n",
    "        )\n",
    "\n",
    "        ### INPUT SIZE ###\n",
    "        all_emb_size = self.model_conf.features_emb_dim * len(\n",
    "            self.data_conf.features.embeddings\n",
    "        )\n",
    "\n",
    "        self.all_numeric_size = (\n",
    "            len(self.data_conf.features.numeric_values)\n",
    "            * self.model_conf.numeric_emb_size\n",
    "        )\n",
    "\n",
    "        self.input_dim = (\n",
    "            all_emb_size + self.all_numeric_size + self.model_conf.use_deltas\n",
    "        )\n",
    "        self.out_dim = self.input_dim\n",
    "        if self.model_conf.time_embedding:\n",
    "            self.input_dim += self.model_conf.time_embedding * 2 - 1\n",
    "\n",
    "        ### INIT ENCODER\n",
    "        ls = []\n",
    "        prev_dim = self.input_dim\n",
    "        for i, dim in enumerate(self.model_conf.timevae.hiddens):\n",
    "            ls.append(nn.Conv1d(in_channels=prev_dim, out_channels=dim, kernel_size=3, stride=2, padding=3))\n",
    "            ls.append(nn.ReLU())\n",
    "            prev_dim = dim\n",
    "        \n",
    "        ls.append(nn.Flatten())\n",
    "        self.encoder_conv = nn.Sequential(*ls)\n",
    "        self.flat_size = self.detect_size(self.input_dim, self.data_conf.train.max_seq_len)\n",
    "        \n",
    "        self.mu_head = nn.Linear(self.flat_size, self.model_conf.timevae.latent_dim)\n",
    "        self.log_std_head = nn.Linear(self.flat_size, self.model_conf.timevae.latent_dim)\n",
    "\n",
    "        ### Decoder Init ###\n",
    "        self.dec_proj = nn.Linear(self.model_conf.timevae.latent_dim, self.flat_size)\n",
    "\n",
    "        ls = []\n",
    "        prev_dim = self.model_conf.timevae.hiddens[-1]\n",
    "        for i, dim in enumerate(reversed(self.model_conf.timevae.hiddens[:-1])):\n",
    "            ls.append(nn.ConvTranspose1d(in_channels=prev_dim, out_channels=dim, kernel_size=3, stride=2, padding=2))\n",
    "            ls.append(nn.ReLU())\n",
    "            prev_dim = dim\n",
    "        ls.append(nn.ConvTranspose1d(in_channels=prev_dim, out_channels=self.input_dim, kernel_size=3, stride=2, padding=2))        \n",
    "        ls.append(nn.ReLU())\n",
    "        ls.append(nn.Flatten())\n",
    "        self.decoder = nn.Sequential(*ls)\n",
    "        out_proj_size = self.detect_dec_size()\n",
    "        self.decoder_out_proj = nn.Linear(out_proj_size, self.data_conf.train.max_seq_len * self.out_dim)\n",
    "\n",
    "\n",
    "        ### LOSS ###\n",
    "        self.embedding_predictor = EmbeddingPredictor(\n",
    "            model_conf=self.model_conf, data_conf=self.data_conf\n",
    "        )\n",
    "        self.numeric_projector = NumericalFeatureProjector(\n",
    "            model_conf=self.model_conf, data_conf=self.data_conf\n",
    "        )\n",
    "        self.mse_fn = torch.nn.MSELoss(reduction=\"none\")\n",
    "        self.ce_fn = torch.nn.CrossEntropyLoss(\n",
    "            reduction=\"mean\", ignore_index=0, label_smoothing=0.05\n",
    "        )\n",
    "\n",
    "    def forward(self, padded_batch):\n",
    "        x, time_steps = self.processor(padded_batch)\n",
    "        x = self.time_encoder(x, time_steps)\n",
    "        \n",
    "        mu, log_std = self.encode(x.transpose(1,2))\n",
    "        z = sample_z(mu, log_std, 1)\n",
    "        out = self.decode(z)\n",
    "\n",
    "        \n",
    "\n",
    "\n",
    "        pred = self.embedding_predictor(out)\n",
    "        pred.update(self.numeric_projector(out))\n",
    "\n",
    "        if self.model_conf.use_deltas:\n",
    "            pred[\"delta\"] = out[:, :, -1].squeeze(-1)\n",
    "\n",
    "        gt = {'input_batch': padded_batch, 'time_steps': time_steps}\n",
    "\n",
    "        res_dict = {'gt': gt,\n",
    "                    'pred': pred,\n",
    "                    'latent': z,\n",
    "                    'mu': mu, \n",
    "                    'log_std': log_std}\n",
    "        return res_dict\n",
    "\n",
    "    \n",
    "    def decode(self, z):\n",
    "        projected = self.dec_proj(z).view(z.size(0), self.model_conf.timevae.hiddens[-1], -1)\n",
    "        #print(projected)\n",
    "        decoded = self.decoder(projected)\n",
    "        print(decoded.size())\n",
    "        out = self.decoder_out_proj(decoded)\n",
    "        return out.view(z.size(0), self.data_conf.train.max_seq_len, self.out_dim)\n",
    "\n",
    "    def encode(self, x):\n",
    "        features = self.encoder_conv(x)\n",
    "        mu = self.mu_head(features)\n",
    "        log_std = self.log_std_head(features)\n",
    "\n",
    "        return mu, log_std\n",
    "    \n",
    "    def detect_size(self, in_dim, seq_len):\n",
    "        test_value = torch.rand(1, seq_len, in_dim)\n",
    "        \n",
    "        with torch.no_grad():\n",
    "            out = self.encoder_conv(test_value.transpose(1,2))\n",
    "\n",
    "        return out.size(1)\n",
    "\n",
    "    def detect_dec_size(self):\n",
    "        test_value = torch.rand(1, self.model_conf.timevae.latent_dim)\n",
    "        with torch.no_grad():\n",
    "            projected = self.dec_proj(test_value).view(1, self.model_conf.timevae.hiddens[-1], -1)\n",
    "            decoded = self.decoder(projected)\n",
    "\n",
    "        return decoded.size(1)\n",
    "    \n",
    "    \n",
    "    def loss(self, output, ground_truth):\n",
    "        \"\"\"\n",
    "        output: Dict that is outputed from forward method\n",
    "        \"\"\"\n",
    "        ### MSE ###\n",
    "        total_mse_loss = self.numerical_loss(output)\n",
    "        delta_mse_loss = self.delta_mse_loss(output)\n",
    "\n",
    "        ### CROSS ENTROPY ###\n",
    "        cross_entropy_losses = self.embedding_predictor.loss(\n",
    "            output[\"pred\"], output[\"gt\"][\"input_batch\"]\n",
    "        )\n",
    "        total_ce_loss = torch.sum(\n",
    "            torch.cat([value.unsqueeze(0) for _, value in cross_entropy_losses.items()])\n",
    "        )\n",
    "\n",
    "        kl_loss = get_normal_kl(mean_1=output['mu'], log_std_1=output['log_std']).sum(dim=1).mean()\n",
    "\n",
    "        losses_dict = {\n",
    "            \"total_mse_loss\": total_mse_loss,\n",
    "            \"total_CE_loss\": total_ce_loss,\n",
    "            \"total_KL_loss\": kl_loss, \n",
    "            \"delta_loss\": delta_mse_loss\n",
    "        }\n",
    "        losses_dict.update(cross_entropy_losses)\n",
    "\n",
    "        total_loss = (\n",
    "            (self.model_conf.mse_weight * losses_dict[\"total_mse_loss\"]\n",
    "            + self.model_conf.CE_weight * total_ce_loss + self.model_conf.delta_weight * delta_mse_loss) * self.model_conf.timevae.recon_weight\n",
    "            + kl_loss\n",
    "        )\n",
    "        losses_dict[\"total_loss\"] = total_loss\n",
    "\n",
    "        return losses_dict\n",
    "\n",
    "    def generate(self, padded_batch, lens):\n",
    "        Z = torch.randn(lens, self.latent_dim)\n",
    "        samples = self.decode(Z)\n",
    "        return samples\n",
    "    \n",
    "\n",
    "    def numerical_loss(self, output):\n",
    "        # MSE\n",
    "        total_mse_loss = 0\n",
    "        for key, values in output[\"gt\"][\"input_batch\"].payload.items():\n",
    "            if key in self.processor.numeric_names:\n",
    "                gt_val = values.float()\n",
    "                gt_val = values.float()\n",
    "                pred_val = output[\"pred\"][key].squeeze(-1)\n",
    "\n",
    "                mse_loss = self.mse_fn(\n",
    "                    gt_val,\n",
    "                    pred_val,\n",
    "                )\n",
    "                mask = gt_val != 0\n",
    "                masked_mse = mse_loss * mask\n",
    "                total_mse_loss += (\n",
    "                    masked_mse.sum(dim=1)  # / (mask != 0).sum(dim=1)\n",
    "                ).mean()\n",
    "\n",
    "        return total_mse_loss\n",
    "\n",
    "    def delta_mse_loss(self, output):\n",
    "        # DELTA MSE\n",
    "        if self.model_conf.use_deltas:\n",
    "            gt_delta = output[\"gt\"][\"time_steps\"].diff(1)\n",
    "            if self.model_conf.use_log_delta:\n",
    "                gt_delta = torch.log(gt_delta + 1e-15)\n",
    "            delta_mse = self.mse_fn(gt_delta, output[\"pred\"][\"delta\"][:, :-1])\n",
    "            # print(delta_mse, gt_delta[0], output[\"gt\"][\"time_steps\"].diff(1)[0], output[\"gt\"][\"time_steps\"][0])\n",
    "            mask = output[\"gt\"][\"time_steps\"] != -1\n",
    "\n",
    "            delta_masked = delta_mse * mask[:, :-1]\n",
    "            delta_mse = delta_masked.sum() / (mask != 0).sum()\n",
    "        else:\n",
    "            delta_mse = torch.tensor(0)\n",
    "\n",
    "        return delta_mse\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {},
   "outputs": [],
   "source": [
    "model_conf = model_configs()\n",
    "data_conf = data_configs()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {},
   "outputs": [],
   "source": [
    "tv = TimeVAE(model_conf=model_conf, data_conf=data_conf)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Data shapes: train 8467, val 946, test 0\n"
     ]
    }
   ],
   "source": [
    "train_loader, val_loader = create_data_loaders(data_conf, supervised=False)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "metadata": {},
   "outputs": [],
   "source": [
    "for batch in train_loader:\n",
    "    break"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "TimeVAE(\n",
       "  (processor): FeatureProcessor(\n",
       "    (embed_layers): ModuleDict(\n",
       "      (channel_type): Embedding(5, 12)\n",
       "      (currency): Embedding(5, 12)\n",
       "      (mcc): Embedding(100, 12)\n",
       "      (trx_category): Embedding(12, 12)\n",
       "    )\n",
       "    (numeric_processor): ModuleDict(\n",
       "      (amount): Linear(in_features=1, out_features=12, bias=True)\n",
       "    )\n",
       "    (numeric_norms): ModuleDict(\n",
       "      (amount): RBatchNormWithLens(\n",
       "        (bn): BatchNorm1d(1, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
       "      )\n",
       "    )\n",
       "  )\n",
       "  (time_encoder): TimeEncoder()\n",
       "  (encoder_conv): Sequential(\n",
       "    (0): Conv1d(64, 64, kernel_size=(3,), stride=(2,), padding=(3,))\n",
       "    (1): ReLU()\n",
       "    (2): Conv1d(64, 64, kernel_size=(3,), stride=(2,), padding=(3,))\n",
       "    (3): ReLU()\n",
       "    (4): Flatten(start_dim=1, end_dim=-1)\n",
       "  )\n",
       "  (mu_head): Linear(in_features=3392, out_features=32, bias=True)\n",
       "  (log_std_head): Linear(in_features=3392, out_features=32, bias=True)\n",
       "  (dec_proj): Linear(in_features=32, out_features=3392, bias=True)\n",
       "  (decoder): Sequential(\n",
       "    (0): ConvTranspose1d(64, 64, kernel_size=(3,), stride=(2,), padding=(2,))\n",
       "    (1): ReLU()\n",
       "    (2): ConvTranspose1d(64, 64, kernel_size=(3,), stride=(2,), padding=(2,))\n",
       "    (3): ReLU()\n",
       "    (4): Flatten(start_dim=1, end_dim=-1)\n",
       "  )\n",
       "  (decoder_out_proj): Linear(in_features=12992, out_features=12200, bias=True)\n",
       "  (embedding_predictor): EmbeddingPredictor(\n",
       "    (criterion): CrossEntropyLoss()\n",
       "    (embed_predictors): ModuleDict(\n",
       "      (channel_type): Linear(in_features=12, out_features=5, bias=True)\n",
       "      (currency): Linear(in_features=12, out_features=5, bias=True)\n",
       "      (mcc): Linear(in_features=12, out_features=100, bias=True)\n",
       "      (trx_category): Linear(in_features=12, out_features=12, bias=True)\n",
       "    )\n",
       "  )\n",
       "  (numeric_projector): NumericalFeatureProjector(\n",
       "    (embed_predictors): ModuleDict(\n",
       "      (amount): Linear(in_features=12, out_features=1, bias=True)\n",
       "    )\n",
       "  )\n",
       "  (mse_fn): MSELoss()\n",
       "  (ce_fn): CrossEntropyLoss()\n",
       ")"
      ]
     },
     "execution_count": 16,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "tv"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "torch.Size([512, 12992])\n"
     ]
    }
   ],
   "source": [
    "out = tv(batch[0])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "metadata": {},
   "outputs": [],
   "source": [
    "losses = tv.loss(out, batch[1])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "{'total_mse_loss': tensor(4505.9702, grad_fn=<AddBackward0>),\n",
       " 'total_CE_loss': tensor(934.3182, grad_fn=<SumBackward0>),\n",
       " 'total_KL_loss': tensor(0.3102, grad_fn=<MeanBackward0>),\n",
       " 'delta_loss': tensor(0.0124, grad_fn=<DivBackward0>),\n",
       " 'channel_type': tensor(147.4019, grad_fn=<MeanBackward0>),\n",
       " 'currency': tensor(154.8810, grad_fn=<MeanBackward0>),\n",
       " 'mcc': tensor(423.8500, grad_fn=<MeanBackward0>),\n",
       " 'trx_category': tensor(208.1854, grad_fn=<MeanBackward0>),\n",
       " 'total_loss': tensor(16321.5498, grad_fn=<AddBackward0>)}"
      ]
     },
     "execution_count": 19,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "losses"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 27,
   "metadata": {},
   "outputs": [],
   "source": [
    "losses['total_loss'].backward()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 99,
   "metadata": {},
   "outputs": [],
   "source": [
    "z = torch.rand(32, 4)\n",
    "dec = tv.decode(z)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 103,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "torch.Size([32, 7, 5])"
      ]
     },
     "execution_count": 103,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "dec.size()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 53,
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "out_proj = nn.Linear(4, 125)\n",
    "projected = out_proj(z)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 61,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "tensor([ 0.0246,  0.9201, -0.1976,  0.6290, -0.2905, -0.0487, -0.6348, -0.5932,\n",
       "         0.1185,  1.0665,  0.1459,  0.0549,  0.7347, -0.1536, -0.5866, -0.4701,\n",
       "         0.0191, -0.6995, -0.2983, -0.0667,  0.2349, -0.2173, -0.4106, -0.8101,\n",
       "         0.5722], grad_fn=<SliceBackward0>)"
      ]
     },
     "execution_count": 61,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "projected.reshape(32, -1, 25)[0, 0, :]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 65,
   "metadata": {},
   "outputs": [],
   "source": [
    "p = projected.reshape(32, 25, -1)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 66,
   "metadata": {},
   "outputs": [],
   "source": [
    "c = nn.ConvTranspose1d(in_channels=25, out_channels=25, kernel_size=3, padding=2, stride=2)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 67,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "torch.Size([32, 25, 7])"
      ]
     },
     "execution_count": 67,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "c(p).size()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 68,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "torch.Size([32, 25, 5])"
      ]
     },
     "execution_count": 68,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "p.size()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 45,
   "metadata": {},
   "outputs": [],
   "source": [
    "x = torch.rand(32, 200, 64)\n",
    "c = nn.ConvTranspose1d(in_channels=64, out_channels=60, kernel_size=3, padding=0, stride=2)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 46,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "torch.Size([32, 60, 401])"
      ]
     },
     "execution_count": 46,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "c(x.transpose(1,2)).size()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.10.12"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
