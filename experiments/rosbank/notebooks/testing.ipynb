{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "import pandas as pd\n",
    "import numpy as np\n",
    "from tqdm import tqdm\n",
    "import pyarrow.parquet as pq\n",
    "import sys\n",
    "import random\n",
    "\n",
    "sys.path.append('../../../')\n",
    "from configs.data_configs.rosbank import data_configs\n",
    "\n",
    "import split_strategy\n",
    "from splitting_dataset import SplittingDataset, TargetEnumeratorDataset, ConvertingTrxDataset, DropoutTrxDataset\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "conf = data_configs()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>cl_id</th>\n",
       "      <th>amount</th>\n",
       "      <th>event_time</th>\n",
       "      <th>mcc</th>\n",
       "      <th>channel_type</th>\n",
       "      <th>currency</th>\n",
       "      <th>trx_category</th>\n",
       "      <th>trx_count</th>\n",
       "      <th>target_target_flag</th>\n",
       "      <th>target_target_sum</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>10018</td>\n",
       "      <td>[10.609081944147828, 10.596659732783579, 10.81...</td>\n",
       "      <td>[17120.38773148148, 17133.667800925927, 17134....</td>\n",
       "      <td>[13, 2, 13, 2, 1, 18, 13, 2, 13, 2, 5, 13, 9, ...</td>\n",
       "      <td>[1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1]</td>\n",
       "      <td>[1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1]</td>\n",
       "      <td>[5, 3, 5, 3, 1, 1, 5, 3, 5, 3, 1, 5, 5, 5, 5]</td>\n",
       "      <td>15</td>\n",
       "      <td>0</td>\n",
       "      <td>0.0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>10030</td>\n",
       "      <td>[4.61512051684126, 6.90875477931522, 10.598857...</td>\n",
       "      <td>[17141.0, 17141.0, 17145.0, 17147.0, 17147.0, ...</td>\n",
       "      <td>[9, 9, 21, 1, 25, 6, 14, 14, 3, 3, 3, 13, 1, 3...</td>\n",
       "      <td>[1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, ...</td>\n",
       "      <td>[1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, ...</td>\n",
       "      <td>[5, 5, 1, 1, 1, 1, 1, 1, 1, 1, 1, 7, 1, 1, 3, ...</td>\n",
       "      <td>42</td>\n",
       "      <td>1</td>\n",
       "      <td>59.51</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>10038</td>\n",
       "      <td>[7.4127640174265625, 7.370230641807081, 7.8180...</td>\n",
       "      <td>[17301.0, 17301.0, 17301.0, 17301.774780092594...</td>\n",
       "      <td>[1, 1, 1, 2, 2, 4, 2, 8, 1, 22, 8, 1, 8, 4, 2,...</td>\n",
       "      <td>[1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, ...</td>\n",
       "      <td>[1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, ...</td>\n",
       "      <td>[1, 1, 1, 2, 2, 1, 3, 1, 1, 1, 1, 1, 1, 1, 2, ...</td>\n",
       "      <td>111</td>\n",
       "      <td>0</td>\n",
       "      <td>0.0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>10057</td>\n",
       "      <td>[7.494708263135679, 7.736394428979239, 10.7789...</td>\n",
       "      <td>[17151.0, 17151.0, 17153.0, 17154.0, 17155.0, ...</td>\n",
       "      <td>[6, 21, 2, 6, 2, 4, 2, 22, 15, 2, 1, 35, 4, 2,...</td>\n",
       "      <td>[1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, ...</td>\n",
       "      <td>[1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, ...</td>\n",
       "      <td>[1, 1, 4, 1, 4, 1, 3, 1, 1, 3, 1, 1, 1, 4, 1, ...</td>\n",
       "      <td>61</td>\n",
       "      <td>1</td>\n",
       "      <td>62961.31</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>10062</td>\n",
       "      <td>[8.31898612539206, 8.824824939175638, 6.509067...</td>\n",
       "      <td>[17143.0, 17143.0, 17143.0, 17144.0, 17144.0, ...</td>\n",
       "      <td>[80, 15, 37, 38, 11, 11, 2, 24, 7, 5, 5, 11, 1...</td>\n",
       "      <td>[1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, ...</td>\n",
       "      <td>[1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, ...</td>\n",
       "      <td>[1, 1, 1, 1, 1, 1, 3, 1, 1, 1, 1, 1, 1, 1, 1, ...</td>\n",
       "      <td>82</td>\n",
       "      <td>1</td>\n",
       "      <td>107126.35</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "   cl_id                                             amount  \\\n",
       "0  10018  [10.609081944147828, 10.596659732783579, 10.81...   \n",
       "1  10030  [4.61512051684126, 6.90875477931522, 10.598857...   \n",
       "2  10038  [7.4127640174265625, 7.370230641807081, 7.8180...   \n",
       "3  10057  [7.494708263135679, 7.736394428979239, 10.7789...   \n",
       "4  10062  [8.31898612539206, 8.824824939175638, 6.509067...   \n",
       "\n",
       "                                          event_time  \\\n",
       "0  [17120.38773148148, 17133.667800925927, 17134....   \n",
       "1  [17141.0, 17141.0, 17145.0, 17147.0, 17147.0, ...   \n",
       "2  [17301.0, 17301.0, 17301.0, 17301.774780092594...   \n",
       "3  [17151.0, 17151.0, 17153.0, 17154.0, 17155.0, ...   \n",
       "4  [17143.0, 17143.0, 17143.0, 17144.0, 17144.0, ...   \n",
       "\n",
       "                                                 mcc  \\\n",
       "0  [13, 2, 13, 2, 1, 18, 13, 2, 13, 2, 5, 13, 9, ...   \n",
       "1  [9, 9, 21, 1, 25, 6, 14, 14, 3, 3, 3, 13, 1, 3...   \n",
       "2  [1, 1, 1, 2, 2, 4, 2, 8, 1, 22, 8, 1, 8, 4, 2,...   \n",
       "3  [6, 21, 2, 6, 2, 4, 2, 22, 15, 2, 1, 35, 4, 2,...   \n",
       "4  [80, 15, 37, 38, 11, 11, 2, 24, 7, 5, 5, 11, 1...   \n",
       "\n",
       "                                        channel_type  \\\n",
       "0      [1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1]   \n",
       "1  [1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, ...   \n",
       "2  [1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, ...   \n",
       "3  [1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, ...   \n",
       "4  [1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, ...   \n",
       "\n",
       "                                            currency  \\\n",
       "0      [1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1]   \n",
       "1  [1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, ...   \n",
       "2  [1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, ...   \n",
       "3  [1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, ...   \n",
       "4  [1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, ...   \n",
       "\n",
       "                                        trx_category  trx_count  \\\n",
       "0      [5, 3, 5, 3, 1, 1, 5, 3, 5, 3, 1, 5, 5, 5, 5]         15   \n",
       "1  [5, 5, 1, 1, 1, 1, 1, 1, 1, 1, 1, 7, 1, 1, 3, ...         42   \n",
       "2  [1, 1, 1, 2, 2, 1, 3, 1, 1, 1, 1, 1, 1, 1, 2, ...        111   \n",
       "3  [1, 1, 4, 1, 4, 1, 3, 1, 1, 3, 1, 1, 1, 4, 1, ...         61   \n",
       "4  [1, 1, 1, 1, 1, 1, 3, 1, 1, 1, 1, 1, 1, 1, 1, ...         82   \n",
       "\n",
       "  target_target_flag target_target_sum  \n",
       "0                  0               0.0  \n",
       "1                  1             59.51  \n",
       "2                  0               0.0  \n",
       "3                  1          62961.31  \n",
       "4                  1         107126.35  "
      ]
     },
     "execution_count": 3,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "df = pd.read_parquet(conf.train_path)\n",
    "df.head()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [],
   "source": [
    "def read_pyarrow_file(path, use_threads=True):\n",
    "    p_table = pq.read_table(\n",
    "        source=path,\n",
    "        use_threads=use_threads,\n",
    "    )\n",
    "\n",
    "    col_indexes = [n for n in p_table.column_names]\n",
    "\n",
    "    def get_records():\n",
    "        for rb in p_table.to_batches():\n",
    "            col_arrays = [rb.column(i) for i, _ in enumerate(col_indexes)]\n",
    "            col_arrays = [a.to_numpy(zero_copy_only=False) for a in col_arrays]\n",
    "            for row in zip(*col_arrays):\n",
    "                # np.array(a) makes `a` writable for future usage\n",
    "                rec = {n: np.array(a) if isinstance(a, np.ndarray) else a for n, a in zip(col_indexes, row)}\n",
    "                yield rec\n",
    "\n",
    "    return get_records()\n",
    "\n",
    "\n",
    "def prepare_embeddings(seq, conf, is_train):\n",
    "    min_seq_len = 1\n",
    "    embeddings = list(conf.features.embeddings.keys())\n",
    "\n",
    "    feature_keys = embeddings + list(conf.features.numeric_values.keys())\n",
    "\n",
    "    for rec in seq:\n",
    "        seq_len = len(rec['event_time'])\n",
    "        if is_train and seq_len < min_seq_len:\n",
    "            continue\n",
    "\n",
    "        if 'feature_arrays' in rec:\n",
    "            feature_arrays = rec['feature_arrays']\n",
    "            feature_arrays = {k: v for k, v in feature_arrays.items() if k in feature_keys}\n",
    "        else:\n",
    "            feature_arrays = {k: v for k, v in rec.items() if k in feature_keys}\n",
    "\n",
    "        # TODO: datetime processing. Take date-time features\n",
    "\n",
    "        # shift embeddings to 1, 0 is padding value\n",
    "        feature_arrays = {k: v + (1 if k in embeddings else 0) for k, v in feature_arrays.items()}\n",
    "\n",
    "        # clip embeddings dictionary by max value\n",
    "        for e_name, e_params in conf.features.embeddings.items():\n",
    "            feature_arrays[e_name] = feature_arrays[e_name].clip(0, e_params['in'] - 1)\n",
    "\n",
    "        feature_arrays['event_time'] = rec['event_time']\n",
    "\n",
    "        rec['feature_arrays'] = feature_arrays\n",
    "        yield rec\n",
    "\n",
    "def shuffle_client_list_reproducible(conf, data):\n",
    "    if conf.client_list_shuffle_seed != 0:\n",
    "        dataset_col_id = conf.get('col_id', 'client_id')\n",
    "        data = sorted(data, key=lambda x: x.get(dataset_col_id)) #changed from COLES a bit\n",
    "        random.Random(conf.client_list_shuffle_seed).shuffle(data)\n",
    "    return data"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [],
   "source": [
    "def prepare_data(conf):\n",
    "    data = read_pyarrow_file(conf.train_path)\n",
    "    data = tqdm(data)\n",
    "    \n",
    "    data = prepare_embeddings(data, conf, is_train=True)\n",
    "    data = shuffle_client_list_reproducible(conf, data)\n",
    "    data = list(data)\n",
    "\n",
    "\n",
    "    valid_ix = np.arange(len(data))\n",
    "    valid_ix = np.random.choice(valid_ix, size=int(len(data) * conf.valid_size), replace=False)\n",
    "    valid_ix = set(valid_ix.tolist())\n",
    "\n",
    "   # logger.info(f'Loaded {len(data)} rows. Split in progress...')\n",
    "    train_data = [rec for i, rec in enumerate(data) if i not in valid_ix]\n",
    "    valid_data = [rec for i, rec in enumerate(data) if i in valid_ix]\n",
    "\n",
    "    #logger.info(f'Train data len: {len(train_data)}, Valid data len: {len(valid_data)}')\n",
    "\n",
    "    return train_data, valid_data"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "4501it [00:00, 22805.94it/s]"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "9717it [00:00, 18333.81it/s]\n"
     ]
    }
   ],
   "source": [
    "train, valid = prepare_data(conf)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [],
   "source": [
    "train_dataset = SplittingDataset(\n",
    "        train,\n",
    "        split_strategy.create(**conf.train.split_strategy)\n",
    "    )\n",
    "\n",
    "train_dataset = TargetEnumeratorDataset(train_dataset)\n",
    "train_dataset = ConvertingTrxDataset(train_dataset)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "({'amount': tensor([ 6.9383,  6.1944,  6.6284,  9.2104, 10.3090,  7.9728, 10.8198, 10.5967,\n",
       "           8.1306,  6.0753,  4.4543,  7.0598,  6.9856,  6.3168,  4.7005,  5.4647,\n",
       "           6.0831,  7.4430,  6.0124,  7.2951,  7.9494,  5.2193,  6.4693,  6.3335,\n",
       "           8.2496,  9.6097,  5.8230,  6.6431,  6.8967,  4.4188,  6.5761, 10.8198,\n",
       "           5.9254,  5.9819,  6.5596,  7.4568,  4.1636,  8.5073,  8.8847,  7.3172,\n",
       "           6.0986,  9.8828,  9.1009,  9.4351,  6.6265,  7.4989,  8.8538, 10.3090,\n",
       "          10.3090,  7.8335,  6.1506,  7.9452,  6.2425,  6.5132, 10.3981,  6.6053,\n",
       "           6.1417], dtype=torch.float64),\n",
       "  'mcc': tensor([ 5,  2, 19,  3,  3, 59,  3,  3,  2, 73,  2,  2,  5,  4,  2,  4,  2,  2,\n",
       "           2, 39,  7,  2,  2,  2, 99, 52,  2,  2,  9,  6,  2,  3,  2,  9,  8,  8,\n",
       "           8, 26, 21,  8,  2, 58, 58, 69,  7,  7,  3,  3,  3,  7, 57,  7,  2,  4,\n",
       "          75, 57,  2], dtype=torch.int32),\n",
       "  'channel_type': tensor([2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2,\n",
       "          2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2,\n",
       "          2, 2, 2, 2, 2, 2, 2, 2, 2], dtype=torch.int32),\n",
       "  'currency': tensor([2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2,\n",
       "          2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2,\n",
       "          2, 2, 2, 2, 2, 2, 2, 2, 2], dtype=torch.int32),\n",
       "  'trx_category': tensor([2, 2, 2, 3, 3, 2, 3, 4, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2,\n",
       "          2, 2, 2, 2, 2, 2, 2, 3, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 7, 3,\n",
       "          3, 2, 2, 2, 2, 2, 2, 2, 2], dtype=torch.int32),\n",
       "  'event_time': tensor([17382.0000, 17384.0000, 17384.0000, 17384.4893, 17386.8512, 17389.0000,\n",
       "          17389.6773, 17389.7142, 17390.0000, 17390.0000, 17394.0000, 17394.0000,\n",
       "          17394.0000, 17394.0000, 17396.0000, 17396.0000, 17397.0000, 17398.0000,\n",
       "          17399.0000, 17399.0000, 17400.0000, 17401.0000, 17401.0000, 17403.0000,\n",
       "          17403.0000, 17404.0000, 17406.0000, 17406.0000, 17406.0000, 17406.0000,\n",
       "          17407.0000, 17408.7536, 17416.0000, 17416.0000, 17416.0000, 17419.0000,\n",
       "          17419.0000, 17423.0000, 17423.0000, 17423.0000, 17424.0000, 17424.0000,\n",
       "          17424.0000, 17424.0000, 17425.0000, 17425.0000, 17427.0000, 17427.5851,\n",
       "          17427.5861, 17428.0000, 17428.0000, 17429.0000, 17430.0000, 17430.0000,\n",
       "          17432.0000, 17433.0000, 17434.0000], dtype=torch.float64)},\n",
       " 5)"
      ]
     },
     "execution_count": 11,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "train_dataset[5][0]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {},
   "outputs": [],
   "source": [
    "import functools\n",
    "import operator\n",
    "from collections import defaultdict\n",
    "\n",
    "import math\n",
    "from collections import OrderedDict\n",
    "from typing import Dict\n",
    "\n",
    "import torch\n",
    "import torch.nn as nn\n",
    "\n",
    "\n",
    "batch = [train_dataset[0], train_dataset[1]]\n",
    "\n",
    "batch1 = functools.reduce(operator.iadd, batch)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {},
   "outputs": [],
   "source": [
    "new_x_ = defaultdict(list)\n",
    "for x, _ in batch1:\n",
    "    for k, v in x.items():\n",
    "        new_x_[k].append(v)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "metadata": {},
   "outputs": [],
   "source": [
    "lengths = torch.IntTensor([len(e) for e in next(iter(new_x_.values()))])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "tensor([ 49,  22, 117, 118,  96,  62,  40,  47,  55,  62], dtype=torch.int32)"
      ]
     },
     "execution_count": 15,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "lengths"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "metadata": {},
   "outputs": [],
   "source": [
    "new_x = {k: torch.nn.utils.rnn.pad_sequence(v, batch_first=True) for k, v in new_x_.items()}"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "metadata": {},
   "outputs": [],
   "source": [
    "new_y = torch.tensor([y for _, y in batch1])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "metadata": {},
   "outputs": [],
   "source": [
    "class PaddedBatch:\n",
    "    def __init__(self, payload: Dict[str, torch.Tensor], length: torch.LongTensor):\n",
    "        self._payload = payload\n",
    "        self._length = length\n",
    "\n",
    "    @property\n",
    "    def payload(self):\n",
    "        return self._payload\n",
    "\n",
    "    @property\n",
    "    def seq_lens(self):\n",
    "        return self._length\n",
    "\n",
    "    def __len__(self):\n",
    "        return len(self._length)\n",
    "\n",
    "    def to(self, device, non_blocking=False):\n",
    "        length = self._length.to(device=device, non_blocking=non_blocking)\n",
    "        payload = {\n",
    "            k: v.to(device=device, non_blocking=non_blocking) for k, v in self._payload.items()\n",
    "        }\n",
    "        return PaddedBatch(payload, length)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 23,
   "metadata": {},
   "outputs": [],
   "source": [
    "from torch.utils.data import DataLoader\n",
    "\n",
    "def padded_collate(batch):\n",
    "    new_x_ = defaultdict(list)\n",
    "    for x, _ in batch:\n",
    "        for k, v in x.items():\n",
    "            new_x_[k].append(v)\n",
    "\n",
    "    lengths = torch.IntTensor([len(e) for e in next(iter(new_x_.values()))])\n",
    "\n",
    "    new_x = {k: torch.nn.utils.rnn.pad_sequence(v, batch_first=True) for k, v in new_x_.items()}\n",
    "    new_y = torch.tensor([y for _, y in batch])\n",
    "\n",
    "    return PaddedBatch(new_x, lengths), new_y\n",
    "\n",
    "def collate_splitted_rows(batch):\n",
    "    # flattens samples in list of lists to samples in list\n",
    "    batch = functools.reduce(operator.iadd, batch)\n",
    "    return padded_collate(batch)\n",
    "\n",
    "\n",
    "\n",
    "def create_data_loaders(conf):\n",
    "    train_data, valid_data = prepare_data(conf)\n",
    "\n",
    "    train_dataset = SplittingDataset(\n",
    "        train_data,\n",
    "        split_strategy.create(**conf.train.split_strategy)\n",
    "    )\n",
    "    train_dataset = TargetEnumeratorDataset(train_dataset)\n",
    "    train_dataset = ConvertingTrxDataset(train_dataset)\n",
    "    # не уверен что нам нужна история с дропаутом точек.\n",
    "    # Но это выглядит неплохой аугментацией в целом\n",
    "    train_dataset = DropoutTrxDataset(train_dataset, trx_dropout=conf.train.dropout,\n",
    "                                       seq_len=conf.train.max_seq_len)\n",
    "    \n",
    "    train_loader = DataLoader(\n",
    "        dataset=train_dataset,\n",
    "        shuffle=True,\n",
    "        collate_fn=collate_splitted_rows,\n",
    "        num_workers=1,\n",
    "        batch_size=3,\n",
    "    )\n",
    "\n",
    "    # valid_dataset = SplittingDataset(\n",
    "    #     valid_data,\n",
    "    #     split_strategy.create(**conf['params.valid.split_strategy'])\n",
    "    # )\n",
    "    # valid_dataset = TargetEnumeratorDataset(valid_dataset)\n",
    "    # valid_dataset = ConvertingTrxDataset(valid_dataset)\n",
    "    # valid_dataset = DropoutTrxDataset(valid_dataset, trx_dropout=0.0,\n",
    "    #                                   seq_len=conf['params.valid.max_seq_len'])\n",
    "    # valid_loader = DataLoader(\n",
    "    #     dataset=valid_dataset,\n",
    "    #     shuffle=False,\n",
    "    #     collate_fn=collate_splitted_rows,\n",
    "    #     num_workers=conf['params.valid'].get('num_workers', 0),\n",
    "    #     batch_size=conf['params.valid.batch_size'],\n",
    "    # )\n",
    "\n",
    "    return train_loader#, valid_loader"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 24,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "0it [00:00, ?it/s]"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "9717it [00:00, 15863.74it/s]\n"
     ]
    }
   ],
   "source": [
    "train_loader = create_data_loaders(conf)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 28,
   "metadata": {},
   "outputs": [],
   "source": [
    "for batch in train_loader:\n",
    "    break"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 29,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "(<__main__.PaddedBatch at 0x7f7741f4c100>,\n",
       " tensor([7824, 7824, 7824, 7824, 7824, 7662, 7662, 7662, 7662, 7662, 1959, 1959,\n",
       "         1959, 1959, 1959]))"
      ]
     },
     "execution_count": 29,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "batch"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.10.12"
  },
  "orig_nbformat": 4
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
