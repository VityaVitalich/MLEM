{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {
    "tags": []
   },
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/home/event_seq/experiments/rosbank/notebooks/../../../src/trainers/base_trainer.py:11: TqdmExperimentalWarning: Using `tqdm.autonotebook.tqdm` in notebook mode. Use `tqdm.tqdm` instead to force console mode (e.g. in jupyter console)\n",
      "  from tqdm.autonotebook import tqdm\n"
     ]
    }
   ],
   "source": [
    "import pandas as pd\n",
    "import numpy as np\n",
    "from tqdm import tqdm\n",
    "import pyarrow.parquet as pq\n",
    "import sys\n",
    "import torch\n",
    "import random\n",
    "import torch.nn as nn\n",
    "import torch.nn.functional as F\n",
    "import math\n",
    "from IPython.display import clear_output\n",
    "import logging\n",
    "from pathlib import Path\n",
    "\n",
    "import matplotlib.pyplot as plt\n",
    "sys.path.append('../../../')\n",
    "\n",
    "from configs.data_configs.rosbank import data_configs\n",
    "from configs.model_configs.gen.rosbank import model_configs\n",
    "from src.models.mTAND.model import MegaNet, MegaNetCE, MegaNetSupervised\n",
    "from src.data_load.dataloader import create_data_loaders, create_test_loader\n",
    "from src.trainers.trainer_mTAND import MtandTrainer\n",
    "\n",
    "from src.create_embeddings import create_embeddings\n",
    "\n",
    "from src.data_load import split_strategy\n",
    "from src.data_load.data_utils import prepare_data\n",
    "from src.data_load.splitting_dataset import (\n",
    "    ConvertingTrxDataset,\n",
    "    TargetDataset,\n",
    "    DropoutTrxDataset,\n",
    "    SplittingDataset,\n",
    "    TargetEnumeratorDataset,\n",
    ")\n",
    "from src.data_load.dataloader import collate_splitted_rows, padded_collate, PaddedBatch\n",
    "from torch.utils.data import DataLoader\n",
    "\n",
    "from sklearn.linear_model import LogisticRegression\n",
    "\n",
    "from src.models.preprocessors import FeatureProcessor\n",
    "\n",
    "from torch.autograd import Variable\n",
    "\n",
    "from src.models.gen_models import SeqGen"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "conf = data_configs()\n",
    "model_conf = model_configs()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {
    "tags": []
   },
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>cl_id</th>\n",
       "      <th>amount</th>\n",
       "      <th>event_time</th>\n",
       "      <th>mcc</th>\n",
       "      <th>channel_type</th>\n",
       "      <th>currency</th>\n",
       "      <th>trx_category</th>\n",
       "      <th>trx_count</th>\n",
       "      <th>target_target_flag</th>\n",
       "      <th>target_target_sum</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>10018</td>\n",
       "      <td>[10.609081944147828, 10.596659732783579, 10.81...</td>\n",
       "      <td>[17120.38773148148, 17133.667800925927, 17134....</td>\n",
       "      <td>[13, 2, 13, 2, 1, 18, 13, 2, 13, 2, 5, 13, 9, ...</td>\n",
       "      <td>[1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1]</td>\n",
       "      <td>[1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1]</td>\n",
       "      <td>[5, 3, 5, 3, 1, 1, 5, 3, 5, 3, 1, 5, 5, 5, 5]</td>\n",
       "      <td>15</td>\n",
       "      <td>0</td>\n",
       "      <td>0.0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>10030</td>\n",
       "      <td>[4.61512051684126, 6.90875477931522, 10.598857...</td>\n",
       "      <td>[17141.0, 17141.0, 17145.0, 17147.0, 17147.0, ...</td>\n",
       "      <td>[9, 9, 21, 1, 25, 6, 14, 14, 3, 3, 3, 13, 1, 3...</td>\n",
       "      <td>[1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, ...</td>\n",
       "      <td>[1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, ...</td>\n",
       "      <td>[5, 5, 1, 1, 1, 1, 1, 1, 1, 1, 1, 7, 1, 1, 3, ...</td>\n",
       "      <td>42</td>\n",
       "      <td>1</td>\n",
       "      <td>59.51</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>10038</td>\n",
       "      <td>[7.4127640174265625, 7.370230641807081, 7.8180...</td>\n",
       "      <td>[17301.0, 17301.0, 17301.0, 17301.774780092594...</td>\n",
       "      <td>[1, 1, 1, 2, 2, 4, 2, 8, 1, 22, 8, 1, 8, 4, 2,...</td>\n",
       "      <td>[1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, ...</td>\n",
       "      <td>[1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, ...</td>\n",
       "      <td>[1, 1, 1, 2, 2, 1, 3, 1, 1, 1, 1, 1, 1, 1, 2, ...</td>\n",
       "      <td>111</td>\n",
       "      <td>0</td>\n",
       "      <td>0.0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>10057</td>\n",
       "      <td>[7.494708263135679, 7.736394428979239, 10.7789...</td>\n",
       "      <td>[17151.0, 17151.0, 17153.0, 17154.0, 17155.0, ...</td>\n",
       "      <td>[6, 21, 2, 6, 2, 4, 2, 22, 15, 2, 1, 35, 4, 2,...</td>\n",
       "      <td>[1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, ...</td>\n",
       "      <td>[1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, ...</td>\n",
       "      <td>[1, 1, 4, 1, 4, 1, 3, 1, 1, 3, 1, 1, 1, 4, 1, ...</td>\n",
       "      <td>61</td>\n",
       "      <td>1</td>\n",
       "      <td>62961.31</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>10062</td>\n",
       "      <td>[8.31898612539206, 8.824824939175638, 6.509067...</td>\n",
       "      <td>[17143.0, 17143.0, 17143.0, 17144.0, 17144.0, ...</td>\n",
       "      <td>[80, 15, 37, 38, 11, 11, 2, 24, 7, 5, 5, 11, 1...</td>\n",
       "      <td>[1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, ...</td>\n",
       "      <td>[1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, ...</td>\n",
       "      <td>[1, 1, 1, 1, 1, 1, 3, 1, 1, 1, 1, 1, 1, 1, 1, ...</td>\n",
       "      <td>82</td>\n",
       "      <td>1</td>\n",
       "      <td>107126.35</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "   cl_id                                             amount  \\\n",
       "0  10018  [10.609081944147828, 10.596659732783579, 10.81...   \n",
       "1  10030  [4.61512051684126, 6.90875477931522, 10.598857...   \n",
       "2  10038  [7.4127640174265625, 7.370230641807081, 7.8180...   \n",
       "3  10057  [7.494708263135679, 7.736394428979239, 10.7789...   \n",
       "4  10062  [8.31898612539206, 8.824824939175638, 6.509067...   \n",
       "\n",
       "                                          event_time  \\\n",
       "0  [17120.38773148148, 17133.667800925927, 17134....   \n",
       "1  [17141.0, 17141.0, 17145.0, 17147.0, 17147.0, ...   \n",
       "2  [17301.0, 17301.0, 17301.0, 17301.774780092594...   \n",
       "3  [17151.0, 17151.0, 17153.0, 17154.0, 17155.0, ...   \n",
       "4  [17143.0, 17143.0, 17143.0, 17144.0, 17144.0, ...   \n",
       "\n",
       "                                                 mcc  \\\n",
       "0  [13, 2, 13, 2, 1, 18, 13, 2, 13, 2, 5, 13, 9, ...   \n",
       "1  [9, 9, 21, 1, 25, 6, 14, 14, 3, 3, 3, 13, 1, 3...   \n",
       "2  [1, 1, 1, 2, 2, 4, 2, 8, 1, 22, 8, 1, 8, 4, 2,...   \n",
       "3  [6, 21, 2, 6, 2, 4, 2, 22, 15, 2, 1, 35, 4, 2,...   \n",
       "4  [80, 15, 37, 38, 11, 11, 2, 24, 7, 5, 5, 11, 1...   \n",
       "\n",
       "                                        channel_type  \\\n",
       "0      [1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1]   \n",
       "1  [1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, ...   \n",
       "2  [1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, ...   \n",
       "3  [1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, ...   \n",
       "4  [1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, ...   \n",
       "\n",
       "                                            currency  \\\n",
       "0      [1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1]   \n",
       "1  [1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, ...   \n",
       "2  [1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, ...   \n",
       "3  [1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, ...   \n",
       "4  [1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, ...   \n",
       "\n",
       "                                        trx_category  trx_count  \\\n",
       "0      [5, 3, 5, 3, 1, 1, 5, 3, 5, 3, 1, 5, 5, 5, 5]         15   \n",
       "1  [5, 5, 1, 1, 1, 1, 1, 1, 1, 1, 1, 7, 1, 1, 3, ...         42   \n",
       "2  [1, 1, 1, 2, 2, 1, 3, 1, 1, 1, 1, 1, 1, 1, 2, ...        111   \n",
       "3  [1, 1, 4, 1, 4, 1, 3, 1, 1, 3, 1, 1, 1, 4, 1, ...         61   \n",
       "4  [1, 1, 1, 1, 1, 1, 3, 1, 1, 1, 1, 1, 1, 1, 1, ...         82   \n",
       "\n",
       "  target_target_flag target_target_sum  \n",
       "0                  0               0.0  \n",
       "1                  1             59.51  \n",
       "2                  0               0.0  \n",
       "3                  1          62961.31  \n",
       "4                  1         107126.35  "
      ]
     },
     "execution_count": 3,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "df = pd.read_parquet(conf.train_supervised_path)\n",
    "df.head()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [],
   "source": [
    "gen = pd.read_parquet('/home/event_seq/experiments/rosbank/gen/ckpt/generated_data/test_2023-10-06_15:07:22')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 133,
   "metadata": {},
   "outputs": [],
   "source": [
    "gen['code'] = gen['event_time'].apply(sum)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 134,
   "metadata": {},
   "outputs": [],
   "source": [
    "df['code'] = df['event_time'].apply(lambda x: x[1:].sum())"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 135,
   "metadata": {},
   "outputs": [],
   "source": [
    "mapping = {}\n",
    "for i in range(len(gen)):\n",
    "    if np.isclose(gen['code'][i], df['code'], atol=1e-5).sum() == 1:\n",
    "        mask = np.isclose(gen['code'][i], df['code'], atol=1e-5)\n",
    "        #print(gen['target_target_flag'][i], df['target_target_flag'][mask].values[0])\n",
    "        if (gen['target_target_flag'][i] == int(df['target_target_flag'][mask].values[0])):\n",
    "            if abs(gen['code'][i] - df['code'][mask].values[0]) < 1e-5:\n",
    "                mapping[i] = df['target_target_flag'][mask].index[0]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 136,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "0.9511666972515144\n",
      "1.8878176075131645\n",
      "1.0234908236712974\n",
      "1.1418171614198176\n",
      "0.8859926109812003\n",
      "0.8411344068253463\n",
      "1.3202936412579669\n",
      "1.8369061396185908\n",
      "2.0071834210052693\n",
      "1.3935551716935046\n",
      "0.8854411743413205\n",
      "1.420195661424216\n",
      "0.7767697318317864\n",
      "1.1155673985677101\n",
      "1.336031126554382\n",
      "1.3776052680968551\n",
      "1.750857847253303\n",
      "0.5341149847021762\n",
      "1.061668143426285\n",
      "0.9401830097842467\n",
      "1.2716436487251594\n",
      "0.7502892205605944\n",
      "1.4030230684833827\n",
      "1.0366119479107532\n",
      "1.5167747912658263\n",
      "1.5339347818712796\n",
      "1.359216972489872\n",
      "1.1577305238196134\n",
      "0.7390862149300411\n",
      "0.8106262812390548\n",
      "2.5551045179558036\n",
      "1.3536853636840034\n",
      "0.8688939741459528\n",
      "1.1508914468950253\n",
      "1.3613846237021199\n",
      "0.7937202409879947\n",
      "0.8541279056968644\n",
      "0.9315014836007286\n",
      "1.9108659403920425\n",
      "1.7749649464859638\n",
      "1.1189922458352881\n",
      "1.4512877595013975\n",
      "1.1357542998164238\n",
      "0.9636332209080171\n",
      "1.5518885249142813\n",
      "1.5612043016140424\n",
      "0.9702518378077687\n",
      "1.310256544842529\n",
      "1.122874576403451\n",
      "0.932448582577139\n",
      "1.275369317925783\n",
      "0.795686714318282\n",
      "0.5758954187329028\n",
      "1.3114682967647717\n",
      "1.7828082906043272\n",
      "1.41834815895664\n",
      "2.0198024482007644\n",
      "1.2543260255780486\n",
      "0.7306327492480638\n",
      "1.0770145841749466\n",
      "1.0035133651044785\n",
      "1.4209692505870284\n",
      "0.44491825583941874\n",
      "0.8105328586640873\n",
      "0.9744217912816703\n",
      "1.5951051890679147\n",
      "1.6893377316234142\n",
      "0.6429112748288109\n",
      "1.445805634160454\n",
      "0.6833861695389885\n",
      "1.2779860631120994\n",
      "1.1723242329576324\n",
      "1.0107171691259913\n",
      "1.8615053861376891\n",
      "1.3819572290071285\n",
      "0.7274546248203776\n",
      "1.6881467075345462\n",
      "1.5890378547253725\n",
      "1.0246425792473661\n",
      "0.8904905590167642\n",
      "2.464075637973774\n",
      "1.3033704469097633\n",
      "0.9881821101309373\n",
      "2.2962714403485385\n",
      "1.1399184787797543\n",
      "1.3228718643852098\n",
      "1.2803334476097978\n",
      "1.180538767856368\n",
      "1.30034459592882\n",
      "1.2838445899902822\n",
      "0.7906098652032083\n",
      "1.1635781676194012\n",
      "1.1304619149582915\n",
      "0.9824297584867085\n",
      "1.0487345106757977\n",
      "0.9864936508858486\n",
      "1.447850165187831\n",
      "1.7060788097658115\n",
      "2.493655996943232\n",
      "1.3849589721157596\n",
      "1.182274299210176\n",
      "1.4836358217139287\n",
      "1.7967244726947782\n",
      "0.9645015953743344\n",
      "1.2913361342133125\n",
      "1.5386469277800436\n",
      "1.6020476546096234\n",
      "1.2916947597387405\n",
      "1.6811608546013312\n",
      "1.2890890317538926\n",
      "2.574013331974756\n",
      "0.9830117596022697\n",
      "1.2430950034067048\n",
      "1.0671657086181543\n",
      "1.7985279278049897\n",
      "1.4190972528521921\n",
      "1.2624790626746822\n",
      "1.3165618999431477\n",
      "1.901987426506946\n",
      "1.2106462390496506\n",
      "0.8122386013484042\n",
      "1.0426993477678608\n",
      "1.536401993704563\n",
      "1.2997037274866003\n",
      "1.5164775084322188\n",
      "1.3422040701238676\n",
      "1.191684782996144\n",
      "0.9062409378581073\n",
      "1.219163424944956\n",
      "1.1560251753436892\n",
      "1.2942035784448438\n",
      "2.288964958778808\n",
      "0.7819515952130742\n",
      "1.132661502495274\n",
      "1.2655981460960486\n",
      "0.8123003809553293\n",
      "0.8532275986030039\n",
      "1.151462551970573\n",
      "0.8033483316501708\n",
      "1.0566691002542716\n",
      "1.382168023219546\n",
      "1.533507749965592\n",
      "0.5431753796175686\n",
      "1.03757256211513\n",
      "0.9990594866670429\n",
      "0.7291391929725013\n",
      "1.5853508563308696\n",
      "1.0476850937596054\n",
      "2.0047412286650443\n",
      "0.9691330986081075\n",
      "1.226928636163984\n",
      "0.9224159916717036\n",
      "1.0977069564005217\n",
      "1.7320237429931769\n",
      "1.774877621147278\n",
      "2.1431235474706054\n",
      "2.1408134281016244\n",
      "1.4695002113566438\n",
      "0.8796311032753618\n",
      "1.2068298969393096\n",
      "1.9995004515149724\n",
      "1.4147139486581677\n",
      "1.3432044946521569\n",
      "1.8627048272504103\n",
      "1.3206026915169424\n",
      "2.0175266237834095\n",
      "1.8731569362506448\n",
      "0.7436138048047454\n",
      "1.182849883553872\n",
      "1.9732191225880926\n",
      "1.725715954470951\n",
      "1.1987322316485913\n",
      "1.1777251505523736\n",
      "0.6621955865660974\n",
      "0.6707355918082664\n",
      "1.1699760653942242\n",
      "1.7062322260713816\n",
      "2.0318774741344887\n",
      "1.0002533753910052\n",
      "1.0514471063326722\n",
      "1.599227800537377\n",
      "1.1579877093646171\n",
      "1.4059933014899695\n",
      "1.5245701572987982\n",
      "1.0168165430836675\n",
      "0.984543271172343\n",
      "0.4220923425460942\n",
      "0.6690398042921346\n",
      "1.2850561089325618\n",
      "0.8564950254125614\n",
      "1.4687144043192046\n",
      "0.9965589641244363\n",
      "1.1356184778516796\n",
      "1.124490701672764\n",
      "0.9098942576109557\n",
      "0.7745328182051194\n",
      "1.112513246776848\n",
      "1.3129961277130044\n",
      "2.145046105559258\n",
      "0.8229412440837051\n",
      "1.261427033641611\n",
      "0.4517403609711168\n",
      "1.206611466407896\n",
      "1.2239991528467438\n",
      "1.4261441963923387\n",
      "1.1732984476231763\n",
      "1.278821832644109\n",
      "1.4165903146871635\n",
      "1.052064422165938\n",
      "0.7517774082213489\n",
      "1.2309479619936432\n",
      "1.382281165153753\n",
      "1.4997266441237822\n",
      "2.124525613736465\n",
      "1.2304719746663833\n",
      "1.159219628124699\n",
      "1.1205423974573876\n",
      "0.9744760993577772\n",
      "1.081258616177856\n",
      "0.586864088227389\n",
      "1.070370784440869\n",
      "1.3213304047007917\n",
      "1.9316801686322649\n",
      "1.9216707700707278\n",
      "1.3326856186751903\n",
      "1.3172159366072633\n",
      "2.4236714342567978\n",
      "0.9522015270078562\n",
      "1.1330433495122716\n",
      "1.1003859069568462\n",
      "1.4650901170190391\n",
      "1.27675808130872\n",
      "0.9107607232809853\n",
      "1.5632833491347704\n",
      "1.2453855844574953\n",
      "1.9506319858758574\n",
      "1.130040324508083\n",
      "0.6335201678608902\n",
      "1.3834048318615548\n",
      "1.1165351907809433\n",
      "0.6245450616270041\n",
      "2.3043099332278745\n",
      "1.5828666197584935\n",
      "1.9185225705047642\n",
      "1.433044806067118\n",
      "1.628543075621302\n",
      "1.0893280021491751\n",
      "1.298960407909663\n",
      "1.2689643663066856\n",
      "1.7706288375868542\n",
      "1.6117282755095828\n",
      "2.802574392795042\n",
      "0.8536644946212228\n",
      "0.9635991912615253\n",
      "1.2099383491634403\n",
      "1.5100457728797676\n",
      "0.8088024696736081\n",
      "1.5428007370307655\n",
      "1.210474170249225\n",
      "1.174967954613836\n",
      "1.1362619140632166\n",
      "0.9118474232520658\n",
      "0.5804918926556946\n",
      "1.2570029063370776\n",
      "1.156935971446306\n",
      "0.6083159015921654\n",
      "1.1858546524720408\n",
      "1.2450613396417443\n",
      "1.2153885005346339\n",
      "1.8430693895426289\n",
      "1.049780448378879\n",
      "1.2764467325473272\n",
      "1.039984352240878\n",
      "1.4530740276611793\n",
      "1.0424995271987436\n",
      "1.2004084810160875\n",
      "2.456319714261769\n",
      "1.2584475491605487\n",
      "1.091068932890822\n",
      "1.4018428163938246\n",
      "1.5852009454009266\n",
      "0.8330550548910832\n",
      "0.8712696288248639\n",
      "1.661630444459832\n",
      "1.0722859133135767\n",
      "0.8689259810578391\n",
      "1.1527736579195642\n",
      "2.5840556846400515\n",
      "1.3480688696105638\n",
      "1.1517562056218777\n",
      "0.886899378333425\n",
      "0.823081564771607\n",
      "1.8999441418657372\n",
      "1.3558870010755542\n",
      "1.6634589631064336\n",
      "1.9404562881394318\n",
      "0.987108647052763\n",
      "1.2847140543832374\n",
      "1.092592389392536\n",
      "1.9448496637755361\n",
      "1.4503325946181802\n",
      "1.1851640822648881\n",
      "1.4743121129448498\n",
      "1.1246656342998842\n",
      "1.0512751772906872\n",
      "1.5803717153130414\n",
      "1.1280059963430469\n",
      "0.9975620251636547\n",
      "1.0046219006923165\n",
      "1.355233112282799\n",
      "1.1630305597418222\n",
      "1.1727767776174731\n",
      "0.947124763360053\n",
      "0.9155113809129696\n",
      "1.134070621788098\n",
      "1.0874157050925528\n",
      "1.78267998772149\n",
      "1.1309384323771197\n",
      "1.6066475937579616\n",
      "1.2816635062599175\n",
      "1.492208608761677\n",
      "0.8438977366321085\n",
      "1.2319117702129954\n",
      "1.191362069467015\n",
      "2.2557821350313594\n",
      "1.3553555265202224\n",
      "1.2452234609990638\n",
      "0.6314714572611115\n",
      "1.101469940115282\n",
      "2.291390607944316\n",
      "0.897731076787202\n",
      "1.1488518981756555\n",
      "1.0849075834495445\n",
      "1.5328774470977593\n",
      "1.1509328119600977\n",
      "1.014128584663742\n",
      "1.6478995519826727\n",
      "0.8999000306075641\n",
      "0.6204082688536009\n",
      "0.6765701716067883\n",
      "1.260159530595916\n",
      "1.0295723409131945\n",
      "0.8345142776846539\n",
      "1.0441087275599998\n",
      "1.8688413293265689\n",
      "1.1388044940556667\n",
      "0.9820737165611242\n",
      "0.8957130669627064\n",
      "1.3380994197769789\n",
      "1.6239695980176305\n",
      "1.2409236152577867\n",
      "0.676476389550955\n",
      "1.141615331819723\n",
      "1.0112175707467244\n",
      "0.9860261588339545\n",
      "1.1242124714835802\n",
      "1.4000397010846368\n",
      "1.0181572503366425\n",
      "1.177286342731989\n",
      "1.4509210762673923\n",
      "1.2695344442448009\n",
      "1.3372430622296665\n",
      "1.3521795592619545\n",
      "1.1755044963778947\n",
      "0.9456688998297065\n",
      "1.4361019128531207\n",
      "1.0089499291036472\n",
      "0.9742920523230368\n",
      "1.0648046209263096\n",
      "1.2778725324635627\n",
      "1.123365493459393\n",
      "0.6658269718113323\n",
      "1.6492861569255621\n",
      "0.8539866262252018\n",
      "0.9744381694407489\n",
      "1.0005554014915574\n",
      "1.0373748138170538\n",
      "1.0250999214741394\n",
      "0.950923245648128\n",
      "1.2153790361491557\n",
      "1.462206438031978\n",
      "1.1270298587223422\n",
      "1.3931454734073196\n",
      "1.2809027375463378\n",
      "1.4596441349197589\n",
      "1.3880649612507248\n",
      "1.0981975386908318\n",
      "0.7746393903569025\n",
      "1.3689346889607787\n",
      "1.0361899135818806\n",
      "2.2042949137796164\n",
      "1.5952045749474313\n",
      "1.1203392946445498\n",
      "2.4936117039477286\n",
      "1.2139022052057007\n",
      "0.851016012085274\n",
      "2.369952031847386\n",
      "0.8278746133483407\n",
      "1.433264586730959\n",
      "2.3425675935420798\n",
      "1.0570386558359848\n",
      "1.978757447583504\n",
      "0.8876397625142941\n",
      "1.94457529555241\n",
      "1.03228608951967\n",
      "1.4786127491252785\n",
      "0.9786669217199664\n",
      "1.5697651671090995\n",
      "0.9617020456687412\n",
      "0.5896618532591337\n",
      "2.257973628330248\n",
      "1.0022855121407863\n",
      "0.45670058903367583\n",
      "1.2220203278727486\n",
      "1.1186260087747226\n",
      "1.252540590161459\n",
      "1.0474003676434582\n",
      "1.8672289655250118\n",
      "1.6807940668618055\n",
      "0.8386045962327262\n",
      "1.1620334668388437\n",
      "0.6768602712932464\n",
      "0.6562497371336641\n",
      "1.5100846997938624\n",
      "1.8044257349779746\n",
      "0.9997856698301737\n",
      "1.5055308552863318\n",
      "1.5590257135903371\n",
      "1.517263135047577\n",
      "1.2458103996734584\n",
      "0.8070417995783598\n",
      "1.0535941945444272\n",
      "1.989589930001145\n",
      "2.3483824866239194\n",
      "1.0384331161604923\n",
      "2.624674833002143\n",
      "1.2491633632642192\n",
      "1.1148976448713834\n",
      "1.1950903561889077\n",
      "2.691218779398579\n",
      "1.9372413512432483\n",
      "1.5608138348846443\n",
      "3.0714916808323456\n",
      "1.3461850632216408\n",
      "0.6574286981986275\n",
      "1.0688881086743853\n",
      "1.2644184659449595\n",
      "1.7076752852066046\n",
      "1.1521250065710273\n",
      "1.4424071436656767\n",
      "1.4551566830147835\n",
      "1.7170944025722672\n",
      "2.016976389166257\n",
      "1.5031443421602522\n",
      "0.9066349600487711\n",
      "1.2993966913458534\n",
      "1.0494479355472732\n",
      "1.5861995229494992\n",
      "1.1394403444219403\n",
      "0.9290300869850864\n",
      "2.048003511420254\n",
      "1.7188635689961316\n",
      "1.3914446602658\n",
      "0.942266338627728\n",
      "0.8421678740406502\n",
      "1.056101972125365\n",
      "1.3690471098460646\n",
      "1.0669619084418163\n",
      "0.8807411578866468\n",
      "1.1908422799986709\n",
      "1.224761282133962\n",
      "1.4151644071117109\n",
      "1.1388721602537666\n",
      "0.6860765435687559\n",
      "1.1438288442603062\n",
      "1.7646445695251423\n",
      "1.4179437746362276\n",
      "1.3579287526895538\n",
      "1.3025065876255573\n",
      "1.9182051367982473\n",
      "1.1650903318135986\n",
      "1.2836073092215374\n",
      "1.51911700631395\n",
      "0.9843264583639852\n",
      "1.1014674627573078\n",
      "1.4416395152863648\n",
      "0.8626296327490559\n",
      "0.7778035004091727\n",
      "0.406605209329123\n",
      "2.6614696100268755\n",
      "1.0273431944174294\n",
      "1.4345309536766342\n",
      "1.3654796729488834\n",
      "0.884278885853041\n",
      "1.1793194133060743\n",
      "1.0671312782793567\n",
      "0.9983252744411011\n",
      "0.8569045909071367\n",
      "0.47326877751737995\n",
      "1.428789861279243\n",
      "1.434935779178182\n",
      "1.2741986668685714\n",
      "1.9193414845610393\n",
      "1.191036331365116\n",
      "1.0071488972427118\n",
      "1.3031520786289028\n",
      "1.0712961298811572\n",
      "0.869140531336933\n",
      "1.2802419080256593\n",
      "1.0285969183088968\n",
      "0.6049764900181117\n",
      "1.527948114067877\n",
      "1.2962448257972432\n",
      "0.9227140764475628\n",
      "1.1487752715011084\n",
      "1.1357497903106895\n",
      "1.046310422758976\n",
      "1.2721551018716626\n",
      "1.2579593942707143\n",
      "1.568146120561416\n",
      "0.7972783740259898\n",
      "1.1703932045519794\n",
      "0.9656183573046768\n",
      "0.141518189250883\n",
      "1.112526225379423\n",
      "0.30208365433509404\n",
      "1.1512298613988987\n",
      "1.487590159546678\n",
      "0.7743703029936573\n",
      "1.3630313549292468\n",
      "1.350160220901235\n",
      "1.2413026045278561\n",
      "1.696949504517462\n",
      "1.143831057496221\n",
      "1.083795522261555\n",
      "1.532939669000844\n",
      "1.4198992098812342\n",
      "0.8755099451747145\n",
      "1.3618554738086697\n",
      "1.5146423511421152\n",
      "1.5404592399315955\n",
      "1.8427296434775762\n",
      "1.174611911211126\n",
      "1.2017650852633814\n",
      "1.2899991531228674\n",
      "1.4313295039025429\n",
      "1.030867857667237\n",
      "2.2826552811784286\n",
      "1.1239484532122372\n",
      "1.0803121746246034\n",
      "1.4290099039844268\n",
      "0.3406294454112696\n",
      "1.031946234385799\n",
      "0.9598341940495585\n",
      "1.464779481269064\n",
      "1.5368162262798748\n",
      "0.9492246140373499\n",
      "1.6377842787684331\n",
      "1.2753348446043387\n",
      "1.6878187889739849\n",
      "0.858473820169666\n",
      "0.5636692893076556\n",
      "1.421572375541533\n",
      "1.2811934611183038\n",
      "1.4140619790323259\n",
      "1.134707245507858\n",
      "1.31192088270653\n",
      "1.2602262820310541\n",
      "2.211116377944951\n",
      "2.180107329474993\n",
      "1.4040517934384025\n",
      "1.640739106962139\n",
      "1.302470248780315\n",
      "1.448442686625438\n",
      "1.2506321851089288\n",
      "1.413799676785966\n",
      "2.977708260184573\n",
      "1.3346163031988345\n",
      "1.051460787023275\n",
      "1.9915869498110779\n",
      "2.6062172809521726\n",
      "1.0703300321829505\n",
      "1.0294713972258678\n",
      "1.071304478507974\n",
      "1.2168588037298895\n",
      "1.2770126998717197\n",
      "1.2417174959916009\n",
      "1.1302545906689538\n",
      "1.6369790114397322\n",
      "2.1209250227881444\n",
      "2.5619852511164685\n",
      "1.064959195829315\n",
      "0.8457376759427829\n",
      "0.8800793532604622\n",
      "0.2223017321046168\n",
      "1.6721215054489957\n",
      "2.6029670262302\n",
      "0.6818780024984074\n",
      "1.0157163505473676\n",
      "1.5006075162730872\n",
      "1.6433588907940297\n",
      "1.8289795359816199\n",
      "1.3882088056727877\n",
      "1.1027850482478796\n",
      "1.8602354971614579\n",
      "1.6687023160235002\n",
      "1.5751605217319975\n",
      "1.5645578386418717\n",
      "1.517681972257856\n",
      "1.5794879556533314\n",
      "1.1368559697789216\n",
      "0.9437857494838271\n",
      "1.9479900673889419\n",
      "1.8663049036813801\n",
      "1.0190794392071918\n",
      "1.3400453362803588\n",
      "0.7849100721441257\n",
      "1.7312350508143828\n",
      "1.3027583345121825\n",
      "1.6188907670002846\n",
      "3.204402054866601\n",
      "1.4283674229826189\n",
      "0.9979984343695569\n",
      "0.8658243930934101\n",
      "1.3412978706111973\n",
      "1.434135582129242\n",
      "0.6908709554992661\n",
      "1.5522606452414065\n",
      "2.3751687417276055\n",
      "0.5589684899884508\n",
      "1.259505398363513\n",
      "1.9222332425963014\n",
      "1.0683884075769063\n",
      "0.65676196132695\n",
      "1.1432279659500035\n",
      "1.2454803139121107\n",
      "1.0389760997325863\n",
      "2.0164037491637212\n",
      "0.7259625845015574\n",
      "1.482962789242131\n",
      "1.7660685720985945\n",
      "1.2378209039751695\n",
      "1.5080433102616295\n",
      "0.759954623602447\n",
      "1.7036865556308476\n",
      "0.9886975713870283\n",
      "0.6684723230109795\n",
      "0.8939951930547965\n",
      "1.7805428457814387\n",
      "1.2181095369902073\n",
      "1.0082979082274317\n",
      "1.7137840800904283\n",
      "1.067051174280369\n",
      "1.312249346094229\n",
      "1.0854336257924637\n",
      "1.0858087385178614\n",
      "1.0440677267232716\n",
      "0.7052880853912556\n",
      "1.2015220730232785\n",
      "1.56513860808101\n",
      "1.4606375076853166\n",
      "1.1754085150937719\n",
      "1.5034638107594598\n",
      "1.08570624747135\n",
      "1.0210352311021673\n",
      "1.1826935772871905\n",
      "1.4754505069018895\n",
      "0.7700384940158906\n",
      "2.0210265389070567\n",
      "1.0166870876550347\n",
      "1.1151315419786163\n",
      "1.5520338233908741\n",
      "1.3278655782523205\n",
      "0.48733091423499586\n",
      "1.9449759795877302\n",
      "1.5934749010914824\n",
      "1.0110897542620714\n",
      "1.1405496369920471\n",
      "1.9438922038020918\n",
      "1.6008523321477894\n",
      "1.0221171517883947\n",
      "1.3996978797224104\n",
      "1.6349631592928677\n",
      "1.2761554294294641\n",
      "1.6960915193160175\n",
      "2.0854568359086345\n",
      "1.2836982427141679\n",
      "0.7758570494841431\n",
      "1.440916522368671\n",
      "1.1379133083294333\n",
      "0.7958596661322984\n",
      "1.4903257788021604\n",
      "0.7443722381658813\n",
      "2.094365559812847\n",
      "1.2709973741822418\n",
      "1.2386360830682581\n",
      "0.35686040343863557\n",
      "0.7333258644233913\n",
      "1.0448795010856222\n",
      "1.3530146650397379\n",
      "1.3502232331609503\n",
      "0.5974534664737153\n",
      "1.0761421984206523\n",
      "0.9594785686820262\n",
      "1.5401796586643284\n",
      "1.3112668563929415\n",
      "0.8116293171081448\n",
      "1.1709432337074566\n",
      "1.7562094864028566\n",
      "1.0799196671946238\n",
      "1.1845675398644202\n",
      "1.2625364002493797\n",
      "0.9936635962720146\n",
      "1.2137362866228525\n",
      "1.4639915076332655\n",
      "1.4741809385569962\n",
      "1.2423170921738302\n",
      "1.881941979297193\n",
      "2.835522194712443\n",
      "1.6566561109207127\n",
      "0.907559280261064\n",
      "1.2611856253195184\n",
      "0.9574576257275922\n",
      "1.429955536812307\n",
      "0.8379466712097315\n",
      "1.7018004453294682\n",
      "0.9807674414854641\n",
      "1.9485497450678666\n",
      "1.1277009089181893\n",
      "1.4553200806649478\n",
      "1.819827895421304\n",
      "1.3311475596831572\n",
      "0.9823160936207032\n",
      "1.1523190340086402\n",
      "0.9195379850437875\n",
      "0.885113183348149\n",
      "1.062944876562191\n",
      "1.2847376540996447\n",
      "1.3338612241252334\n",
      "1.2645549402724903\n",
      "1.1040747851234165\n",
      "1.0871230315129512\n",
      "0.7711786441491874\n",
      "1.2498407565467904\n",
      "1.8149589978120835\n",
      "1.110851947071938\n",
      "1.453486306081743\n",
      "1.3803453512419752\n",
      "0.8551925113762\n",
      "1.248994322513838\n",
      "3.2159882081465154\n",
      "1.6550403782821306\n",
      "0.8757568772673782\n",
      "1.2771549560021487\n",
      "1.1028652874509408\n",
      "1.1988298076902526\n",
      "0.6412493584877567\n",
      "1.2272469368786245\n",
      "0.98429226850286\n",
      "1.1497498233460164\n",
      "1.6268384174586417\n",
      "1.171553747827002\n",
      "1.4391534927267762\n",
      "0.2512225381413179\n",
      "1.0186438163094447\n",
      "1.2604324259132011\n",
      "1.0059453773187161\n",
      "1.145016753690348\n",
      "1.0903537657800864\n",
      "0.9888742518861614\n",
      "1.5164017454364422\n",
      "1.2481647284740922\n",
      "1.6015458921339571\n",
      "0.989277234511941\n",
      "1.3339199494457563\n",
      "1.7953559428081094\n",
      "1.438041122346743\n",
      "0.9991710527599972\n",
      "2.7042119229538\n",
      "1.2038585833083122\n",
      "1.1607783596758743\n",
      "0.910826176993793\n",
      "1.7112056545752157\n",
      "1.4176544541547162\n",
      "1.2242987890011778\n",
      "1.490426684644935\n",
      "1.090687040122719\n",
      "1.1747539066105517\n",
      "1.0778625951705763\n",
      "1.180962295175372\n",
      "0.5511871308796558\n",
      "1.6051173368388607\n",
      "1.4609585593835384\n",
      "1.4169671718956536\n",
      "1.3452736315947094\n",
      "1.037966042162456\n",
      "1.5878489308273938\n",
      "1.4770588985162187\n",
      "2.137997813838102\n",
      "1.3744771266080336\n",
      "1.273830165388389\n",
      "1.2093498364254955\n",
      "0.9545188373145219\n",
      "1.2409607583590323\n",
      "1.0217326966496847\n",
      "0.8531750354359396\n",
      "2.345479390615581\n",
      "0.9774732921050544\n",
      "1.0297520661374067\n",
      "0.632620931710649\n",
      "1.3898268070512314\n",
      "1.7130100525153313\n",
      "0.48372309039181727\n",
      "1.4522414336509855\n",
      "1.6064540443579656\n",
      "1.2422532169799936\n",
      "1.1128668722705368\n",
      "1.1098548335312801\n",
      "1.0086921049782727\n",
      "0.4547378442414523\n",
      "0.6530816109852965\n",
      "1.7778404018257585\n",
      "1.786106050723837\n",
      "1.3454230679274666\n",
      "1.3090299611649798\n",
      "1.3090556876658472\n",
      "1.4116060307444802\n",
      "1.024117900890837\n",
      "1.5735538966617322\n",
      "1.2017036222685997\n",
      "1.2995302829340563\n",
      "1.4585489679347106\n",
      "0.9904353907487722\n",
      "1.4772471791615664\n",
      "1.4980021501799359\n",
      "1.4790897488283397\n",
      "1.3013398824462379\n",
      "0.6366046804958264\n",
      "1.0092811969252062\n",
      "1.5720408204593685\n",
      "1.8029061276300395\n",
      "1.2295791234862758\n"
     ]
    }
   ],
   "source": [
    "for gen_id, df_id in mapping.items():\n",
    "    (gen['code'][gen_id] - df.loc[df_id]['code'])\n",
    "    print(np.abs(gen.loc[gen_id]['amount'] - df.loc[df_id]['amount'][1:]).mean())\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 137,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "(array([1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1,\n",
       "        1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1,\n",
       "        1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1,\n",
       "        1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1]),\n",
       " array([1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1,\n",
       "        1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1,\n",
       "        1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1,\n",
       "        1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1], dtype=int32))"
      ]
     },
     "execution_count": 137,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "gen.loc[gen_id]['channel_type'], df.loc[df_id]['channel_type']"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 150,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "(array([1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1,\n",
       "        1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1,\n",
       "        1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1,\n",
       "        1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1]),\n",
       " array([ 9,  1, 36, 11, 16, 15, 41,  1,  1, 14,  3,  7,  7, 16, 16,  1, 14,\n",
       "         7, 14,  1, 13, 13, 13, 11, 11, 14, 11,  1,  1, 11,  5, 36,  1, 11,\n",
       "        11,  1, 16,  7,  1,  1,  6, 14,  7,  1,  1, 11,  1,  1, 13,  1,  7,\n",
       "        11,  7, 14,  1, 20,  1,  1, 14, 11,  1, 16, 16, 13, 14,  1,  1,  1,\n",
       "        11,  1, 14,  1, 14,  1, 11,  8,  8,  1,  8, 13,  5,  2,  2],\n",
       "       dtype=int32))"
      ]
     },
     "execution_count": 150,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "gen.loc[gen_id]['mcc'], df.loc[df_id]['mcc']"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 139,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "(array([1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1,\n",
       "        1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1,\n",
       "        1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1,\n",
       "        1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1]),\n",
       " array([1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1,\n",
       "        1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1,\n",
       "        1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1,\n",
       "        1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1], dtype=int32))"
      ]
     },
     "execution_count": 139,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "gen.loc[gen_id]['currency'], df.loc[df_id]['currency']"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 149,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "array([ 0, 79, 58,  0, 36])"
      ]
     },
     "execution_count": 149,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "np.bincount(gen['mcc'][6])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 146,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "array([1, 1, 1, 1, 1, 2, 1, 2, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1,\n",
       "       1, 1, 2, 2, 1, 1, 1, 1, 1, 1, 1, 1, 2, 1, 1, 1, 1, 1, 1, 1, 1, 1,\n",
       "       1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 2, 2, 2, 1, 2, 1, 1, 1, 4, 1, 1, 1,\n",
       "       2, 1, 1, 2, 1, 1, 2, 1, 2, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1,\n",
       "       1, 2, 1, 2, 1, 1, 1, 1, 2, 1, 2, 1])"
      ]
     },
     "execution_count": 146,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "gen['mcc'][0]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 115,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "0"
      ]
     },
     "execution_count": 115,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "df['amount'].apply(lambda x: sum(x<0)).sum()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 124,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "0           [1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1]\n",
       "1       [1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, ...\n",
       "2       [1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, ...\n",
       "3       [1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, ...\n",
       "4       [1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, ...\n",
       "                              ...                        \n",
       "4495    [1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, ...\n",
       "4496    [1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, ...\n",
       "4497    [2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, ...\n",
       "4498    [1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, ...\n",
       "4499    [1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, ...\n",
       "Name: channel_type, Length: 4500, dtype: object"
      ]
     },
     "execution_count": 124,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "df['channel_type']"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 127,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "0"
      ]
     },
     "execution_count": 127,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "gen['channel_type'].apply(lambda x: sum(x == 1)).sum()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "1698it [00:00, 16972.85it/s]"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "4500it [00:00, 19468.85it/s]\n"
     ]
    }
   ],
   "source": [
    "train_loader, valid_loader = create_data_loaders(conf)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [],
   "source": [
    "for batch in train_loader:\n",
    "    break"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "tensor([[ 0.1510,  0.1514,  0.1731,  ..., -1.0000, -1.0000, -1.0000],\n",
       "        [ 0.1750,  0.1764,  0.1779,  ..., -1.0000, -1.0000, -1.0000],\n",
       "        [ 0.1805,  0.1823,  0.1823,  ..., -1.0000, -1.0000, -1.0000],\n",
       "        ...,\n",
       "        [ 0.4954,  0.5009,  0.5046,  ..., -1.0000, -1.0000, -1.0000],\n",
       "        [ 0.4825,  0.4862,  0.4881,  ..., -1.0000, -1.0000, -1.0000],\n",
       "        [ 0.5323,  0.5323,  0.5323,  ..., -1.0000, -1.0000, -1.0000]],\n",
       "       dtype=torch.float64)"
      ]
     },
     "execution_count": 17,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "batch[0].payload['event_time']"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "0"
      ]
     },
     "execution_count": 7,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "(df['cl_id'] == 0).sum()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>mcc</th>\n",
       "      <th>curr</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>[1, 2]</td>\n",
       "      <td>[0, 1]</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>[3, 4]</td>\n",
       "      <td>[0, 0]</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "      mcc    curr\n",
       "0  [1, 2]  [0, 1]\n",
       "1  [3, 4]  [0, 0]"
      ]
     },
     "execution_count": 8,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "df_dic = {'mcc': [[1, 2], [3,4]], 'curr': [[0,1], [0,0]]}\n",
    "pd.DataFrame.from_dict(df_dic)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [
    {
     "ename": "KeyError",
     "evalue": "'event_time'",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mKeyError\u001b[0m                                  Traceback (most recent call last)",
      "\u001b[1;32m/home/event_seq/experiments/rosbank/notebooks/testing.ipynb Cell 7\u001b[0m line \u001b[0;36m9\n\u001b[1;32m      <a href='vscode-notebook-cell://attached-container%2B7b22636f6e7461696e65724e616d65223a222f76765f6576656e74222c2273657474696e6773223a7b22686f7374223a227373683a2f2f6d696461732e736b6f6c746563682e7275227d7d/home/event_seq/experiments/rosbank/notebooks/testing.ipynb#Y201sdnNjb2RlLXJlbW90ZQ%3D%3D?line=5'>6</a>\u001b[0m     df_dic[feature] \u001b[39m=\u001b[39m []\n\u001b[1;32m      <a href='vscode-notebook-cell://attached-container%2B7b22636f6e7461696e65724e616d65223a222f76765f6576656e74222c2273657474696e6773223a7b22686f7374223a227373683a2f2f6d696461732e736b6f6c746563682e7275227d7d/home/event_seq/experiments/rosbank/notebooks/testing.ipynb#Y201sdnNjb2RlLXJlbW90ZQ%3D%3D?line=7'>8</a>\u001b[0m \u001b[39mfor\u001b[39;00m key, val \u001b[39min\u001b[39;00m batch[\u001b[39m0\u001b[39m]\u001b[39m.\u001b[39mpayload\u001b[39m.\u001b[39mitems():\n\u001b[0;32m----> <a href='vscode-notebook-cell://attached-container%2B7b22636f6e7461696e65724e616d65223a222f76765f6576656e74222c2273657474696e6773223a7b22686f7374223a227373683a2f2f6d696461732e736b6f6c746563682e7275227d7d/home/event_seq/experiments/rosbank/notebooks/testing.ipynb#Y201sdnNjb2RlLXJlbW90ZQ%3D%3D?line=8'>9</a>\u001b[0m     df_dic[key]\u001b[39m.\u001b[39mextend(val\u001b[39m.\u001b[39mtolist())\n",
      "\u001b[0;31mKeyError\u001b[0m: 'event_time'"
     ]
    }
   ],
   "source": [
    "df_dic = {}\n",
    "for feature in conf.features.embeddings.keys():\n",
    "    df_dic[feature] = []\n",
    "\n",
    "for feature in conf.features.numeric_values.keys():\n",
    "    df_dic[feature] = []\n",
    "\n",
    "for key, val in batch[0].payload.items():\n",
    "    df_dic[key].extend(val.tolist())"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>channel_type</th>\n",
       "      <th>currency</th>\n",
       "      <th>mcc</th>\n",
       "      <th>trx_category</th>\n",
       "      <th>amount</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>[3, 3, 3, 3, 3, 3, 3, 3, 3, 3, 3, 3, 3, 3, 3, ...</td>\n",
       "      <td>[2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, ...</td>\n",
       "      <td>[7, 3, 2, 2, 9, 3, 2, 3, 99, 2, 7, 8, 14, 2, 9...</td>\n",
       "      <td>[2, 4, 2, 2, 2, 4, 2, 5, 2, 2, 2, 2, 8, 2, 2, ...</td>\n",
       "      <td>[6.8416154764775925, 6.2166061010848646, 6.590...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>[2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, ...</td>\n",
       "      <td>[2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, ...</td>\n",
       "      <td>[8, 3, 3, 3, 3, 2, 2, 68, 95, 2, 2, 38, 37, 74...</td>\n",
       "      <td>[2, 3, 3, 3, 3, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, ...</td>\n",
       "      <td>[7.06296254328452, 7.313886831633462, 10.77897...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>[2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, ...</td>\n",
       "      <td>[2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, ...</td>\n",
       "      <td>[96, 12, 3, 8, 8, 8, 2, 16, 16, 12, 48, 92, 21...</td>\n",
       "      <td>[2, 2, 4, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, ...</td>\n",
       "      <td>[6.730862957183067, 7.02197642307216, 9.210440...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>[3, 3, 3, 3, 3, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, ...</td>\n",
       "      <td>[2, 2, 2, 2, 2, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, ...</td>\n",
       "      <td>[59, 2, 30, 59, 59, 0, 0, 0, 0, 0, 0, 0, 0, 0,...</td>\n",
       "      <td>[2, 2, 2, 2, 2, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, ...</td>\n",
       "      <td>[11.225256725762893, 5.863631175598097, 11.225...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>[3, 3, 3, 3, 3, 3, 3, 3, 3, 3, 3, 3, 3, 3, 3, ...</td>\n",
       "      <td>[2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, ...</td>\n",
       "      <td>[3, 3, 16, 37, 12, 3, 3, 4, 4, 4, 4, 5, 13, 5,...</td>\n",
       "      <td>[3, 3, 2, 2, 2, 3, 3, 2, 2, 2, 2, 2, 2, 2, 2, ...</td>\n",
       "      <td>[9.852246888342531, 6.90875477931522, 9.888932...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>...</th>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>123</th>\n",
       "      <td>[3, 3, 3, 3, 3, 3, 3, 3, 3, 3, 3, 3, 3, 3, 3, ...</td>\n",
       "      <td>[2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 3, 3, 3, ...</td>\n",
       "      <td>[32, 32, 14, 49, 82, 49, 46, 46, 29, 56, 49, 4...</td>\n",
       "      <td>[2, 2, 6, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, ...</td>\n",
       "      <td>[5.303304908059076, 6.311734809152915, 9.35452...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>124</th>\n",
       "      <td>[2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, ...</td>\n",
       "      <td>[2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, ...</td>\n",
       "      <td>[2, 3, 47, 85, 6, 6, 2, 2, 2, 2, 39, 7, 2, 2, ...</td>\n",
       "      <td>[2, 3, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, ...</td>\n",
       "      <td>[7.071573364211532, 11.156264806643742, 9.8632...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>125</th>\n",
       "      <td>[2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, ...</td>\n",
       "      <td>[2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, ...</td>\n",
       "      <td>[6, 3, 3, 3, 2, 2, 8, 4, 3, 3, 9, 9, 9, 3, 4, ...</td>\n",
       "      <td>[2, 4, 5, 9, 2, 2, 2, 2, 4, 4, 2, 2, 2, 4, 2, ...</td>\n",
       "      <td>[6.566672429803241, 11.938199736300925, 12.013...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>126</th>\n",
       "      <td>[3, 3, 3, 3, 3, 3, 3, 3, 3, 3, 3, 3, 3, 3, 3, ...</td>\n",
       "      <td>[2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, ...</td>\n",
       "      <td>[3, 3, 3, 3, 3, 3, 3, 3, 66, 3, 3, 3, 3, 3, 3,...</td>\n",
       "      <td>[4, 4, 4, 4, 4, 4, 4, 4, 2, 4, 4, 4, 4, 4, 4, ...</td>\n",
       "      <td>[10.596659732783579, 10.596659732783579, 10.59...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>127</th>\n",
       "      <td>[3, 3, 3, 3, 3, 3, 3, 3, 3, 3, 3, 3, 3, 3, 3, ...</td>\n",
       "      <td>[2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, ...</td>\n",
       "      <td>[4, 3, 2, 4, 7, 2, 2, 4, 3, 32, 5, 5, 47, 47, ...</td>\n",
       "      <td>[2, 3, 2, 2, 2, 2, 2, 2, 3, 2, 2, 2, 2, 2, 2, ...</td>\n",
       "      <td>[5.5254529391317835, 8.517393171418904, 7.0824...</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "<p>128 rows  5 columns</p>\n",
       "</div>"
      ],
      "text/plain": [
       "                                          channel_type  \\\n",
       "0    [3, 3, 3, 3, 3, 3, 3, 3, 3, 3, 3, 3, 3, 3, 3, ...   \n",
       "1    [2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, ...   \n",
       "2    [2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, ...   \n",
       "3    [3, 3, 3, 3, 3, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, ...   \n",
       "4    [3, 3, 3, 3, 3, 3, 3, 3, 3, 3, 3, 3, 3, 3, 3, ...   \n",
       "..                                                 ...   \n",
       "123  [3, 3, 3, 3, 3, 3, 3, 3, 3, 3, 3, 3, 3, 3, 3, ...   \n",
       "124  [2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, ...   \n",
       "125  [2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, ...   \n",
       "126  [3, 3, 3, 3, 3, 3, 3, 3, 3, 3, 3, 3, 3, 3, 3, ...   \n",
       "127  [3, 3, 3, 3, 3, 3, 3, 3, 3, 3, 3, 3, 3, 3, 3, ...   \n",
       "\n",
       "                                              currency  \\\n",
       "0    [2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, ...   \n",
       "1    [2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, ...   \n",
       "2    [2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, ...   \n",
       "3    [2, 2, 2, 2, 2, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, ...   \n",
       "4    [2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, ...   \n",
       "..                                                 ...   \n",
       "123  [2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 3, 3, 3, ...   \n",
       "124  [2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, ...   \n",
       "125  [2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, ...   \n",
       "126  [2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, ...   \n",
       "127  [2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, ...   \n",
       "\n",
       "                                                   mcc  \\\n",
       "0    [7, 3, 2, 2, 9, 3, 2, 3, 99, 2, 7, 8, 14, 2, 9...   \n",
       "1    [8, 3, 3, 3, 3, 2, 2, 68, 95, 2, 2, 38, 37, 74...   \n",
       "2    [96, 12, 3, 8, 8, 8, 2, 16, 16, 12, 48, 92, 21...   \n",
       "3    [59, 2, 30, 59, 59, 0, 0, 0, 0, 0, 0, 0, 0, 0,...   \n",
       "4    [3, 3, 16, 37, 12, 3, 3, 4, 4, 4, 4, 5, 13, 5,...   \n",
       "..                                                 ...   \n",
       "123  [32, 32, 14, 49, 82, 49, 46, 46, 29, 56, 49, 4...   \n",
       "124  [2, 3, 47, 85, 6, 6, 2, 2, 2, 2, 39, 7, 2, 2, ...   \n",
       "125  [6, 3, 3, 3, 2, 2, 8, 4, 3, 3, 9, 9, 9, 3, 4, ...   \n",
       "126  [3, 3, 3, 3, 3, 3, 3, 3, 66, 3, 3, 3, 3, 3, 3,...   \n",
       "127  [4, 3, 2, 4, 7, 2, 2, 4, 3, 32, 5, 5, 47, 47, ...   \n",
       "\n",
       "                                          trx_category  \\\n",
       "0    [2, 4, 2, 2, 2, 4, 2, 5, 2, 2, 2, 2, 8, 2, 2, ...   \n",
       "1    [2, 3, 3, 3, 3, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, ...   \n",
       "2    [2, 2, 4, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, ...   \n",
       "3    [2, 2, 2, 2, 2, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, ...   \n",
       "4    [3, 3, 2, 2, 2, 3, 3, 2, 2, 2, 2, 2, 2, 2, 2, ...   \n",
       "..                                                 ...   \n",
       "123  [2, 2, 6, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, ...   \n",
       "124  [2, 3, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, ...   \n",
       "125  [2, 4, 5, 9, 2, 2, 2, 2, 4, 4, 2, 2, 2, 4, 2, ...   \n",
       "126  [4, 4, 4, 4, 4, 4, 4, 4, 2, 4, 4, 4, 4, 4, 4, ...   \n",
       "127  [2, 3, 2, 2, 2, 2, 2, 2, 3, 2, 2, 2, 2, 2, 2, ...   \n",
       "\n",
       "                                                amount  \n",
       "0    [6.8416154764775925, 6.2166061010848646, 6.590...  \n",
       "1    [7.06296254328452, 7.313886831633462, 10.77897...  \n",
       "2    [6.730862957183067, 7.02197642307216, 9.210440...  \n",
       "3    [11.225256725762893, 5.863631175598097, 11.225...  \n",
       "4    [9.852246888342531, 6.90875477931522, 9.888932...  \n",
       "..                                                 ...  \n",
       "123  [5.303304908059076, 6.311734809152915, 9.35452...  \n",
       "124  [7.071573364211532, 11.156264806643742, 9.8632...  \n",
       "125  [6.566672429803241, 11.938199736300925, 12.013...  \n",
       "126  [10.596659732783579, 10.596659732783579, 10.59...  \n",
       "127  [5.5254529391317835, 8.517393171418904, 7.0824...  \n",
       "\n",
       "[128 rows x 5 columns]"
      ]
     },
     "execution_count": 27,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "pd.DataFrame.from_dict(df_dic)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [],
   "source": [
    "net = SeqGen(model_conf=model_conf, data_conf=conf)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "  0%|          | 0/62 [00:02<?, ?it/s]\n"
     ]
    }
   ],
   "source": [
    "gts = []\n",
    "outs = []\n",
    "for batch in tqdm(train_loader):\n",
    "    out = net(batch[0])\n",
    "    outs.append(out)\n",
    "    gts.append(batch[1])\n",
    "    break"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [],
   "source": [
    "def out_to_padded_batch(out, conf):\n",
    "    order = {}\n",
    "\n",
    "    k = 0\n",
    "    for key in outs[0]['input_batch'].payload.keys():\n",
    "        if key in conf.features.numeric_values.keys():\n",
    "            order[k] = key\n",
    "            k += 1\n",
    "\n",
    "    num_numeric = len(conf.features.numeric_values.keys())\n",
    "\n",
    "    payload = {}\n",
    "    payload['event_time'] = out['time_steps'][:,1:]\n",
    "    length = (out['time_steps'][:,1:] != -1).sum(dim=1)\n",
    "    mask = (out['time_steps'][:,1:] != -1)\n",
    "    for key, val in out['emb_dist'].items():\n",
    "        payload[key] = val.cpu().argmax(dim=-1).detach()\n",
    "        payload[key][~mask] = 0\n",
    "\n",
    "\n",
    "    if model_conf.use_deltas:\n",
    "        pred_delta = out['pred'][:, :, -1].squeeze(-1)\n",
    "        pred = out['pred'][:, :, :-1]\n",
    "\n",
    "    numeric_pred = pred[:,:,-num_numeric:]\n",
    "    for i in range(num_numeric):\n",
    "        cur_key = order[i]\n",
    "        cur_val = numeric_pred[:,:,i].cpu().detach()\n",
    "        payload[cur_key] = cur_val\n",
    "        payload[cur_key][~mask] = 0\n",
    "\n",
    "    return PaddedBatch(payload, length)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [],
   "source": [
    "gen_batch = out_to_padded_batch(out, conf)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "(<src.data_load.dataloader.PaddedBatch at 0x7f888a853910>,\n",
       " <src.data_load.dataloader.PaddedBatch at 0x7f8599aab1c0>)"
      ]
     },
     "execution_count": 11,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "gen_batch, batch[0]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "(torch.Size([64, 199]), torch.Size([64, 199]))"
      ]
     },
     "execution_count": 12,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "gen_batch.payload['mcc'].size(), batch[0].payload['mcc'][:,:-1].size()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "torch.Size([128, 199])"
      ]
     },
     "execution_count": 13,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "torch.cat([gen_batch.payload['mcc'], batch[0].payload['mcc'][:,:-1]], dim=0).size()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 41,
   "metadata": {},
   "outputs": [],
   "source": [
    "def mix_batches(gen_batch, true_batch):\n",
    "\n",
    "    new_payload = {}\n",
    "    for key in gen_batch.payload.keys():\n",
    "        new_payload[key] = torch.cat([gen_batch.payload[key], true_batch.payload[key][:,:-1]], dim=0)\n",
    "    new_lens = torch.cat([gen_batch.seq_lens, true_batch.seq_lens-1])\n",
    "\n",
    "    new_batch = PaddedBatch(new_payload, new_lens)\n",
    "    \n",
    "    d_labels = torch.zeros(len(new_batch), dtype=torch.long)\n",
    "    d_labels[:len(new_batch) // 2] = 1\n",
    "\n",
    "    return new_batch, d_labels"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 42,
   "metadata": {},
   "outputs": [],
   "source": [
    "b, l = mix_batches(gen_batch, batch[0])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 48,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "tensor([1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1,\n",
       "        1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1,\n",
       "        1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 0, 0, 0, 0, 0, 0, 0, 0,\n",
       "        0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0,\n",
       "        0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0,\n",
       "        0, 0, 0, 0, 0, 0, 0, 0])"
      ]
     },
     "execution_count": 48,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "l.unsqueeze(0).repeat(2, 1)[1]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 23,
   "metadata": {},
   "outputs": [],
   "source": [
    "new_batch = PaddedBatch(new_payload, new_lens)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 24,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "128"
      ]
     },
     "execution_count": 24,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "len(new_batch)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 27,
   "metadata": {},
   "outputs": [],
   "source": [
    "d_labels = torch.zeros(len(new_batch))\n",
    "d_labels[:len(new_batch) // 2] = 1"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 37,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "tensor(0.)"
      ]
     },
     "execution_count": 37,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "d_labels[127]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {},
   "outputs": [],
   "source": [
    "mask = (out['time_steps'][:,1:] != -1)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "metadata": {},
   "outputs": [],
   "source": [
    "val_pred = val.cpu().argmax(dim=-1)\n",
    "val_pred[~mask] = 0"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 21,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "tensor([5, 5, 4, 4, 5, 5, 4, 4, 5, 5, 5, 5, 5, 5, 5, 5, 5, 4, 5, 5, 4, 5, 4, 4,\n",
       "        4, 4, 5, 5, 4, 5, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0,\n",
       "        0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0,\n",
       "        0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0,\n",
       "        0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0,\n",
       "        0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0,\n",
       "        0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0,\n",
       "        0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0,\n",
       "        0, 0, 0, 0, 0, 0, 0])"
      ]
     },
     "execution_count": 21,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "val_pred[1]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 60,
   "metadata": {},
   "outputs": [],
   "source": [
    "def output_to_df(outs, gts):\n",
    "    order = {}\n",
    "\n",
    "    k = 0\n",
    "    for key in outs[0]['input_batch'].payload.keys():\n",
    "        if key in conf.features.numeric_values.keys():\n",
    "            order[k] = key\n",
    "            k += 1\n",
    "\n",
    "    df_dic = {'event_time': [], 'trx_count': [], conf.features.target_col: []}\n",
    "    for feature in conf.features.embeddings.keys():\n",
    "        df_dic[feature] = []\n",
    "\n",
    "    for feature in conf.features.numeric_values.keys():\n",
    "        df_dic[feature] = []\n",
    "\n",
    "    for out, gt in zip(outs, gts):\n",
    "        for key, val in out['emb_dist'].items():\n",
    "            df_dic[key].extend(val.cpu().argmax(dim=-1).tolist())\n",
    "\n",
    "\n",
    "        if model_conf.use_deltas:\n",
    "            pred_delta = out['pred'][:, :, -1].squeeze(-1)\n",
    "            pred = out['pred'][:, :, :-1]\n",
    "\n",
    "        num_numeric = len(conf.features.numeric_values.keys())\n",
    "        numeric_pred = pred[:,:,-num_numeric:]\n",
    "        for i in range(num_numeric):\n",
    "            cur_key = order[i]\n",
    "            cur_val = numeric_pred[:,:,i].cpu().tolist()\n",
    "            df_dic[cur_key].extend(cur_val)\n",
    "\n",
    "        df_dic['event_time'].extend(out['time_steps'][:,1:].tolist())\n",
    "        df_dic['trx_count'].extend((out['time_steps'][:,1:] != -1).sum(dim=1).tolist())\n",
    "        df_dic['target_target_flag'].extend(gt[1].cpu().tolist())\n",
    "\n",
    "\n",
    "    generated_df = pd.DataFrame.from_dict(df_dic)\n",
    "    generated_df['event_time'] = generated_df['event_time'].apply(lambda x: (np.array(x) * (conf.max_time - conf.min_time)) + conf.min_time)\n",
    "    def truncate_lists(row):\n",
    "        value = row[\"trx_count\"]\n",
    "        for col in row.index:\n",
    "            if isinstance(row[col], (np.ndarray, list)):\n",
    "                row[col] = row[col][:value]\n",
    "        return row\n",
    "\n",
    "    generated_df = generated_df.apply(func=truncate_lists, axis=1)\n",
    "\n",
    "    generated_df[conf.col_id] = np.arange(len(generated_df))\n",
    "    return generated_df"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 61,
   "metadata": {},
   "outputs": [],
   "source": [
    "gen_df = output_to_df(outs, gts)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 62,
   "metadata": {},
   "outputs": [],
   "source": [
    "gen_df.to_parquet('test_df.parquet')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 63,
   "metadata": {},
   "outputs": [],
   "source": [
    "conf.train_supervised_path = Path('/home/event_seq/experiments/rosbank/notebooks/test_df.parquet')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 64,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "0it [00:00, ?it/s]"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "3918it [00:00, 21584.40it/s]\n"
     ]
    }
   ],
   "source": [
    "train_loader, valid_loader = create_data_loaders(conf, supervised=True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 65,
   "metadata": {},
   "outputs": [],
   "source": [
    "for batch in train_loader:\n",
    "    break"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 69,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "{'channel_type': tensor([[3, 3, 3,  ..., 0, 0, 0],\n",
       "         [3, 3, 3,  ..., 0, 0, 0],\n",
       "         [3, 3, 3,  ..., 0, 0, 0],\n",
       "         ...,\n",
       "         [3, 3, 3,  ..., 0, 0, 0],\n",
       "         [3, 3, 3,  ..., 3, 3, 3],\n",
       "         [3, 3, 3,  ..., 3, 3, 3]]),\n",
       " 'currency': tensor([[3, 3, 3,  ..., 0, 0, 0],\n",
       "         [3, 3, 3,  ..., 0, 0, 0],\n",
       "         [3, 3, 3,  ..., 0, 0, 0],\n",
       "         ...,\n",
       "         [3, 3, 3,  ..., 0, 0, 0],\n",
       "         [3, 3, 3,  ..., 3, 3, 3],\n",
       "         [3, 3, 3,  ..., 3, 3, 3]]),\n",
       " 'mcc': tensor([[2, 2, 2,  ..., 0, 0, 0],\n",
       "         [2, 2, 2,  ..., 0, 0, 0],\n",
       "         [2, 2, 2,  ..., 0, 0, 0],\n",
       "         ...,\n",
       "         [2, 2, 2,  ..., 0, 0, 0],\n",
       "         [2, 2, 2,  ..., 2, 2, 2],\n",
       "         [2, 2, 2,  ..., 2, 2, 2]]),\n",
       " 'trx_category': tensor([[9, 9, 9,  ..., 0, 0, 0],\n",
       "         [9, 9, 9,  ..., 0, 0, 0],\n",
       "         [9, 9, 9,  ..., 0, 0, 0],\n",
       "         ...,\n",
       "         [9, 9, 9,  ..., 0, 0, 0],\n",
       "         [9, 9, 9,  ..., 9, 9, 9],\n",
       "         [9, 9, 9,  ..., 9, 9, 9]]),\n",
       " 'amount': tensor([[0.1943, 0.1114, 0.1366,  ..., 0.0000, 0.0000, 0.0000],\n",
       "         [0.1864, 0.1073, 0.1300,  ..., 0.0000, 0.0000, 0.0000],\n",
       "         [0.1755, 0.1064, 0.1317,  ..., 0.0000, 0.0000, 0.0000],\n",
       "         ...,\n",
       "         [0.1879, 0.1184, 0.1382,  ..., 0.0000, 0.0000, 0.0000],\n",
       "         [0.1841, 0.1096, 0.1320,  ..., 0.1325, 0.1358, 0.1307],\n",
       "         [0.1817, 0.1070, 0.1326,  ..., 0.1303, 0.1313, 0.1311]],\n",
       "        dtype=torch.float64),\n",
       " 'event_time': tensor([[ 0.2136,  0.2136,  0.2136,  ..., -1.0000, -1.0000, -1.0000],\n",
       "         [ 0.0626,  0.0663,  0.0663,  ..., -1.0000, -1.0000, -1.0000],\n",
       "         [ 0.2431,  0.2431,  0.2449,  ..., -1.0000, -1.0000, -1.0000],\n",
       "         ...,\n",
       "         [ 0.1363,  0.1371,  0.1510,  ..., -1.0000, -1.0000, -1.0000],\n",
       "         [ 0.3444,  0.3462,  0.3462,  ...,  0.4738,  0.4738,  0.4738],\n",
       "         [ 0.3702,  0.3702,  0.3720,  ...,  0.4825,  0.4825,  0.4825]],\n",
       "        dtype=torch.float64)}"
      ]
     },
     "execution_count": 69,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "batch[0].payload"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 40,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "tensor([ 0.2167,  0.4243,  0.6401,  0.8549,  1.0706,  1.2812,  1.4994,  1.7086,\n",
       "         1.9264,  2.1430,  2.3536,  2.5688,  2.7799,  2.9977,  3.2112,  3.4298,\n",
       "         3.6440,  3.8559,  4.0742,  4.2852,  4.5045,  4.7123,  4.9371,  5.1520,\n",
       "         5.3646,  5.5771,  5.7939,  6.0026,  6.2179,  6.4309,  6.6444,  6.8651,\n",
       "         7.0795,  7.2949,  7.5053,  7.7209,  7.9334,  8.1450,  8.3612,  8.5773,\n",
       "         8.7870,  9.0079,  9.2151,  9.4337,  9.6470,  9.8614, 10.0750, 10.2931,\n",
       "        10.5078, 10.7236, 10.9383, 11.1544, 11.3685, 11.5835, 11.7935, 12.0073,\n",
       "        12.2203, 12.4350, 12.6478, 12.8651, 13.0787, 13.2881, 13.5084, 13.7253,\n",
       "        13.9408, 14.1587, 14.3687, 14.5847, 14.8013, 15.0105, 15.2255, 15.4438,\n",
       "        15.6534, 15.8676, 16.0804, 16.2990, 16.5115, 16.7281, 16.9444, 17.1618,\n",
       "        17.3783, 17.5956, 17.8040, 18.0197, 18.2352, 18.4484, 18.6684, 18.8786,\n",
       "        19.0920, 19.3045, 19.5223, 19.7397, 19.9560, 20.1680, 20.3867, 20.6042,\n",
       "        20.8174, 21.0353, 21.2498, 21.4592, 21.6804, 21.8967, 22.1160, 22.3350,\n",
       "        22.5540, 22.7731, 22.9922, 23.2114, 23.4305, 23.6496, 23.8687, 24.0878,\n",
       "        24.3069, 24.5260, 24.7451, 24.9642, 25.1833, 25.4025, 25.6216, 25.8407,\n",
       "        26.0598, 26.2789, 26.4980, 26.7171, 26.9362, 27.1553, 27.3744, 27.5936,\n",
       "        27.8127, 28.0318, 28.2509, 28.4700, 28.6891, 28.9082, 29.1273, 29.3464,\n",
       "        29.5655, 29.7846, 30.0038, 30.2229, 30.4420, 30.6611, 30.8802, 31.0993,\n",
       "        31.3184, 31.5375, 31.7566, 31.9757, 32.1949, 32.4140, 32.6331, 32.8522,\n",
       "        33.0713, 33.2904, 33.5095, 33.7286, 33.9477, 34.1668, 34.3860, 34.6051,\n",
       "        34.8242, 35.0433, 35.2624, 35.4815, 35.7006, 35.9197, 36.1388, 36.3579,\n",
       "        36.5770, 36.7962, 37.0153, 37.2344, 37.4535, 37.6726, 37.8917, 38.1108,\n",
       "        38.3299, 38.5490, 38.7681, 38.9873, 39.2064, 39.4255, 39.6446, 39.8637,\n",
       "        40.0828, 40.3019, 40.5210, 40.7401, 40.9592, 41.1784, 41.3975, 41.6166,\n",
       "        41.8357, 42.0548, 42.2739, 42.4930, 42.7121, 42.9312, 43.1503],\n",
       "       grad_fn=<CumsumBackward0>)"
      ]
     },
     "execution_count": 40,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "torch.cumsum(pred_delta[0], dim=0)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [],
   "source": [
    "loss = net.loss(out, batch[1])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "{'mse_loss': tensor(1.0479, grad_fn=<DivBackward0>),\n",
       " 'total_CE_loss': tensor(9.9209, grad_fn=<SumBackward0>),\n",
       " 'delta_loss': tensor(0.0690, grad_fn=<DivBackward0>),\n",
       " 'channel_type': tensor(1.4719, grad_fn=<NllLoss2DBackward0>),\n",
       " 'currency': tensor(1.3469, grad_fn=<NllLoss2DBackward0>),\n",
       " 'mcc': tensor(4.6315, grad_fn=<NllLoss2DBackward0>),\n",
       " 'trx_category': tensor(2.4707, grad_fn=<NllLoss2DBackward0>),\n",
       " 'total_loss': tensor(17.8654, grad_fn=<AddBackward0>)}"
      ]
     },
     "execution_count": 10,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "loss"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "metadata": {},
   "outputs": [],
   "source": [
    "for batch in train_loader:\n",
    "    break\n",
    "\n",
    "fp = FeatureProcessor(model_conf=model_conf, data_conf=conf)\n",
    "x, time_steps = fp(batch[0])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [],
   "source": [
    "gt_delta = time_steps.diff(1)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "metadata": {},
   "outputs": [],
   "source": [
    "gt_delta = time_steps.diff(1)\n",
    "delta_feature =torch.cat([gt_delta, torch.zeros(128, 1)], dim=1)\n",
    "torch.cat([x, delta_feature.unsqueeze(-1)], dim=-1)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "(torch.Size([128, 200, 33]), torch.Size([128, 200]))"
      ]
     },
     "execution_count": 17,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "x.size(), delta_feature.size()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "torch.Size([128, 200, 34])"
      ]
     },
     "execution_count": 18,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "torch.cat([x, delta_feature.unsqueeze(-1)], dim=-1).size()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [],
   "source": [
    "class GRUCell(nn.Module):\n",
    "    def __init__(self, input_size, hidden_size, global_hidden_size, bias=True):\n",
    "        super(GRUCell, self).__init__()\n",
    "        self.input_size = input_size\n",
    "        self.hidden_size = hidden_size\n",
    "        self.bias = bias\n",
    "\n",
    "        self.x2h = nn.Linear(input_size, 3 * hidden_size, bias=bias)\n",
    "        self.h2h = nn.Linear(hidden_size, 3 * hidden_size, bias=bias)\n",
    "        self.mix_global = nn.Linear(hidden_size + global_hidden_size, hidden_size)\n",
    "\n",
    "        self.reset_parameters()\n",
    "\n",
    "\n",
    "    def reset_parameters(self):\n",
    "        std = 1.0 / np.sqrt(self.hidden_size)\n",
    "        for w in self.parameters():\n",
    "            w.data.uniform_(-std, std)\n",
    "\n",
    "    def forward(self, input, global_hidden, hx=None):\n",
    "\n",
    "        # Inputs:\n",
    "        #       input: of shape (batch_size, input_size)\n",
    "        #       global_hidden: of shape (batch_size, global_hidden_size)\n",
    "        #       hx: of shape (batch_size, hidden_size)\n",
    "        # Output:\n",
    "        #       hy: of shape (batch_size, hidden_size)\n",
    "\n",
    "        if hx is None:\n",
    "            hx = Variable(input.new_zeros(input.size(0), self.hidden_size))\n",
    "        \n",
    "        hx = self.mix_global(torch.cat([global_hidden, hx], dim=-1))\n",
    "        x_t = self.x2h(input)\n",
    "        h_t = self.h2h(hx)\n",
    "\n",
    "\n",
    "        x_reset, x_upd, x_new = x_t.chunk(3, 1)\n",
    "        h_reset, h_upd, h_new = h_t.chunk(3, 1)\n",
    "\n",
    "        reset_gate = torch.sigmoid(x_reset + h_reset)\n",
    "        update_gate = torch.sigmoid(x_upd + h_upd)\n",
    "        new_gate = torch.tanh(x_new + (reset_gate * h_new))\n",
    "\n",
    "        hy = update_gate * hx + (1 - update_gate) * new_gate\n",
    "\n",
    "        return hy\n",
    "\n",
    "class DecoderGRU(nn.Module):\n",
    "    def __init__(self, input_size, hidden_size, global_hidden_size, num_layers, bias=True):\n",
    "        super(DecoderGRU, self).__init__()\n",
    "\n",
    "        self.input_size = input_size\n",
    "        self.hidden_size = hidden_size\n",
    "        self.global_hidden_size = global_hidden_size\n",
    "        self.num_layers = num_layers\n",
    "        self.bias = bias\n",
    "\n",
    "        self.rnn_cell_list = nn.ModuleList()\n",
    "        self.rnn_cell_list.append(GRUCell(self.input_size,\n",
    "                                          self.hidden_size,\n",
    "                                          self.global_hidden_size,\n",
    "                                          self.bias))\n",
    "        for l in range(1, self.num_layers):\n",
    "            self.rnn_cell_list.append(GRUCell(self.hidden_size,\n",
    "                                              self.hidden_size,\n",
    "                                              self.global_hidden_size,\n",
    "                                              self.bias))\n",
    "\n",
    "    def forward(self, input, global_hidden, hx=None):\n",
    "\n",
    "        # Input of shape (batch_size, seqence length, input_size)\n",
    "        #\n",
    "        # Output of shape (batch_size, output_size)\n",
    "\n",
    "        if hx is None:\n",
    "            h0 = Variable(torch.zeros(self.num_layers, input.size(0), self.hidden_size).to(input.device))\n",
    "\n",
    "        else:\n",
    "             h0 = hx\n",
    "\n",
    "        outs = []\n",
    "\n",
    "        hidden = list()\n",
    "        for layer in range(self.num_layers):\n",
    "            hidden.append(h0[layer, :, :])\n",
    "\n",
    "        for t in range(input.size(1)):\n",
    "\n",
    "            for layer in range(self.num_layers):\n",
    "\n",
    "                if layer == 0:\n",
    "                    hidden_l = self.rnn_cell_list[layer](input[:, t, :], global_hidden, hidden[layer])\n",
    "                else:\n",
    "                    hidden_l = self.rnn_cell_list[layer](hidden[layer - 1], global_hidden, hidden[layer])\n",
    "                hidden[layer] = hidden_l\n",
    "\n",
    "                hidden[layer] = hidden_l\n",
    "\n",
    "            outs.append(hidden_l.unsqueeze(1))\n",
    "\n",
    "        return torch.cat(outs, dim=1)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "metadata": {},
   "outputs": [],
   "source": [
    "encoder = nn.GRU(33, 15)\n",
    "decoder = DecoderGRU(33, 13, global_hidden_size=15, num_layers=1)\n",
    "out_proj = nn.Linear(13, 33)\n",
    "delta_proj = nn.Linear(13, 1)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [],
   "source": [
    "all_hid, hn = encoder(x)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [],
   "source": [
    "lens = batch[0].seq_lens - 1\n",
    "last_hidden = all_hid[:, lens, :].diagonal().T"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "torch.Size([128, 15])"
      ]
     },
     "execution_count": 11,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "last_hidden.size()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 34,
   "metadata": {},
   "outputs": [],
   "source": [
    "hid_dec = decoder(x, last_hidden)\n",
    "out = out_proj(hid_dec)\n",
    "\n",
    "pred_delta = delta_proj(hid_dec)[:,:-1, :].squeeze(-1)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 35,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "torch.Size([128, 199])"
      ]
     },
     "execution_count": 35,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "pred_delta.size()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 36,
   "metadata": {},
   "outputs": [],
   "source": [
    "gt_delta = time_steps.diff(1)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 37,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "torch.Size([128, 199])"
      ]
     },
     "execution_count": 37,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "gt_delta.size()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 38,
   "metadata": {},
   "outputs": [],
   "source": [
    "loss_fn = torch.nn.MSELoss(reduction='none')\n",
    "delta_mse = loss_fn(gt_delta, pred_delta)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 41,
   "metadata": {},
   "outputs": [],
   "source": [
    "mask = time_steps != -1\n",
    "delta_mse = delta_mse * mask[:,1:]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 42,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "tensor([[1.2237e-02, 4.1044e-03, 1.5251e-02,  ..., 0.0000e+00, 0.0000e+00,\n",
       "         0.0000e+00],\n",
       "        [9.1590e-04, 8.7555e-02, 4.7797e-02,  ..., 0.0000e+00, 0.0000e+00,\n",
       "         0.0000e+00],\n",
       "        [1.4013e-01, 9.1870e-02, 3.8092e-02,  ..., 0.0000e+00, 0.0000e+00,\n",
       "         0.0000e+00],\n",
       "        ...,\n",
       "        [8.0061e-02, 4.5746e-02, 1.0774e-01,  ..., 0.0000e+00, 0.0000e+00,\n",
       "         0.0000e+00],\n",
       "        [8.0557e-03, 1.0376e-02, 1.1565e-02,  ..., 0.0000e+00, 0.0000e+00,\n",
       "         0.0000e+00],\n",
       "        [8.8725e-07, 4.5245e-03, 1.5004e-04,  ..., 0.0000e+00, 0.0000e+00,\n",
       "         0.0000e+00]], grad_fn=<MulBackward0>)"
      ]
     },
     "execution_count": 42,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "delta_mse "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 98,
   "metadata": {},
   "outputs": [],
   "source": [
    "labels = x[:, 1:, :]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 99,
   "metadata": {},
   "outputs": [],
   "source": [
    "pred = out[:,:-1, :]\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 102,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "(torch.Size([128, 199, 1]), torch.Size([128, 199, 1]))"
      ]
     },
     "execution_count": 102,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "pred[:,:,-1:].size(), labels[:,:,-1:].size()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 103,
   "metadata": {},
   "outputs": [],
   "source": [
    "loss_fn = torch.nn.MSELoss(reduction='none')\n",
    "loss = loss_fn(labels[:,:,-1:], pred[:,:,-1:])\n",
    "mask = labels[:,:,-1:] != 0\n",
    "masked_loss = loss * mask\n",
    "mse_loss = masked_mse.sum() / (masked_mse != 0).sum()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "mask = torch.ones(128, 199)\n",
    "mask[torch.arange(0, 199).repeat(128, 1) > lens.unsqueeze(1) - 1] = 0\n",
    "masked_mse = (loss * mask.unsqueeze(dim=-1))[:,:,-1]\n",
    "mse_loss = masked_mse.sum() / (masked_mse != 0).sum()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 93,
   "metadata": {},
   "outputs": [],
   "source": [
    "class EmbeddingPredictor(nn.Module):\n",
    "    def __init__(self, model_conf, data_conf):\n",
    "        super().__init__()\n",
    "        self.model_conf = model_conf\n",
    "        self.data_conf = data_conf\n",
    "\n",
    "        self.criterion = nn.CrossEntropyLoss(reduction=\"mean\", ignore_index=0)\n",
    "\n",
    "        self.emb_names = list(self.data_conf.features.embeddings.keys())\n",
    "        self.num_embeds = len(self.emb_names)\n",
    "        self.categorical_len = self.num_embeds * self.model_conf.features_emb_dim\n",
    "\n",
    "        self.init_embed_predictors()\n",
    "\n",
    "    def init_embed_predictors(self):\n",
    "        self.embed_predictors = nn.ModuleDict()\n",
    "\n",
    "        for name in self.emb_names:\n",
    "            vocab_size = self.data_conf.features.embeddings[name][\"max_value\"]\n",
    "            self.embed_predictors[name] = nn.Linear(\n",
    "                self.model_conf.features_emb_dim, vocab_size\n",
    "            )\n",
    "\n",
    "    def forward(self, x_recon):\n",
    "        batch_size, seq_len, out_dim = x_recon.size()\n",
    "\n",
    "        resized_x = x_recon[:, :, : self.categorical_len].view(\n",
    "            batch_size,\n",
    "            seq_len,\n",
    "            self.num_embeds,\n",
    "            self.model_conf.features_emb_dim,\n",
    "        )\n",
    "\n",
    "        embeddings_distribution = {}\n",
    "        for i, name in enumerate(self.emb_names):\n",
    "            embeddings_distribution[name] = self.embed_predictors[name](\n",
    "                resized_x[:, :, i, :]\n",
    "            )\n",
    "\n",
    "        return embeddings_distribution\n",
    "\n",
    "    def loss(self, embedding_distribution, padded_batch):\n",
    "        embed_losses = {}\n",
    "        for name, dist in embedding_distribution.items():\n",
    "            shifted_labels = padded_batch.payload[name].long()[:, 1:]\n",
    "            embed_losses[name] = (\n",
    "                self.criterion(dist.permute(0, 2, 1), shifted_labels)\n",
    "            )\n",
    "\n",
    "        return embed_losses\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 94,
   "metadata": {},
   "outputs": [],
   "source": [
    "embedding_predictor = EmbeddingPredictor(\n",
    "            model_conf=model_conf, data_conf=conf\n",
    "        )"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 95,
   "metadata": {},
   "outputs": [],
   "source": [
    "dist = embedding_predictor(pred)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 96,
   "metadata": {},
   "outputs": [],
   "source": [
    "cat_losses = embedding_predictor.loss(dist, batch[0])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 97,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "{'channel_type': tensor(1.7276, grad_fn=<NllLoss2DBackward0>),\n",
       " 'currency': tensor(1.8919, grad_fn=<NllLoss2DBackward0>),\n",
       " 'mcc': tensor(4.5331, grad_fn=<NllLoss2DBackward0>),\n",
       " 'trx_category': tensor(2.0777, grad_fn=<NllLoss2DBackward0>)}"
      ]
     },
     "execution_count": 97,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "cat_losses"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "100%|| 1069/1069 [00:11<00:00, 94.87it/s] \n"
     ]
    }
   ],
   "source": [
    "preds, gt = [], []\n",
    "with torch.no_grad():\n",
    "    for batch in tqdm(train_loader):\n",
    "        out = net(batch[0].to(model_conf.device))\n",
    "        preds.append(out['y_pred'].cpu().argmax(dim=1))\n",
    "        gt.append(batch[1][1])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [],
   "source": [
    "y_pred = torch.cat(preds)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [],
   "source": [
    "y_test = torch.cat(gt)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [],
   "source": [
    "from sklearn.metrics import accuracy_score, roc_auc_score"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "0.605381633870006"
      ]
     },
     "execution_count": 10,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "roc_auc_score(y_test, y_pred)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 21,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "0.4909212880143113"
      ]
     },
     "execution_count": 21,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "y_pred_rand = torch.randint_like(y_pred, low=0, high=2)\n",
    "roc_auc_score(y_test, y_pred_rand)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "3718it [00:00, 12329.55it/s]"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "4500it [00:00, 12424.79it/s]\n"
     ]
    }
   ],
   "source": [
    "train_data, valid_data = prepare_data(conf)\n",
    "\n",
    "train_dataset = SplittingDataset(\n",
    "    train_data,\n",
    "    split_strategy.create(**conf.train.split_strategy),\n",
    "    conf.features.target_col,\n",
    ")\n",
    "train_dataset = TargetEnumeratorDataset(train_dataset)\n",
    "# train_dataset = TargetDataset(train_dataset)\n",
    "train_dataset = ConvertingTrxDataset(train_dataset)\n",
    "# #         .\n",
    "# #       \n",
    "train_dataset = DropoutTrxDataset(\n",
    "    train_dataset, trx_dropout=conf.train.dropout, seq_len=conf.train.max_seq_len\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "train_loader = DataLoader(\n",
    "    dataset=train_dataset,\n",
    "    shuffle=True,\n",
    "    collate_fn=collate_splitted_rows,\n",
    "    num_workers=conf.train.num_workers,\n",
    "    batch_size=conf.train.batch_size,\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [],
   "source": [
    "for batch in train_loader:\n",
    "    break"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "(<src.data_load.dataloader.PaddedBatch at 0x7f86b0167e50>,\n",
       " tensor([[3090, 3766, 1988,  592],\n",
       "         [   1,    0,    0,    0]]))"
      ]
     },
     "execution_count": 8,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "batch"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [],
   "source": [
    "net = MegaNetSupervised(model_conf=model_conf, data_conf=conf)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "metadata": {},
   "outputs": [],
   "source": [
    "net = net.to(model_conf.device)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "0it [00:00, ?it/s]"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "6it [00:00, 22.21it/s]\n"
     ]
    }
   ],
   "source": [
    "mout = []\n",
    "mgt = []\n",
    "for i, (batch, gt) in tqdm(enumerate(train_loader)):\n",
    "    out = net(batch.to(model_conf.device))\n",
    "    mout.append(out)\n",
    "    mgt.append(gt)\n",
    "    if i > 5:\n",
    "        break"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "[tensor([[3479, 3461, 2770, 3175],\n",
       "         [   1,    0,    0,    1]]),\n",
       " tensor([[3395, 2160, 3245,  693],\n",
       "         [   1,    1,    1,    1]]),\n",
       " tensor([[2879,  868,  196,  579],\n",
       "         [   0,    1,    0,    1]]),\n",
       " tensor([[2259, 2787,  768, 3039],\n",
       "         [   0,    0,    1,    1]]),\n",
       " tensor([[1095, 4162, 1082, 1319],\n",
       "         [   0,    1,    1,    0]]),\n",
       " tensor([[1406, 3965,  566,  271],\n",
       "         [   1,    1,    1,    1]]),\n",
       " tensor([[2885,  778,  493, 2060],\n",
       "         [   1,    0,    0,    1]])]"
      ]
     },
     "execution_count": 16,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "mgt"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "{'total_loss': tensor(7314717.5000, grad_fn=<AddBackward0>),\n",
       " 'kl_loss': tensor(5.7943, grad_fn=<MeanBackward0>),\n",
       " 'recon_loss': tensor(7314642., grad_fn=<MeanBackward0>),\n",
       " 'classification_loss': tensor(0.6954, grad_fn=<NllLossBackward0>)}"
      ]
     },
     "execution_count": 10,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "net.loss(out, batch[1])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "conf = data_configs()\n",
    "model_conf = model_configs()\n",
    "\n",
    "train_loader, valid_loader = create_data_loaders(conf)\n",
    "net = MegaNetSupervised(model_conf=model_conf, data_conf=conf)\n",
    "opt = torch.optim.Adam(net.parameters(), lr=3e-4, weight_decay=1e-4)\n",
    "trainer = MtandTrainer(\n",
    "    model=net,\n",
    "    optimizer=opt,\n",
    "    train_loader=train_loader,\n",
    "    val_loader=valid_loader,\n",
    "    run_name=run_name,\n",
    "    ckpt_dir=Path(__file__).parent / \"experiments\" / \"rosbank\" / \"ckpt\",\n",
    "    ckpt_replace=True,\n",
    "    ckpt_resume=args.resume,\n",
    "    ckpt_track_metric=\"loss\",\n",
    "    metrics_on_train=False,\n",
    "    total_epochs=args.total_epochs,\n",
    "    device=args.device,\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {},
   "outputs": [],
   "source": [
    "class Classifier(nn.Module):\n",
    "    \n",
    "    def __init__(self, model_conf, data_conf):\n",
    "        super().__init__()\n",
    "\n",
    "        self.model_conf = model_conf\n",
    "        self.data_conf = data_conf\n",
    "\n",
    "        self.gru = nn.GRU(self.model_conf.latent_dim, self.model_conf.classifier_gru_hidden_dim, batch_first=True)\n",
    "        self.net = nn.Sequential(\n",
    "            nn.Linear(self.model_conf.classifier_gru_hidden_dim, self.model_conf.classifier_linear_hidden_dim),\n",
    "            nn.ReLU(),\n",
    "            nn.Linear(self.model_conf.classifier_linear_hidden_dim, self.model_conf.classifier_linear_hidden_dim),\n",
    "            nn.ReLU(),\n",
    "            nn.Linear(self.model_conf.classifier_linear_hidden_dim, self.data_conf.num_classes)\n",
    "        )\n",
    "       \n",
    "    def forward(self, z):\n",
    "        _, out = self.gru(z)\n",
    "        return self.net(out.squeeze(0))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {},
   "outputs": [],
   "source": [
    "clss = Classifier(model_conf=model_conf, data_conf=conf)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {},
   "outputs": [],
   "source": [
    "outc = clss(out['z'])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "tensor([[ 0.1533, -0.0234],\n",
       "        [ 0.1615, -0.0675],\n",
       "        [ 0.1531, -0.0214],\n",
       "        [ 0.1529, -0.0195]], grad_fn=<AddmmBackward0>)"
      ]
     },
     "execution_count": 14,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "outc"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "tensor([0, 0, 0, 1])"
      ]
     },
     "execution_count": 15,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "batch[2]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "metadata": {},
   "outputs": [],
   "source": [
    "loss_fn = nn.funcCrossEntropyLoss()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 21,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "tensor(0.6466, grad_fn=<NllLossBackward0>)"
      ]
     },
     "execution_count": 21,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "nn.functional.cross_entropy(outc, batch[2])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {
    "tags": []
   },
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "0it [00:00, ?it/s]"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "9717it [00:00, 13510.03it/s]\n",
      "500it [00:00, 7271.96it/s]\n"
     ]
    }
   ],
   "source": [
    "train_loader, valid_loader = create_data_loaders(conf)\n",
    "test_loader = create_test_loader(conf)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "for batch in test_loader:\n",
    "    break"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "net = MegaNetCE(model_conf=model_conf, data_conf=conf)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "9717it [00:00, 11396.91it/s]\n",
      "500it [00:00, 11221.56it/s]\n",
      "100%|| 122/122 [00:01<00:00, 64.07it/s]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "valid embeds saved\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "100%|| 125/125 [00:02<00:00, 62.01it/s]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "test embeds saved\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "100%|| 2308/2308 [00:35<00:00, 64.66it/s]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "train embeds saved\n"
     ]
    }
   ],
   "source": [
    "create_embeddings(conf, model_conf)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [],
   "source": [
    "train_embeds = pd.read_csv(conf.train_embed_path, index_col=0)\n",
    "test_embeds = pd.read_csv(conf.test_embed_path, index_col=0)\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 25,
   "metadata": {},
   "outputs": [],
   "source": [
    "train_y = pd.read_parquet(conf.train_path)[conf.features.target_col]\n",
    "test_y = pd.read_parquet(conf.test_path)[conf.features.target_col].astype(int)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 26,
   "metadata": {},
   "outputs": [],
   "source": [
    "train = train_embeds.join(train_y)\n",
    "train = train.dropna()\n",
    "train[conf.features.target_col] = train[conf.features.target_col].astype(int)\n",
    "\n",
    "test = test_embeds.join(test_y)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 27,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": 37,
   "metadata": {},
   "outputs": [],
   "source": [
    "from lightgbm import LGBMClassifier"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 39,
   "metadata": {},
   "outputs": [],
   "source": [
    "params = {\n",
    "    \"n_estimators\": 500,\n",
    "    \"boosting_type\": \"gbdt\",\n",
    "    \"objective\": \"binary\",\n",
    "    \"metric\": \"auc\",\n",
    "    \"subsample\": 0.5,\n",
    "    \"subsample_freq\": 1,\n",
    "    \"learning_rate\": 0.02,\n",
    "    \"feature_fraction\": 0.75,\n",
    "    \"max_depth\": 6,\n",
    "    \"lambda_l1\": 1,\n",
    "    \"lambda_l2\": 1,\n",
    "    \"min_data_in_leaf\": 50,\n",
    "    \"random_state\": 42,\n",
    "    \"n_jobs\": 8,\n",
    "    \"reg_alpha\": None,\n",
    "    \"reg_lambda\": None,\n",
    "    \"colsample_bytree\": None,\n",
    "    \"min_child_samples\": None\n",
    "}"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 40,
   "metadata": {},
   "outputs": [],
   "source": [
    "model = LGBMClassifier(**params)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 41,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[LightGBM] [Warning] Accuracy may be bad since you didn't explicitly set num_leaves OR 2^max_depth > num_leaves. (num_leaves=31).\n",
      "[LightGBM] [Warning] Accuracy may be bad since you didn't explicitly set num_leaves OR 2^max_depth > num_leaves. (num_leaves=31).\n",
      "[LightGBM] [Info] Number of positive: 2474, number of negative: 2026\n",
      "[LightGBM] [Warning] Auto-choosing col-wise multi-threading, the overhead of testing was 0.001982 seconds.\n",
      "You can set `force_col_wise=true` to remove the overhead.\n",
      "[LightGBM] [Info] Total Bins 8160\n",
      "[LightGBM] [Info] Number of data points in the train set: 4500, number of used features: 32\n",
      "[LightGBM] [Info] [binary:BoostFromScore]: pavg=0.549778 -> initscore=0.199773\n",
      "[LightGBM] [Info] Start training from score 0.199773\n",
      "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
      "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
      "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
      "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
      "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
      "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
      "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
      "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
      "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
      "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
      "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
      "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
      "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
      "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
      "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
      "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
      "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
      "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
      "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
      "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
      "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
      "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
      "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
      "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
      "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
      "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
      "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
      "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
      "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
      "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
      "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
      "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
      "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
      "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
      "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
      "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
      "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
      "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
      "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
      "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
      "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
      "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
      "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
      "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
      "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
      "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
      "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
      "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
      "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
      "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
      "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
      "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
      "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
      "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
      "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
      "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
      "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
      "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
      "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
      "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
      "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
      "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
      "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
      "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
      "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
      "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
      "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
      "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
      "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
      "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
      "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
      "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
      "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
      "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
      "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
      "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
      "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
      "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
      "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
      "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
      "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
      "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
      "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
      "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
      "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
      "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
      "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
      "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
      "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
      "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
      "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
      "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
      "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
      "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
      "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
      "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
      "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
      "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
      "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
      "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
      "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
      "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
      "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
      "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
      "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
      "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
      "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
      "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
      "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
      "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
      "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
      "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
      "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
      "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
      "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
      "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
      "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
      "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
      "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
      "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
      "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
      "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
      "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
      "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
      "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
      "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
      "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
      "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
      "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
      "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
      "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
      "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
      "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
      "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
      "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
      "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
      "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
      "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
      "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
      "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
      "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
      "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
      "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
      "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
      "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
      "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
      "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
      "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
      "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
      "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
      "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
      "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
      "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
      "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
      "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
      "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
      "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
      "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
      "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
      "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
      "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
      "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
      "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
      "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
      "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
      "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
      "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
      "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
      "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
      "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
      "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
      "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
      "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
      "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
      "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
      "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
      "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
      "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
      "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
      "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
      "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
      "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
      "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
      "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
      "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
      "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
      "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
      "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
      "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
      "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
      "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
      "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
      "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
      "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
      "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
      "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
      "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
      "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
      "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
      "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
      "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
      "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
      "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
      "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
      "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
      "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
      "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
      "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
      "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
      "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
      "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
      "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
      "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
      "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
      "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
      "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
      "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
      "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
      "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
      "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
      "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
      "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
      "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
      "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
      "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
      "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
      "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
      "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
      "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
      "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
      "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
      "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
      "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
      "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
      "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
      "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
      "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
      "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
      "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
      "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
      "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
      "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
      "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
      "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
      "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
      "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
      "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
      "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
      "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
      "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
      "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
      "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
      "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
      "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
      "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
      "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
      "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
      "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
      "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
      "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
      "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
      "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
      "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
      "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
      "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
      "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
      "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
      "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
      "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
      "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
      "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
      "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
      "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
      "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
      "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
      "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
      "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
      "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
      "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
      "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
      "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
      "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
      "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
      "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
      "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
      "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
      "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
      "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
      "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
      "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
      "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
      "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
      "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
      "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
      "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
      "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
      "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
      "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
      "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
      "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
      "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
      "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
      "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
      "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
      "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
      "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
      "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
      "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
      "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
      "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
      "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
      "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
      "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
      "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
      "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
      "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
      "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
      "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
      "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
      "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
      "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
      "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
      "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
      "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
      "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
      "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
      "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
      "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
      "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
      "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
      "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
      "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
      "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
      "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
      "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
      "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
      "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
      "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
      "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
      "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
      "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
      "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
      "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
      "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
      "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
      "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
      "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
      "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
      "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
      "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
      "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
      "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
      "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
      "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
      "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
      "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
      "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
      "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
      "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
      "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
      "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
      "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
      "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
      "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
      "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
      "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
      "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
      "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
      "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
      "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
      "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
      "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
      "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
      "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
      "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
      "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
      "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
      "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
      "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
      "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
      "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
      "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
      "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
      "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
      "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
      "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
      "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
      "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
      "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
      "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
      "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
      "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
      "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
      "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
      "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
      "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
      "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
      "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
      "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
      "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
      "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
      "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
      "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
      "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
      "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
      "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
      "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
      "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
      "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
      "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
      "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
      "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
      "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
      "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
      "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
      "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
      "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
      "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
      "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
      "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
      "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
      "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
      "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
      "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
      "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
      "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
      "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
      "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
      "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
      "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
      "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
      "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
      "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
      "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
      "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
      "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
      "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
      "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
      "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
      "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
      "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
      "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
      "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
      "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
      "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
      "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
      "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
      "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
      "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
      "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
      "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
      "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
      "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
      "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
      "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
      "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
      "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
      "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
      "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
      "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
      "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
      "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
      "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
      "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
      "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
      "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
      "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
      "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
      "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
      "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
      "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
      "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
      "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
      "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
      "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
      "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
      "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
      "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
      "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
      "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
      "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
      "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
      "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
      "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
      "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
      "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
      "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
      "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
      "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
      "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
      "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
      "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
      "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
      "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
      "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
      "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n"
     ]
    },
    {
     "data": {
      "text/html": [
       "<style>#sk-container-id-4 {color: black;}#sk-container-id-4 pre{padding: 0;}#sk-container-id-4 div.sk-toggleable {background-color: white;}#sk-container-id-4 label.sk-toggleable__label {cursor: pointer;display: block;width: 100%;margin-bottom: 0;padding: 0.3em;box-sizing: border-box;text-align: center;}#sk-container-id-4 label.sk-toggleable__label-arrow:before {content: \"\";float: left;margin-right: 0.25em;color: #696969;}#sk-container-id-4 label.sk-toggleable__label-arrow:hover:before {color: black;}#sk-container-id-4 div.sk-estimator:hover label.sk-toggleable__label-arrow:before {color: black;}#sk-container-id-4 div.sk-toggleable__content {max-height: 0;max-width: 0;overflow: hidden;text-align: left;background-color: #f0f8ff;}#sk-container-id-4 div.sk-toggleable__content pre {margin: 0.2em;color: black;border-radius: 0.25em;background-color: #f0f8ff;}#sk-container-id-4 input.sk-toggleable__control:checked~div.sk-toggleable__content {max-height: 200px;max-width: 100%;overflow: auto;}#sk-container-id-4 input.sk-toggleable__control:checked~label.sk-toggleable__label-arrow:before {content: \"\";}#sk-container-id-4 div.sk-estimator input.sk-toggleable__control:checked~label.sk-toggleable__label {background-color: #d4ebff;}#sk-container-id-4 div.sk-label input.sk-toggleable__control:checked~label.sk-toggleable__label {background-color: #d4ebff;}#sk-container-id-4 input.sk-hidden--visually {border: 0;clip: rect(1px 1px 1px 1px);clip: rect(1px, 1px, 1px, 1px);height: 1px;margin: -1px;overflow: hidden;padding: 0;position: absolute;width: 1px;}#sk-container-id-4 div.sk-estimator {font-family: monospace;background-color: #f0f8ff;border: 1px dotted black;border-radius: 0.25em;box-sizing: border-box;margin-bottom: 0.5em;}#sk-container-id-4 div.sk-estimator:hover {background-color: #d4ebff;}#sk-container-id-4 div.sk-parallel-item::after {content: \"\";width: 100%;border-bottom: 1px solid gray;flex-grow: 1;}#sk-container-id-4 div.sk-label:hover label.sk-toggleable__label {background-color: #d4ebff;}#sk-container-id-4 div.sk-serial::before {content: \"\";position: absolute;border-left: 1px solid gray;box-sizing: border-box;top: 0;bottom: 0;left: 50%;z-index: 0;}#sk-container-id-4 div.sk-serial {display: flex;flex-direction: column;align-items: center;background-color: white;padding-right: 0.2em;padding-left: 0.2em;position: relative;}#sk-container-id-4 div.sk-item {position: relative;z-index: 1;}#sk-container-id-4 div.sk-parallel {display: flex;align-items: stretch;justify-content: center;background-color: white;position: relative;}#sk-container-id-4 div.sk-item::before, #sk-container-id-4 div.sk-parallel-item::before {content: \"\";position: absolute;border-left: 1px solid gray;box-sizing: border-box;top: 0;bottom: 0;left: 50%;z-index: -1;}#sk-container-id-4 div.sk-parallel-item {display: flex;flex-direction: column;z-index: 1;position: relative;background-color: white;}#sk-container-id-4 div.sk-parallel-item:first-child::after {align-self: flex-end;width: 50%;}#sk-container-id-4 div.sk-parallel-item:last-child::after {align-self: flex-start;width: 50%;}#sk-container-id-4 div.sk-parallel-item:only-child::after {width: 0;}#sk-container-id-4 div.sk-dashed-wrapped {border: 1px dashed gray;margin: 0 0.4em 0.5em 0.4em;box-sizing: border-box;padding-bottom: 0.4em;background-color: white;}#sk-container-id-4 div.sk-label label {font-family: monospace;font-weight: bold;display: inline-block;line-height: 1.2em;}#sk-container-id-4 div.sk-label-container {text-align: center;}#sk-container-id-4 div.sk-container {/* jupyter's `normalize.less` sets `[hidden] { display: none; }` but bootstrap.min.css set `[hidden] { display: none !important; }` so we also need the `!important` here to be able to override the default hidden behavior on the sphinx rendered scikit-learn.org. See: https://github.com/scikit-learn/scikit-learn/issues/21755 */display: inline-block !important;position: relative;}#sk-container-id-4 div.sk-text-repr-fallback {display: none;}</style><div id=\"sk-container-id-4\" class=\"sk-top-container\"><div class=\"sk-text-repr-fallback\"><pre>LGBMClassifier(colsample_bytree=None, feature_fraction=0.75, lambda_l1=1,\n",
       "               lambda_l2=1, learning_rate=0.02, max_depth=6, metric=&#x27;auc&#x27;,\n",
       "               min_child_samples=None, min_data_in_leaf=50, n_estimators=500,\n",
       "               n_jobs=8, objective=&#x27;binary&#x27;, random_state=42, reg_alpha=None,\n",
       "               reg_lambda=None, subsample=0.5, subsample_freq=1)</pre><b>In a Jupyter environment, please rerun this cell to show the HTML representation or trust the notebook. <br />On GitHub, the HTML representation is unable to render, please try loading this page with nbviewer.org.</b></div><div class=\"sk-container\" hidden><div class=\"sk-item\"><div class=\"sk-estimator sk-toggleable\"><input class=\"sk-toggleable__control sk-hidden--visually\" id=\"sk-estimator-id-4\" type=\"checkbox\" checked><label for=\"sk-estimator-id-4\" class=\"sk-toggleable__label sk-toggleable__label-arrow\">LGBMClassifier</label><div class=\"sk-toggleable__content\"><pre>LGBMClassifier(colsample_bytree=None, feature_fraction=0.75, lambda_l1=1,\n",
       "               lambda_l2=1, learning_rate=0.02, max_depth=6, metric=&#x27;auc&#x27;,\n",
       "               min_child_samples=None, min_data_in_leaf=50, n_estimators=500,\n",
       "               n_jobs=8, objective=&#x27;binary&#x27;, random_state=42, reg_alpha=None,\n",
       "               reg_lambda=None, subsample=0.5, subsample_freq=1)</pre></div></div></div></div></div>"
      ],
      "text/plain": [
       "LGBMClassifier(colsample_bytree=None, feature_fraction=0.75, lambda_l1=1,\n",
       "               lambda_l2=1, learning_rate=0.02, max_depth=6, metric='auc',\n",
       "               min_child_samples=None, min_data_in_leaf=50, n_estimators=500,\n",
       "               n_jobs=8, objective='binary', random_state=42, reg_alpha=None,\n",
       "               reg_lambda=None, subsample=0.5, subsample_freq=1)"
      ]
     },
     "execution_count": 41,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "model.fit(train.drop(columns=[conf.features.target_col]), train[conf.features.target_col])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 42,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[LightGBM] [Warning] Accuracy may be bad since you didn't explicitly set num_leaves OR 2^max_depth > num_leaves. (num_leaves=31).\n"
     ]
    }
   ],
   "source": [
    "y_pred = model.predict(test.drop(columns=[conf.features.target_col]))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 43,
   "metadata": {},
   "outputs": [],
   "source": [
    "from sklearn.metrics import accuracy_score, roc_auc_score"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 44,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "0.49788132100047816"
      ]
     },
     "execution_count": 44,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "roc_auc_score(test_y, y_pred)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 27,
   "metadata": {},
   "outputs": [],
   "source": [
    "ckpt = torch.load(chkp_path)\n",
    "net.load_state_dict(ckpt[\"model\"])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 36,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "tensor([0, 0, 0, 0, 0, 1, 1, 1, 1, 1, 2, 2, 2, 2, 2, 3, 3, 3, 3, 3])"
      ]
     },
     "execution_count": 36,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "batch[1]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 49,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "0it [00:00, ?it/s]"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "9717it [00:00, 13852.25it/s]\n"
     ]
    }
   ],
   "source": [
    "split_strategy_dict = {\n",
    "    \"split_strategy\": \"NoSplit\"\n",
    "}\n",
    "train_data, valid_data = prepare_data(conf)\n",
    "\n",
    "train_dataset = SplittingDataset(\n",
    "    train_data,\n",
    "    split_strategy.create(**split_strategy_dict),\n",
    "    # conf.features.target_col,\n",
    ")\n",
    "train_dataset = TargetEnumeratorDataset(train_dataset)\n",
    "# train_dataset = TargetDataset(train_dataset)\n",
    "train_dataset = ConvertingTrxDataset(train_dataset)\n",
    "#         .\n",
    "#       \n",
    "\n",
    "\n",
    "train_loader = DataLoader(\n",
    "    dataset=train_dataset,\n",
    "    shuffle=True,\n",
    "    collate_fn=collate_splitted_rows,\n",
    "    num_workers=conf.train.num_workers,\n",
    "    batch_size=conf.train.batch_size,\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "100%|| 2308/2308 [00:31<00:00, 74.11it/s]\n"
     ]
    }
   ],
   "source": [
    "with torch.no_grad():\n",
    "    train_embeddings = []\n",
    "    ids = []\n",
    "    for batch in tqdm(train_loader):\n",
    "        out = net(batch[0])\n",
    "        embeddings = out['z'].view(4, -1)\n",
    "        train_embeddings.append(embeddings)\n",
    "        ids.append(batch[1])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "metadata": {},
   "outputs": [],
   "source": [
    "all_embeds = torch.cat(train_embeddings)\n",
    "all_indices = torch.cat(ids)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 21,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "torch.Size([9232, 32])"
      ]
     },
     "execution_count": 21,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "all_embeds.size()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 22,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>0</th>\n",
       "      <th>1</th>\n",
       "      <th>2</th>\n",
       "      <th>3</th>\n",
       "      <th>4</th>\n",
       "      <th>5</th>\n",
       "      <th>6</th>\n",
       "      <th>7</th>\n",
       "      <th>8</th>\n",
       "      <th>9</th>\n",
       "      <th>...</th>\n",
       "      <th>22</th>\n",
       "      <th>23</th>\n",
       "      <th>24</th>\n",
       "      <th>25</th>\n",
       "      <th>26</th>\n",
       "      <th>27</th>\n",
       "      <th>28</th>\n",
       "      <th>29</th>\n",
       "      <th>30</th>\n",
       "      <th>31</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>7443</th>\n",
       "      <td>0.102797</td>\n",
       "      <td>1.169541</td>\n",
       "      <td>0.019089</td>\n",
       "      <td>-0.514786</td>\n",
       "      <td>-1.388183</td>\n",
       "      <td>0.961653</td>\n",
       "      <td>-0.590597</td>\n",
       "      <td>-0.262099</td>\n",
       "      <td>0.144197</td>\n",
       "      <td>1.172947</td>\n",
       "      <td>...</td>\n",
       "      <td>1.550775</td>\n",
       "      <td>1.656553</td>\n",
       "      <td>-1.211777</td>\n",
       "      <td>0.713178</td>\n",
       "      <td>-0.422202</td>\n",
       "      <td>0.057362</td>\n",
       "      <td>-0.811645</td>\n",
       "      <td>-0.603588</td>\n",
       "      <td>-0.488893</td>\n",
       "      <td>0.160870</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>8096</th>\n",
       "      <td>0.803449</td>\n",
       "      <td>1.956381</td>\n",
       "      <td>0.596296</td>\n",
       "      <td>0.496732</td>\n",
       "      <td>0.486962</td>\n",
       "      <td>-1.346716</td>\n",
       "      <td>-1.141745</td>\n",
       "      <td>0.070024</td>\n",
       "      <td>-1.569932</td>\n",
       "      <td>-0.472612</td>\n",
       "      <td>...</td>\n",
       "      <td>-0.723514</td>\n",
       "      <td>0.423374</td>\n",
       "      <td>-1.114043</td>\n",
       "      <td>2.086276</td>\n",
       "      <td>-2.363746</td>\n",
       "      <td>1.266546</td>\n",
       "      <td>-0.149737</td>\n",
       "      <td>-0.264849</td>\n",
       "      <td>-0.322092</td>\n",
       "      <td>-1.082480</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3679</th>\n",
       "      <td>-0.002727</td>\n",
       "      <td>-1.473100</td>\n",
       "      <td>1.137815</td>\n",
       "      <td>0.073783</td>\n",
       "      <td>0.528109</td>\n",
       "      <td>-0.423857</td>\n",
       "      <td>-0.033820</td>\n",
       "      <td>0.668729</td>\n",
       "      <td>1.464864</td>\n",
       "      <td>-1.710192</td>\n",
       "      <td>...</td>\n",
       "      <td>-0.659925</td>\n",
       "      <td>-0.128479</td>\n",
       "      <td>-0.790339</td>\n",
       "      <td>1.412809</td>\n",
       "      <td>0.014140</td>\n",
       "      <td>0.823128</td>\n",
       "      <td>-0.271114</td>\n",
       "      <td>0.360713</td>\n",
       "      <td>0.073781</td>\n",
       "      <td>1.201015</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>7751</th>\n",
       "      <td>0.523437</td>\n",
       "      <td>-1.091199</td>\n",
       "      <td>0.383193</td>\n",
       "      <td>-0.536097</td>\n",
       "      <td>0.407316</td>\n",
       "      <td>0.998033</td>\n",
       "      <td>-1.337681</td>\n",
       "      <td>-0.492494</td>\n",
       "      <td>-0.177824</td>\n",
       "      <td>0.827958</td>\n",
       "      <td>...</td>\n",
       "      <td>0.050278</td>\n",
       "      <td>-0.186300</td>\n",
       "      <td>-0.929536</td>\n",
       "      <td>0.382475</td>\n",
       "      <td>0.188110</td>\n",
       "      <td>-0.039815</td>\n",
       "      <td>-0.135864</td>\n",
       "      <td>0.371854</td>\n",
       "      <td>-0.226536</td>\n",
       "      <td>-1.075974</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1058</th>\n",
       "      <td>-0.090836</td>\n",
       "      <td>0.931548</td>\n",
       "      <td>0.487700</td>\n",
       "      <td>1.544706</td>\n",
       "      <td>0.134991</td>\n",
       "      <td>-0.388189</td>\n",
       "      <td>-0.351403</td>\n",
       "      <td>0.690511</td>\n",
       "      <td>0.369822</td>\n",
       "      <td>-0.996384</td>\n",
       "      <td>...</td>\n",
       "      <td>0.037646</td>\n",
       "      <td>0.474755</td>\n",
       "      <td>0.105620</td>\n",
       "      <td>1.116618</td>\n",
       "      <td>0.331259</td>\n",
       "      <td>-0.752344</td>\n",
       "      <td>-1.333730</td>\n",
       "      <td>0.134372</td>\n",
       "      <td>-1.005091</td>\n",
       "      <td>-0.208797</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>...</th>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>5050</th>\n",
       "      <td>0.647142</td>\n",
       "      <td>0.396011</td>\n",
       "      <td>-1.058875</td>\n",
       "      <td>1.620824</td>\n",
       "      <td>-2.104278</td>\n",
       "      <td>0.130711</td>\n",
       "      <td>0.205906</td>\n",
       "      <td>1.588393</td>\n",
       "      <td>-0.809435</td>\n",
       "      <td>0.058372</td>\n",
       "      <td>...</td>\n",
       "      <td>0.256889</td>\n",
       "      <td>-0.805244</td>\n",
       "      <td>-1.666883</td>\n",
       "      <td>-0.457891</td>\n",
       "      <td>-0.153310</td>\n",
       "      <td>1.100958</td>\n",
       "      <td>-0.033816</td>\n",
       "      <td>-0.539116</td>\n",
       "      <td>-0.516178</td>\n",
       "      <td>0.900175</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>7619</th>\n",
       "      <td>-0.387352</td>\n",
       "      <td>0.407266</td>\n",
       "      <td>0.045567</td>\n",
       "      <td>1.079472</td>\n",
       "      <td>0.276017</td>\n",
       "      <td>0.598434</td>\n",
       "      <td>0.116289</td>\n",
       "      <td>-0.153847</td>\n",
       "      <td>-0.532574</td>\n",
       "      <td>-0.304695</td>\n",
       "      <td>...</td>\n",
       "      <td>-0.291467</td>\n",
       "      <td>0.979066</td>\n",
       "      <td>-2.529730</td>\n",
       "      <td>-0.883053</td>\n",
       "      <td>-0.234192</td>\n",
       "      <td>1.234930</td>\n",
       "      <td>-0.504991</td>\n",
       "      <td>0.397301</td>\n",
       "      <td>-1.507193</td>\n",
       "      <td>-1.275558</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4744</th>\n",
       "      <td>-0.665543</td>\n",
       "      <td>1.066480</td>\n",
       "      <td>0.286753</td>\n",
       "      <td>0.817286</td>\n",
       "      <td>-1.457128</td>\n",
       "      <td>0.637894</td>\n",
       "      <td>0.323675</td>\n",
       "      <td>1.191625</td>\n",
       "      <td>-0.373805</td>\n",
       "      <td>-0.026182</td>\n",
       "      <td>...</td>\n",
       "      <td>-0.252915</td>\n",
       "      <td>0.829856</td>\n",
       "      <td>2.367053</td>\n",
       "      <td>1.139469</td>\n",
       "      <td>-0.526248</td>\n",
       "      <td>0.331766</td>\n",
       "      <td>-0.604189</td>\n",
       "      <td>0.403450</td>\n",
       "      <td>-2.052348</td>\n",
       "      <td>-1.497805</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>8663</th>\n",
       "      <td>0.188203</td>\n",
       "      <td>-0.145553</td>\n",
       "      <td>0.806009</td>\n",
       "      <td>-0.384668</td>\n",
       "      <td>-0.735343</td>\n",
       "      <td>-0.895407</td>\n",
       "      <td>-1.491430</td>\n",
       "      <td>1.100353</td>\n",
       "      <td>1.275650</td>\n",
       "      <td>0.737131</td>\n",
       "      <td>...</td>\n",
       "      <td>-0.490947</td>\n",
       "      <td>-1.864704</td>\n",
       "      <td>-0.011014</td>\n",
       "      <td>-0.212244</td>\n",
       "      <td>0.133844</td>\n",
       "      <td>0.649861</td>\n",
       "      <td>0.691730</td>\n",
       "      <td>-0.337050</td>\n",
       "      <td>-0.022557</td>\n",
       "      <td>0.339424</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>8239</th>\n",
       "      <td>-0.282619</td>\n",
       "      <td>1.473131</td>\n",
       "      <td>0.222512</td>\n",
       "      <td>-0.791756</td>\n",
       "      <td>-0.829103</td>\n",
       "      <td>1.993044</td>\n",
       "      <td>1.280572</td>\n",
       "      <td>2.522489</td>\n",
       "      <td>1.288986</td>\n",
       "      <td>0.759570</td>\n",
       "      <td>...</td>\n",
       "      <td>1.366453</td>\n",
       "      <td>-0.027310</td>\n",
       "      <td>0.828379</td>\n",
       "      <td>1.281615</td>\n",
       "      <td>0.028337</td>\n",
       "      <td>-1.453597</td>\n",
       "      <td>-1.242622</td>\n",
       "      <td>0.122499</td>\n",
       "      <td>0.269273</td>\n",
       "      <td>-0.909372</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "<p>9232 rows  32 columns</p>\n",
       "</div>"
      ],
      "text/plain": [
       "            0         1         2         3         4         5         6   \\\n",
       "7443  0.102797  1.169541  0.019089 -0.514786 -1.388183  0.961653 -0.590597   \n",
       "8096  0.803449  1.956381  0.596296  0.496732  0.486962 -1.346716 -1.141745   \n",
       "3679 -0.002727 -1.473100  1.137815  0.073783  0.528109 -0.423857 -0.033820   \n",
       "7751  0.523437 -1.091199  0.383193 -0.536097  0.407316  0.998033 -1.337681   \n",
       "1058 -0.090836  0.931548  0.487700  1.544706  0.134991 -0.388189 -0.351403   \n",
       "...        ...       ...       ...       ...       ...       ...       ...   \n",
       "5050  0.647142  0.396011 -1.058875  1.620824 -2.104278  0.130711  0.205906   \n",
       "7619 -0.387352  0.407266  0.045567  1.079472  0.276017  0.598434  0.116289   \n",
       "4744 -0.665543  1.066480  0.286753  0.817286 -1.457128  0.637894  0.323675   \n",
       "8663  0.188203 -0.145553  0.806009 -0.384668 -0.735343 -0.895407 -1.491430   \n",
       "8239 -0.282619  1.473131  0.222512 -0.791756 -0.829103  1.993044  1.280572   \n",
       "\n",
       "            7         8         9   ...        22        23        24  \\\n",
       "7443 -0.262099  0.144197  1.172947  ...  1.550775  1.656553 -1.211777   \n",
       "8096  0.070024 -1.569932 -0.472612  ... -0.723514  0.423374 -1.114043   \n",
       "3679  0.668729  1.464864 -1.710192  ... -0.659925 -0.128479 -0.790339   \n",
       "7751 -0.492494 -0.177824  0.827958  ...  0.050278 -0.186300 -0.929536   \n",
       "1058  0.690511  0.369822 -0.996384  ...  0.037646  0.474755  0.105620   \n",
       "...        ...       ...       ...  ...       ...       ...       ...   \n",
       "5050  1.588393 -0.809435  0.058372  ...  0.256889 -0.805244 -1.666883   \n",
       "7619 -0.153847 -0.532574 -0.304695  ... -0.291467  0.979066 -2.529730   \n",
       "4744  1.191625 -0.373805 -0.026182  ... -0.252915  0.829856  2.367053   \n",
       "8663  1.100353  1.275650  0.737131  ... -0.490947 -1.864704 -0.011014   \n",
       "8239  2.522489  1.288986  0.759570  ...  1.366453 -0.027310  0.828379   \n",
       "\n",
       "            25        26        27        28        29        30        31  \n",
       "7443  0.713178 -0.422202  0.057362 -0.811645 -0.603588 -0.488893  0.160870  \n",
       "8096  2.086276 -2.363746  1.266546 -0.149737 -0.264849 -0.322092 -1.082480  \n",
       "3679  1.412809  0.014140  0.823128 -0.271114  0.360713  0.073781  1.201015  \n",
       "7751  0.382475  0.188110 -0.039815 -0.135864  0.371854 -0.226536 -1.075974  \n",
       "1058  1.116618  0.331259 -0.752344 -1.333730  0.134372 -1.005091 -0.208797  \n",
       "...        ...       ...       ...       ...       ...       ...       ...  \n",
       "5050 -0.457891 -0.153310  1.100958 -0.033816 -0.539116 -0.516178  0.900175  \n",
       "7619 -0.883053 -0.234192  1.234930 -0.504991  0.397301 -1.507193 -1.275558  \n",
       "4744  1.139469 -0.526248  0.331766 -0.604189  0.403450 -2.052348 -1.497805  \n",
       "8663 -0.212244  0.133844  0.649861  0.691730 -0.337050 -0.022557  0.339424  \n",
       "8239  1.281615  0.028337 -1.453597 -1.242622  0.122499  0.269273 -0.909372  \n",
       "\n",
       "[9232 rows x 32 columns]"
      ]
     },
     "execution_count": 22,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "pd.DataFrame(data=all_embeds.numpy(), index=all_indices.numpy())"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 29,
   "metadata": {},
   "outputs": [],
   "source": [
    "criterion = nn.CrossEntropyLoss(reduction='none')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 22,
   "metadata": {},
   "outputs": [],
   "source": [
    "out_dist = torch.rand(20, 4, 100)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 23,
   "metadata": {},
   "outputs": [],
   "source": [
    "target = torch.rand(20, 1, 100)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "torch.Size([2000, 4])"
      ]
     },
     "execution_count": 15,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "out_dist.view(-1, 4).size()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 33,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "tensor(141.1932)"
      ]
     },
     "execution_count": 33,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "criterion(out_dist, target.squeeze(1).long(), ).sum(dim=1).mean()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 47,
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "loss_out = net.loss(out)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "metadata": {
    "tags": []
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "{'elbo_loss': tensor(12326.9258, grad_fn=<AddBackward0>),\n",
       " 'kl_loss': tensor(0.5898, grad_fn=<MeanBackward0>),\n",
       " 'recon_loss': tensor(12326.8672, grad_fn=<MeanBackward0>)}"
      ]
     },
     "execution_count": 16,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "loss_out"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "trainer = MtandTrainer(\n",
    "    model=net,\n",
    "    optimizer=torch.optim.Adam(net.parameters(), lr=3e-4),\n",
    "    train_loader=train_loader,\n",
    "    val_loader=valid_loader,\n",
    "    total_iters=50,\n",
    "    iters_per_epoch=20,\n",
    "    ckpt_dir=\"./ckpt\",\n",
    "    ckpt_replace=True,\n",
    "    ckpt_track_metric=\"loss\",\n",
    "    metrics_on_train=True,\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "metadata": {
    "tags": []
   },
   "outputs": [
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "eb21d5f726fa43708ece01e5557e1f99",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "  0%|          | 0/20 [00:00<?, ?it/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "b98b399db3e9443395b42783a636fb68",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "  0%|          | 0/122 [00:00<?, ?it/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "182497721c094f468ef61c1bd33acb12",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "  0%|          | 0/20 [00:00<?, ?it/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "46d27938b4e245e095e8715abbb2ad70",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "  0%|          | 0/122 [00:00<?, ?it/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "eea6cd2d8dc44daa8dbd272e6327785d",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "  0%|          | 0/10 [00:00<?, ?it/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "9065eb3294be4583aa8c0f78d418c97a",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "  0%|          | 0/122 [00:00<?, ?it/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "trainer.run()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": 20,
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "def perform_epoch(model, batch, optimizer):\n",
    "    optimizer.zero_grad()\n",
    "    out = model(batch[0])\n",
    "    loss = model.loss(out)\n",
    "    loss['elbo_loss'].backward()\n",
    "    optimizer.step()\n",
    "\n",
    "    return loss\n",
    "\n",
    "def train(model, loader, optimizer, num_epochs):\n",
    "\n",
    "    model.train()\n",
    "    recon_loss = []\n",
    "    kl_loss = []\n",
    "    for epoch in range(num_epochs):\n",
    "        for i, batch in tqdm(enumerate(loader), total=len(loader)):\n",
    "            loss = perform_epoch(model, batch, optimizer)\n",
    "            recon_loss.append(loss['recon_loss'].detach().item())\n",
    "            kl_loss.append(loss['kl_loss'].detach().item())\n",
    "\n",
    "            if (i+1) % 1 == 0:\n",
    "                plot_losses(recon_loss, kl_loss)\n",
    "\n",
    "\n",
    "def plot_losses(recon_loss, kl_loss):\n",
    "    clear_output(True)\n",
    "    plt.plot(np.log(recon_loss), color='g', label='recon')\n",
    "    plt.plot(kl_loss, color='r', label='kl')\n",
    "    plt.legend()   \n",
    "    plt.show() "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 21,
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "lr = 3e-4\n",
    "net = MegaNet(model_conf=model_conf, data_conf=conf)\n",
    "optimizer = torch.optim.Adam(net.parameters(), lr=lr)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 22,
   "metadata": {
    "tags": []
   },
   "outputs": [
    {
     "data": {
      "image/png": "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",
      "text/plain": [
       "<Figure size 640x480 with 1 Axes>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "  1%|          | 26/2308 [00:07<11:32,  3.30it/s]\n"
     ]
    },
    {
     "ename": "KeyboardInterrupt",
     "evalue": "",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mKeyboardInterrupt\u001b[0m                         Traceback (most recent call last)",
      "Cell \u001b[0;32mIn[22], line 1\u001b[0m\n\u001b[0;32m----> 1\u001b[0m \u001b[43mtrain\u001b[49m\u001b[43m(\u001b[49m\u001b[43mnet\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mtrain_loader\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43moptimizer\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;241;43m10\u001b[39;49m\u001b[43m)\u001b[49m\n",
      "Cell \u001b[0;32mIn[20], line 17\u001b[0m, in \u001b[0;36mtrain\u001b[0;34m(model, loader, optimizer, num_epochs)\u001b[0m\n\u001b[1;32m     15\u001b[0m \u001b[38;5;28;01mfor\u001b[39;00m epoch \u001b[38;5;129;01min\u001b[39;00m \u001b[38;5;28mrange\u001b[39m(num_epochs):\n\u001b[1;32m     16\u001b[0m     \u001b[38;5;28;01mfor\u001b[39;00m i, batch \u001b[38;5;129;01min\u001b[39;00m tqdm(\u001b[38;5;28menumerate\u001b[39m(loader), total\u001b[38;5;241m=\u001b[39m\u001b[38;5;28mlen\u001b[39m(loader)):\n\u001b[0;32m---> 17\u001b[0m         loss \u001b[38;5;241m=\u001b[39m \u001b[43mperform_epoch\u001b[49m\u001b[43m(\u001b[49m\u001b[43mmodel\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mbatch\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43moptimizer\u001b[49m\u001b[43m)\u001b[49m\n\u001b[1;32m     18\u001b[0m         recon_loss\u001b[38;5;241m.\u001b[39mappend(loss[\u001b[38;5;124m'\u001b[39m\u001b[38;5;124mrecon_loss\u001b[39m\u001b[38;5;124m'\u001b[39m]\u001b[38;5;241m.\u001b[39mdetach()\u001b[38;5;241m.\u001b[39mitem())\n\u001b[1;32m     19\u001b[0m         kl_loss\u001b[38;5;241m.\u001b[39mappend(loss[\u001b[38;5;124m'\u001b[39m\u001b[38;5;124mkl_loss\u001b[39m\u001b[38;5;124m'\u001b[39m]\u001b[38;5;241m.\u001b[39mdetach()\u001b[38;5;241m.\u001b[39mitem())\n",
      "Cell \u001b[0;32mIn[20], line 3\u001b[0m, in \u001b[0;36mperform_epoch\u001b[0;34m(model, batch, optimizer)\u001b[0m\n\u001b[1;32m      1\u001b[0m \u001b[38;5;28;01mdef\u001b[39;00m \u001b[38;5;21mperform_epoch\u001b[39m(model, batch, optimizer):\n\u001b[1;32m      2\u001b[0m     optimizer\u001b[38;5;241m.\u001b[39mzero_grad()\n\u001b[0;32m----> 3\u001b[0m     out \u001b[38;5;241m=\u001b[39m \u001b[43mmodel\u001b[49m\u001b[43m(\u001b[49m\u001b[43mbatch\u001b[49m\u001b[43m[\u001b[49m\u001b[38;5;241;43m0\u001b[39;49m\u001b[43m]\u001b[49m\u001b[43m)\u001b[49m\n\u001b[1;32m      4\u001b[0m     loss \u001b[38;5;241m=\u001b[39m model\u001b[38;5;241m.\u001b[39mloss(out)\n\u001b[1;32m      5\u001b[0m     loss[\u001b[38;5;124m'\u001b[39m\u001b[38;5;124melbo_loss\u001b[39m\u001b[38;5;124m'\u001b[39m]\u001b[38;5;241m.\u001b[39mbackward()\n",
      "File \u001b[0;32m~/anaconda3/lib/python3.9/site-packages/torch/nn/modules/module.py:1501\u001b[0m, in \u001b[0;36mModule._call_impl\u001b[0;34m(self, *args, **kwargs)\u001b[0m\n\u001b[1;32m   1496\u001b[0m \u001b[38;5;66;03m# If we don't have any hooks, we want to skip the rest of the logic in\u001b[39;00m\n\u001b[1;32m   1497\u001b[0m \u001b[38;5;66;03m# this function, and just call forward.\u001b[39;00m\n\u001b[1;32m   1498\u001b[0m \u001b[38;5;28;01mif\u001b[39;00m \u001b[38;5;129;01mnot\u001b[39;00m (\u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_backward_hooks \u001b[38;5;129;01mor\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_backward_pre_hooks \u001b[38;5;129;01mor\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_forward_hooks \u001b[38;5;129;01mor\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_forward_pre_hooks\n\u001b[1;32m   1499\u001b[0m         \u001b[38;5;129;01mor\u001b[39;00m _global_backward_pre_hooks \u001b[38;5;129;01mor\u001b[39;00m _global_backward_hooks\n\u001b[1;32m   1500\u001b[0m         \u001b[38;5;129;01mor\u001b[39;00m _global_forward_hooks \u001b[38;5;129;01mor\u001b[39;00m _global_forward_pre_hooks):\n\u001b[0;32m-> 1501\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[43mforward_call\u001b[49m\u001b[43m(\u001b[49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[43margs\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[43mkwargs\u001b[49m\u001b[43m)\u001b[49m\n\u001b[1;32m   1502\u001b[0m \u001b[38;5;66;03m# Do not call functions when jit is used\u001b[39;00m\n\u001b[1;32m   1503\u001b[0m full_backward_hooks, non_full_backward_hooks \u001b[38;5;241m=\u001b[39m [], []\n",
      "File \u001b[0;32m~/workspace/event_seq/experiments/rosbank/notebooks/../../../src/models/mTAND/model.py:267\u001b[0m, in \u001b[0;36mMegaNet.forward\u001b[0;34m(self, padded_batch)\u001b[0m\n\u001b[1;32m    260\u001b[0m z \u001b[38;5;241m=\u001b[39m sample_z(qz_mean, qz_logstd, \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mmodel_conf\u001b[38;5;241m.\u001b[39mk_iwae)\n\u001b[1;32m    262\u001b[0m iwae_steps \u001b[38;5;241m=\u001b[39m (\n\u001b[1;32m    263\u001b[0m     time_steps[\u001b[38;5;28;01mNone\u001b[39;00m, :, :]\n\u001b[1;32m    264\u001b[0m     \u001b[38;5;241m.\u001b[39mrepeat(\u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mmodel_conf\u001b[38;5;241m.\u001b[39mk_iwae, \u001b[38;5;241m1\u001b[39m, \u001b[38;5;241m1\u001b[39m)\n\u001b[1;32m    265\u001b[0m     \u001b[38;5;241m.\u001b[39mview(\u001b[38;5;241m-\u001b[39m\u001b[38;5;241m1\u001b[39m, time_steps\u001b[38;5;241m.\u001b[39mshape[\u001b[38;5;241m1\u001b[39m])\n\u001b[1;32m    266\u001b[0m )\n\u001b[0;32m--> 267\u001b[0m dec_out \u001b[38;5;241m=\u001b[39m \u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mdecoder\u001b[49m\u001b[43m(\u001b[49m\u001b[43mz\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43miwae_steps\u001b[49m\u001b[43m)\u001b[49m\n\u001b[1;32m    268\u001b[0m dec_out \u001b[38;5;241m=\u001b[39m dec_out\u001b[38;5;241m.\u001b[39mview(\n\u001b[1;32m    269\u001b[0m     \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mmodel_conf\u001b[38;5;241m.\u001b[39mk_iwae,\n\u001b[1;32m    270\u001b[0m     time_steps\u001b[38;5;241m.\u001b[39mshape[\u001b[38;5;241m0\u001b[39m],\n\u001b[1;32m    271\u001b[0m     dec_out\u001b[38;5;241m.\u001b[39mshape[\u001b[38;5;241m1\u001b[39m],\n\u001b[1;32m    272\u001b[0m     dec_out\u001b[38;5;241m.\u001b[39mshape[\u001b[38;5;241m2\u001b[39m],\n\u001b[1;32m    273\u001b[0m )\n\u001b[1;32m    275\u001b[0m \u001b[38;5;28;01mreturn\u001b[39;00m {\n\u001b[1;32m    276\u001b[0m     \u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mx_recon\u001b[39m\u001b[38;5;124m\"\u001b[39m: dec_out,\n\u001b[1;32m    277\u001b[0m     \u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mz\u001b[39m\u001b[38;5;124m\"\u001b[39m: z,\n\u001b[0;32m   (...)\u001b[0m\n\u001b[1;32m    281\u001b[0m     \u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mlog_std\u001b[39m\u001b[38;5;124m\"\u001b[39m: qz_logstd,\n\u001b[1;32m    282\u001b[0m }\n",
      "File \u001b[0;32m~/anaconda3/lib/python3.9/site-packages/torch/nn/modules/module.py:1501\u001b[0m, in \u001b[0;36mModule._call_impl\u001b[0;34m(self, *args, **kwargs)\u001b[0m\n\u001b[1;32m   1496\u001b[0m \u001b[38;5;66;03m# If we don't have any hooks, we want to skip the rest of the logic in\u001b[39;00m\n\u001b[1;32m   1497\u001b[0m \u001b[38;5;66;03m# this function, and just call forward.\u001b[39;00m\n\u001b[1;32m   1498\u001b[0m \u001b[38;5;28;01mif\u001b[39;00m \u001b[38;5;129;01mnot\u001b[39;00m (\u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_backward_hooks \u001b[38;5;129;01mor\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_backward_pre_hooks \u001b[38;5;129;01mor\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_forward_hooks \u001b[38;5;129;01mor\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_forward_pre_hooks\n\u001b[1;32m   1499\u001b[0m         \u001b[38;5;129;01mor\u001b[39;00m _global_backward_pre_hooks \u001b[38;5;129;01mor\u001b[39;00m _global_backward_hooks\n\u001b[1;32m   1500\u001b[0m         \u001b[38;5;129;01mor\u001b[39;00m _global_forward_hooks \u001b[38;5;129;01mor\u001b[39;00m _global_forward_pre_hooks):\n\u001b[0;32m-> 1501\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[43mforward_call\u001b[49m\u001b[43m(\u001b[49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[43margs\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[43mkwargs\u001b[49m\u001b[43m)\u001b[49m\n\u001b[1;32m   1502\u001b[0m \u001b[38;5;66;03m# Do not call functions when jit is used\u001b[39;00m\n\u001b[1;32m   1503\u001b[0m full_backward_hooks, non_full_backward_hooks \u001b[38;5;241m=\u001b[39m [], []\n",
      "File \u001b[0;32m~/workspace/event_seq/experiments/rosbank/notebooks/../../../src/models/mTAND/model.py:231\u001b[0m, in \u001b[0;36mMegaDecoder.forward\u001b[0;34m(self, z, time_steps)\u001b[0m\n\u001b[1;32m    230\u001b[0m \u001b[38;5;28;01mdef\u001b[39;00m \u001b[38;5;21mforward\u001b[39m(\u001b[38;5;28mself\u001b[39m, z, time_steps):\n\u001b[0;32m--> 231\u001b[0m     out \u001b[38;5;241m=\u001b[39m \u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mdecoder\u001b[49m\u001b[43m(\u001b[49m\u001b[43mz\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mtime_steps\u001b[49m\u001b[43m)\u001b[49m\n\u001b[1;32m    233\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m out\n",
      "File \u001b[0;32m~/anaconda3/lib/python3.9/site-packages/torch/nn/modules/module.py:1501\u001b[0m, in \u001b[0;36mModule._call_impl\u001b[0;34m(self, *args, **kwargs)\u001b[0m\n\u001b[1;32m   1496\u001b[0m \u001b[38;5;66;03m# If we don't have any hooks, we want to skip the rest of the logic in\u001b[39;00m\n\u001b[1;32m   1497\u001b[0m \u001b[38;5;66;03m# this function, and just call forward.\u001b[39;00m\n\u001b[1;32m   1498\u001b[0m \u001b[38;5;28;01mif\u001b[39;00m \u001b[38;5;129;01mnot\u001b[39;00m (\u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_backward_hooks \u001b[38;5;129;01mor\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_backward_pre_hooks \u001b[38;5;129;01mor\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_forward_hooks \u001b[38;5;129;01mor\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_forward_pre_hooks\n\u001b[1;32m   1499\u001b[0m         \u001b[38;5;129;01mor\u001b[39;00m _global_backward_pre_hooks \u001b[38;5;129;01mor\u001b[39;00m _global_backward_hooks\n\u001b[1;32m   1500\u001b[0m         \u001b[38;5;129;01mor\u001b[39;00m _global_forward_hooks \u001b[38;5;129;01mor\u001b[39;00m _global_forward_pre_hooks):\n\u001b[0;32m-> 1501\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[43mforward_call\u001b[49m\u001b[43m(\u001b[49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[43margs\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[43mkwargs\u001b[49m\u001b[43m)\u001b[49m\n\u001b[1;32m   1502\u001b[0m \u001b[38;5;66;03m# Do not call functions when jit is used\u001b[39;00m\n\u001b[1;32m   1503\u001b[0m full_backward_hooks, non_full_backward_hooks \u001b[38;5;241m=\u001b[39m [], []\n",
      "File \u001b[0;32m~/workspace/event_seq/experiments/rosbank/notebooks/../../../src/models/mTAND/model.py:137\u001b[0m, in \u001b[0;36mDecMtanRnn.forward\u001b[0;34m(self, z, time_steps)\u001b[0m\n\u001b[1;32m    134\u001b[0m keys \u001b[38;5;241m=\u001b[39m torch\u001b[38;5;241m.\u001b[39mcat(keys, dim\u001b[38;5;241m=\u001b[39m\u001b[38;5;241m1\u001b[39m)\u001b[38;5;241m.\u001b[39mto(\u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mdevice)\n\u001b[1;32m    135\u001b[0m querys \u001b[38;5;241m=\u001b[39m torch\u001b[38;5;241m.\u001b[39mcat(querys, dim\u001b[38;5;241m=\u001b[39m\u001b[38;5;241m1\u001b[39m)\u001b[38;5;241m.\u001b[39mto(\u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mdevice)\n\u001b[0;32m--> 137\u001b[0m out \u001b[38;5;241m=\u001b[39m \u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43matt\u001b[49m\u001b[43m(\u001b[49m\u001b[43mquerys\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mkeys\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mout\u001b[49m\u001b[43m)\u001b[49m\n\u001b[1;32m    138\u001b[0m out \u001b[38;5;241m=\u001b[39m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mz0_to_obs(out)\n\u001b[1;32m    139\u001b[0m \u001b[38;5;28;01mreturn\u001b[39;00m out\n",
      "File \u001b[0;32m~/anaconda3/lib/python3.9/site-packages/torch/nn/modules/module.py:1501\u001b[0m, in \u001b[0;36mModule._call_impl\u001b[0;34m(self, *args, **kwargs)\u001b[0m\n\u001b[1;32m   1496\u001b[0m \u001b[38;5;66;03m# If we don't have any hooks, we want to skip the rest of the logic in\u001b[39;00m\n\u001b[1;32m   1497\u001b[0m \u001b[38;5;66;03m# this function, and just call forward.\u001b[39;00m\n\u001b[1;32m   1498\u001b[0m \u001b[38;5;28;01mif\u001b[39;00m \u001b[38;5;129;01mnot\u001b[39;00m (\u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_backward_hooks \u001b[38;5;129;01mor\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_backward_pre_hooks \u001b[38;5;129;01mor\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_forward_hooks \u001b[38;5;129;01mor\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_forward_pre_hooks\n\u001b[1;32m   1499\u001b[0m         \u001b[38;5;129;01mor\u001b[39;00m _global_backward_pre_hooks \u001b[38;5;129;01mor\u001b[39;00m _global_backward_hooks\n\u001b[1;32m   1500\u001b[0m         \u001b[38;5;129;01mor\u001b[39;00m _global_forward_hooks \u001b[38;5;129;01mor\u001b[39;00m _global_forward_pre_hooks):\n\u001b[0;32m-> 1501\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[43mforward_call\u001b[49m\u001b[43m(\u001b[49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[43margs\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[43mkwargs\u001b[49m\u001b[43m)\u001b[49m\n\u001b[1;32m   1502\u001b[0m \u001b[38;5;66;03m# Do not call functions when jit is used\u001b[39;00m\n\u001b[1;32m   1503\u001b[0m full_backward_hooks, non_full_backward_hooks \u001b[38;5;241m=\u001b[39m [], []\n",
      "File \u001b[0;32m~/workspace/event_seq/experiments/rosbank/notebooks/../../../src/models/mTAND/model_utils.py:114\u001b[0m, in \u001b[0;36mMultiTimeAttention.forward\u001b[0;34m(self, query, key, value, mask, dropout)\u001b[0m\n\u001b[1;32m    105\u001b[0m value \u001b[38;5;241m=\u001b[39m value\u001b[38;5;241m.\u001b[39munsqueeze(\u001b[38;5;241m1\u001b[39m)\n\u001b[1;32m    107\u001b[0m query, key \u001b[38;5;241m=\u001b[39m [\n\u001b[1;32m    108\u001b[0m     \u001b[38;5;66;03m# divided to  bs, num_time_emb, num_heads, L, dim_head after transpose\u001b[39;00m\n\u001b[1;32m    109\u001b[0m     linear(x)\n\u001b[0;32m   (...)\u001b[0m\n\u001b[1;32m    112\u001b[0m     \u001b[38;5;28;01mfor\u001b[39;00m linear, x \u001b[38;5;129;01min\u001b[39;00m (\u001b[38;5;28mzip\u001b[39m(\u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mlinears, (query, key)))\n\u001b[1;32m    113\u001b[0m ]\n\u001b[0;32m--> 114\u001b[0m x, _ \u001b[38;5;241m=\u001b[39m \u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mattention\u001b[49m\u001b[43m(\u001b[49m\u001b[43mquery\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mkey\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mvalue\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mmask\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mdropout\u001b[49m\u001b[43m)\u001b[49m\n\u001b[1;32m    115\u001b[0m \u001b[38;5;66;03m# dimensions are bs, num_time_emb, num_ref_points, num_head * input_dim\u001b[39;00m\n\u001b[1;32m    116\u001b[0m \u001b[38;5;66;03m# transpose applied to flatten last 2 dimensions\u001b[39;00m\n\u001b[1;32m    117\u001b[0m x \u001b[38;5;241m=\u001b[39m x\u001b[38;5;241m.\u001b[39mtranspose(\u001b[38;5;241m2\u001b[39m, \u001b[38;5;241m3\u001b[39m)\u001b[38;5;241m.\u001b[39mcontiguous()\u001b[38;5;241m.\u001b[39mview(batch, \u001b[38;5;241m-\u001b[39m\u001b[38;5;241m1\u001b[39m, num_ref_points, \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mh \u001b[38;5;241m*\u001b[39m dim)\n",
      "File \u001b[0;32m~/workspace/event_seq/experiments/rosbank/notebooks/../../../src/models/mTAND/model_utils.py:85\u001b[0m, in \u001b[0;36mMultiTimeAttention.attention\u001b[0;34m(self, query, key, value, mask, dropout)\u001b[0m\n\u001b[1;32m     83\u001b[0m scores \u001b[38;5;241m=\u001b[39m torch\u001b[38;5;241m.\u001b[39mmatmul(query, key\u001b[38;5;241m.\u001b[39mtranspose(\u001b[38;5;241m-\u001b[39m\u001b[38;5;241m2\u001b[39m, \u001b[38;5;241m-\u001b[39m\u001b[38;5;241m1\u001b[39m)) \u001b[38;5;241m/\u001b[39m math\u001b[38;5;241m.\u001b[39msqrt(d_k)\n\u001b[1;32m     84\u001b[0m \u001b[38;5;66;03m# repeated to match dimension of input values\u001b[39;00m\n\u001b[0;32m---> 85\u001b[0m scores \u001b[38;5;241m=\u001b[39m \u001b[43mscores\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43munsqueeze\u001b[49m\u001b[43m(\u001b[49m\u001b[38;5;241;43m-\u001b[39;49m\u001b[38;5;241;43m1\u001b[39;49m\u001b[43m)\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mrepeat_interleave\u001b[49m\u001b[43m(\u001b[49m\u001b[43mdim\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mdim\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[38;5;241;43m-\u001b[39;49m\u001b[38;5;241;43m1\u001b[39;49m\u001b[43m)\u001b[49m\n\u001b[1;32m     86\u001b[0m \u001b[38;5;28;01mif\u001b[39;00m mask \u001b[38;5;129;01mis\u001b[39;00m \u001b[38;5;129;01mnot\u001b[39;00m \u001b[38;5;28;01mNone\u001b[39;00m:\n\u001b[1;32m     87\u001b[0m     scores \u001b[38;5;241m=\u001b[39m scores\u001b[38;5;241m.\u001b[39mmasked_fill(mask\u001b[38;5;241m.\u001b[39munsqueeze(\u001b[38;5;241m-\u001b[39m\u001b[38;5;241m3\u001b[39m) \u001b[38;5;241m==\u001b[39m \u001b[38;5;241m0\u001b[39m, \u001b[38;5;241m-\u001b[39m\u001b[38;5;241m1e9\u001b[39m)\n",
      "\u001b[0;31mKeyboardInterrupt\u001b[0m: "
     ]
    }
   ],
   "source": [
    "train(net, train_loader, optimizer, 10)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.10.12"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
