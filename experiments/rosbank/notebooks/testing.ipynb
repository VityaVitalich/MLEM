{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "import pandas as pd\n",
    "import numpy as np\n",
    "from tqdm import tqdm\n",
    "import pyarrow.parquet as pq\n",
    "import sys\n",
    "import torch\n",
    "import random\n",
    "import torch.nn as nn\n",
    "import torch.nn.functional as F\n",
    "import math\n",
    "\n",
    "sys.path.append('../../../')\n",
    "from configs.data_configs.rosbank import data_configs\n",
    "from configs.model_configs.mTAN.rosbank import model_configs\n",
    "from src.data_load.dataloader import create_data_loaders\n",
    "from src.models.mTAND.model import MegaEncoder"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "conf = data_configs()\n",
    "model_conf = model_configs()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>cl_id</th>\n",
       "      <th>amount</th>\n",
       "      <th>event_time</th>\n",
       "      <th>mcc</th>\n",
       "      <th>channel_type</th>\n",
       "      <th>currency</th>\n",
       "      <th>trx_category</th>\n",
       "      <th>trx_count</th>\n",
       "      <th>target_target_flag</th>\n",
       "      <th>target_target_sum</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>10018</td>\n",
       "      <td>[10.609081944147828, 10.596659732783579, 10.81...</td>\n",
       "      <td>[17120.38773148148, 17133.667800925927, 17134....</td>\n",
       "      <td>[13, 2, 13, 2, 1, 18, 13, 2, 13, 2, 5, 13, 9, ...</td>\n",
       "      <td>[1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1]</td>\n",
       "      <td>[1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1]</td>\n",
       "      <td>[5, 3, 5, 3, 1, 1, 5, 3, 5, 3, 1, 5, 5, 5, 5]</td>\n",
       "      <td>15</td>\n",
       "      <td>0</td>\n",
       "      <td>0.0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>10030</td>\n",
       "      <td>[4.61512051684126, 6.90875477931522, 10.598857...</td>\n",
       "      <td>[17141.0, 17141.0, 17145.0, 17147.0, 17147.0, ...</td>\n",
       "      <td>[9, 9, 21, 1, 25, 6, 14, 14, 3, 3, 3, 13, 1, 3...</td>\n",
       "      <td>[1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, ...</td>\n",
       "      <td>[1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, ...</td>\n",
       "      <td>[5, 5, 1, 1, 1, 1, 1, 1, 1, 1, 1, 7, 1, 1, 3, ...</td>\n",
       "      <td>42</td>\n",
       "      <td>1</td>\n",
       "      <td>59.51</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>10038</td>\n",
       "      <td>[7.4127640174265625, 7.370230641807081, 7.8180...</td>\n",
       "      <td>[17301.0, 17301.0, 17301.0, 17301.774780092594...</td>\n",
       "      <td>[1, 1, 1, 2, 2, 4, 2, 8, 1, 22, 8, 1, 8, 4, 2,...</td>\n",
       "      <td>[1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, ...</td>\n",
       "      <td>[1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, ...</td>\n",
       "      <td>[1, 1, 1, 2, 2, 1, 3, 1, 1, 1, 1, 1, 1, 1, 2, ...</td>\n",
       "      <td>111</td>\n",
       "      <td>0</td>\n",
       "      <td>0.0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>10057</td>\n",
       "      <td>[7.494708263135679, 7.736394428979239, 10.7789...</td>\n",
       "      <td>[17151.0, 17151.0, 17153.0, 17154.0, 17155.0, ...</td>\n",
       "      <td>[6, 21, 2, 6, 2, 4, 2, 22, 15, 2, 1, 35, 4, 2,...</td>\n",
       "      <td>[1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, ...</td>\n",
       "      <td>[1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, ...</td>\n",
       "      <td>[1, 1, 4, 1, 4, 1, 3, 1, 1, 3, 1, 1, 1, 4, 1, ...</td>\n",
       "      <td>61</td>\n",
       "      <td>1</td>\n",
       "      <td>62961.31</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>10062</td>\n",
       "      <td>[8.31898612539206, 8.824824939175638, 6.509067...</td>\n",
       "      <td>[17143.0, 17143.0, 17143.0, 17144.0, 17144.0, ...</td>\n",
       "      <td>[80, 15, 37, 38, 11, 11, 2, 24, 7, 5, 5, 11, 1...</td>\n",
       "      <td>[1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, ...</td>\n",
       "      <td>[1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, ...</td>\n",
       "      <td>[1, 1, 1, 1, 1, 1, 3, 1, 1, 1, 1, 1, 1, 1, 1, ...</td>\n",
       "      <td>82</td>\n",
       "      <td>1</td>\n",
       "      <td>107126.35</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "   cl_id                                             amount  \\\n",
       "0  10018  [10.609081944147828, 10.596659732783579, 10.81...   \n",
       "1  10030  [4.61512051684126, 6.90875477931522, 10.598857...   \n",
       "2  10038  [7.4127640174265625, 7.370230641807081, 7.8180...   \n",
       "3  10057  [7.494708263135679, 7.736394428979239, 10.7789...   \n",
       "4  10062  [8.31898612539206, 8.824824939175638, 6.509067...   \n",
       "\n",
       "                                          event_time  \\\n",
       "0  [17120.38773148148, 17133.667800925927, 17134....   \n",
       "1  [17141.0, 17141.0, 17145.0, 17147.0, 17147.0, ...   \n",
       "2  [17301.0, 17301.0, 17301.0, 17301.774780092594...   \n",
       "3  [17151.0, 17151.0, 17153.0, 17154.0, 17155.0, ...   \n",
       "4  [17143.0, 17143.0, 17143.0, 17144.0, 17144.0, ...   \n",
       "\n",
       "                                                 mcc  \\\n",
       "0  [13, 2, 13, 2, 1, 18, 13, 2, 13, 2, 5, 13, 9, ...   \n",
       "1  [9, 9, 21, 1, 25, 6, 14, 14, 3, 3, 3, 13, 1, 3...   \n",
       "2  [1, 1, 1, 2, 2, 4, 2, 8, 1, 22, 8, 1, 8, 4, 2,...   \n",
       "3  [6, 21, 2, 6, 2, 4, 2, 22, 15, 2, 1, 35, 4, 2,...   \n",
       "4  [80, 15, 37, 38, 11, 11, 2, 24, 7, 5, 5, 11, 1...   \n",
       "\n",
       "                                        channel_type  \\\n",
       "0      [1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1]   \n",
       "1  [1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, ...   \n",
       "2  [1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, ...   \n",
       "3  [1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, ...   \n",
       "4  [1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, ...   \n",
       "\n",
       "                                            currency  \\\n",
       "0      [1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1]   \n",
       "1  [1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, ...   \n",
       "2  [1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, ...   \n",
       "3  [1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, ...   \n",
       "4  [1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, ...   \n",
       "\n",
       "                                        trx_category  trx_count  \\\n",
       "0      [5, 3, 5, 3, 1, 1, 5, 3, 5, 3, 1, 5, 5, 5, 5]         15   \n",
       "1  [5, 5, 1, 1, 1, 1, 1, 1, 1, 1, 1, 7, 1, 1, 3, ...         42   \n",
       "2  [1, 1, 1, 2, 2, 1, 3, 1, 1, 1, 1, 1, 1, 1, 2, ...        111   \n",
       "3  [1, 1, 4, 1, 4, 1, 3, 1, 1, 3, 1, 1, 1, 4, 1, ...         61   \n",
       "4  [1, 1, 1, 1, 1, 1, 3, 1, 1, 1, 1, 1, 1, 1, 1, ...         82   \n",
       "\n",
       "  target_target_flag target_target_sum  \n",
       "0                  0               0.0  \n",
       "1                  1             59.51  \n",
       "2                  0               0.0  \n",
       "3                  1          62961.31  \n",
       "4                  1         107126.35  "
      ]
     },
     "execution_count": 3,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "df = pd.read_parquet(conf.train_path)\n",
    "df.head()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "2086it [00:00, 20848.94it/s]"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "9717it [00:00, 17738.15it/s]\n"
     ]
    }
   ],
   "source": [
    "train_loader, valid_loader = create_data_loaders(conf)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [],
   "source": [
    "for batch in train_loader:\n",
    "    break"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [],
   "source": [
    "encoder = MegaEncoder(model_conf=model_conf, data_conf=conf)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [],
   "source": [
    "out = encoder(batch[0])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "torch.Size([20, 64, 4])"
      ]
     },
     "execution_count": 8,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "out.size()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "MegaEncoder(\n",
       "  (preprocessor): FeatureProcessor(\n",
       "    (embed_layers): ModuleDict(\n",
       "      (channel_type): Embedding(400, 16)\n",
       "      (currency): Embedding(400, 16)\n",
       "      (mcc): Embedding(400, 16)\n",
       "      (trx_category): Embedding(400, 16)\n",
       "    )\n",
       "  )\n",
       "  (encoder): enc_mtan_rnn(\n",
       "    (att): multiTimeAttention(\n",
       "      (linears): ModuleList(\n",
       "        (0-1): 2 x Linear(in_features=16, out_features=16, bias=True)\n",
       "        (2): Linear(in_features=130, out_features=16, bias=True)\n",
       "      )\n",
       "    )\n",
       "    (gru_rnn): GRU(16, 16, batch_first=True, bidirectional=True)\n",
       "    (hiddens_to_z0): Sequential(\n",
       "      (0): Linear(in_features=32, out_features=32, bias=True)\n",
       "      (1): ReLU()\n",
       "      (2): Linear(in_features=32, out_features=4, bias=True)\n",
       "    )\n",
       "    (periodic): Linear(in_features=1, out_features=15, bias=True)\n",
       "    (linear): Linear(in_features=1, out_features=1, bias=True)\n",
       "  )\n",
       ")"
      ]
     },
     "execution_count": 9,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "encoder"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [],
   "source": [
    "features = batch[0]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "{'amount': tensor([[ 8.6127,  5.8522,  7.8228,  ...,  8.1312,  6.4938,  6.7951],\n",
       "         [ 7.8317,  8.6127,  5.8522,  ...,  0.0000,  0.0000,  0.0000],\n",
       "         [ 7.9664,  9.9295,  6.9088,  ...,  0.0000,  0.0000,  0.0000],\n",
       "         ...,\n",
       "         [12.2061, 11.6784, 11.5129,  ...,  0.0000,  0.0000,  0.0000],\n",
       "         [12.2061, 11.6784, 11.9184,  ...,  0.0000,  0.0000,  0.0000],\n",
       "         [11.6784, 11.9184, 10.8198,  ...,  0.0000,  0.0000,  0.0000]],\n",
       "        dtype=torch.float64),\n",
       " 'mcc': tensor([[16,  4,  6,  ..., 78,  6,  4],\n",
       "         [16, 16,  4,  ...,  0,  0,  0],\n",
       "         [ 6, 99,  3,  ...,  0,  0,  0],\n",
       "         ...,\n",
       "         [ 3,  3,  3,  ...,  0,  0,  0],\n",
       "         [ 3,  3,  3,  ...,  0,  0,  0],\n",
       "         [ 3,  3,  3,  ...,  0,  0,  0]], dtype=torch.int32),\n",
       " 'channel_type': tensor([[3, 3, 3,  ..., 3, 3, 3],\n",
       "         [3, 3, 3,  ..., 0, 0, 0],\n",
       "         [3, 3, 3,  ..., 0, 0, 0],\n",
       "         ...,\n",
       "         [2, 2, 2,  ..., 0, 0, 0],\n",
       "         [2, 2, 2,  ..., 0, 0, 0],\n",
       "         [2, 2, 2,  ..., 0, 0, 0]], dtype=torch.int32),\n",
       " 'currency': tensor([[2, 2, 2,  ..., 2, 2, 2],\n",
       "         [2, 2, 2,  ..., 0, 0, 0],\n",
       "         [2, 2, 2,  ..., 0, 0, 0],\n",
       "         ...,\n",
       "         [2, 2, 2,  ..., 0, 0, 0],\n",
       "         [2, 2, 2,  ..., 0, 0, 0],\n",
       "         [2, 2, 2,  ..., 0, 0, 0]], dtype=torch.int32),\n",
       " 'trx_category': tensor([[2, 2, 2,  ..., 2, 2, 2],\n",
       "         [2, 2, 2,  ..., 0, 0, 0],\n",
       "         [2, 2, 4,  ..., 0, 0, 0],\n",
       "         ...,\n",
       "         [5, 5, 5,  ..., 0, 0, 0],\n",
       "         [5, 5, 5,  ..., 0, 0, 0],\n",
       "         [5, 5, 5,  ..., 0, 0, 0]], dtype=torch.int32),\n",
       " 'event_time': tensor([[17377.0000, 17377.0000, 17382.0000,  ..., 17453.0000, 17453.0000,\n",
       "          17453.0000],\n",
       "         [17377.0000, 17377.0000, 17377.0000,  ...,     0.0000,     0.0000,\n",
       "              0.0000],\n",
       "         [17391.0000, 17392.0000, 17392.4816,  ...,     0.0000,     0.0000,\n",
       "              0.0000],\n",
       "         ...,\n",
       "         [17252.0000, 17253.0000, 17267.0000,  ...,     0.0000,     0.0000,\n",
       "              0.0000],\n",
       "         [17252.0000, 17253.0000, 17275.0000,  ...,     0.0000,     0.0000,\n",
       "              0.0000],\n",
       "         [17253.0000, 17275.0000, 17276.0000,  ...,     0.0000,     0.0000,\n",
       "              0.0000]], dtype=torch.float64)}"
      ]
     },
     "execution_count": 11,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "features.payload"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {},
   "outputs": [],
   "source": [
    "class multiTimeAttention(nn.Module):\n",
    "    \n",
    "    def __init__(self, input_dim, nhidden=16, \n",
    "                 embed_time=16, num_heads=1):\n",
    "        super(multiTimeAttention, self).__init__()\n",
    "        assert embed_time % num_heads == 0\n",
    "        self.embed_time = embed_time\n",
    "        self.embed_time_k = embed_time // num_heads\n",
    "        self.h = num_heads\n",
    "        self.dim = input_dim\n",
    "        self.nhidden = nhidden\n",
    "        self.linears = nn.ModuleList([nn.Linear(embed_time, embed_time), \n",
    "                                      nn.Linear(embed_time, embed_time),\n",
    "                                      nn.Linear(input_dim*num_heads, nhidden)])\n",
    "        \n",
    "    def attention(self, query, key, value, mask=None, dropout=None):\n",
    "        \"Compute 'Scaled Dot Product Attention'\"\n",
    "        dim = value.size(-1)\n",
    "        d_k = query.size(-1)\n",
    "        scores = torch.matmul(query, key.transpose(-2, -1)) \\\n",
    "                 / math.sqrt(d_k)\n",
    "        scores = scores.unsqueeze(-1).repeat_interleave(dim, dim=-1)\n",
    "        if mask is not None:\n",
    "            scores = scores.masked_fill(mask.unsqueeze(-3) == 0, -1e9)\n",
    "        p_attn = F.softmax(scores, dim = -2)\n",
    "        if dropout is not None:\n",
    "            p_attn = dropout(p_attn)\n",
    "        return torch.sum(p_attn*value.unsqueeze(-3), -2), p_attn\n",
    "    \n",
    "    \n",
    "    def forward(self, query, key, value, mask=None, dropout=None):\n",
    "        \"Compute 'Scaled Dot Product Attention'\"\n",
    "        batch, seq_len, dim = value.size()\n",
    "        if mask is not None:\n",
    "            # Same mask applied to all h heads.\n",
    "            mask = mask.unsqueeze(1)\n",
    "        value = value.unsqueeze(1)\n",
    "        \n",
    "        query, key = [l(x).view(x.size(0), -1, self.h, self.embed_time_k).transpose(1, 2)\n",
    "                      for l, x in tqdm(zip(self.linears, (query, key)))]\n",
    "        x, _ = self.attention(query, key, value, mask, dropout)\n",
    "        x = x.transpose(1, 2).contiguous() \\\n",
    "             .view(batch, -1, self.h * dim)\n",
    "\n",
    "        print(x.size())\n",
    "        print(self.linears)\n",
    "        print(self.linears[-1].weight.size())\n",
    "        return self.linears[-1](x)\n",
    "\n",
    "class enc_mtan_rnn(nn.Module):\n",
    "    def __init__(self, input_dim, query, latent_dim=2, nhidden=16, \n",
    "                 embed_time=16, num_heads=1, learn_emb=False, device='cuda'):\n",
    "        super(enc_mtan_rnn, self).__init__()\n",
    "        self.embed_time = embed_time\n",
    "        self.dim = input_dim\n",
    "        self.device = device\n",
    "        self.nhidden = nhidden\n",
    "        self.query = query\n",
    "        self.learn_emb = learn_emb\n",
    "        self.att = multiTimeAttention(input_dim, nhidden, embed_time, num_heads)\n",
    "        self.gru_rnn = nn.GRU(nhidden, nhidden, bidirectional=True, batch_first=True)\n",
    "        self.hiddens_to_z0 = nn.Sequential(\n",
    "            nn.Linear(2*nhidden, 50),\n",
    "            nn.ReLU(),\n",
    "            nn.Linear(50, latent_dim * 2))\n",
    "        if learn_emb:\n",
    "            self.periodic = nn.Linear(1, embed_time-1)\n",
    "            self.linear = nn.Linear(1, 1)\n",
    "        \n",
    "    \n",
    "    def learn_time_embedding(self, tt):\n",
    "        tt = tt.to(self.device)\n",
    "        tt = tt.unsqueeze(-1)\n",
    "        out2 = torch.sin(self.periodic(tt))\n",
    "        out1 = self.linear(tt)\n",
    "        return torch.cat([out1, out2], -1)\n",
    "    \n",
    "    def fixed_time_embedding(self, pos):\n",
    "        d_model=self.embed_time\n",
    "        pe = torch.zeros(pos.shape[0], pos.shape[1], d_model)\n",
    "        position = 48.*pos.unsqueeze(2)\n",
    "        div_term = torch.exp(torch.arange(0, d_model, 2) *\n",
    "                             -(np.log(10.0) / d_model))\n",
    "        pe[:, :, 0::2] = torch.sin(position * div_term)\n",
    "        pe[:, :, 1::2] = torch.cos(position * div_term)\n",
    "        return pe\n",
    "       \n",
    "    def forward(self, x, time_steps):\n",
    "        time_steps = time_steps.cpu()\n",
    "        mask = x[:, :, self.dim:]\n",
    "        mask = torch.cat((mask, mask), 2)\n",
    "        if self.learn_emb:\n",
    "            key = self.learn_time_embedding(time_steps).to(self.device)\n",
    "            query = self.learn_time_embedding(self.query.unsqueeze(0)).to(self.device)\n",
    "        else:\n",
    "            key = self.fixed_time_embedding(time_steps).to(self.device)\n",
    "            query = self.fixed_time_embedding(self.query.unsqueeze(0)).to(self.device)\n",
    "        out = self.att(query, key, x, mask=None)\n",
    "        out, _ = self.gru_rnn(out)\n",
    "        out = self.hiddens_to_z0(out)\n",
    "        return out"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "metadata": {},
   "outputs": [],
   "source": [
    "ref_points = torch.linspace(0, 1., 5)\n",
    "\n",
    "\n",
    "am = features.payload['amount']\n",
    "mcc = features.payload['mcc']\n",
    "\n",
    "emb1 = nn.Embedding(384, 16)\n",
    "mcce = emb1(mcc)\n",
    "\n",
    "am = am.unsqueeze(-1)\n",
    "x = torch.cat([mcce, am], dim=-1)\n",
    "\n",
    "time_steps = features.payload['event_time']"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "metadata": {},
   "outputs": [],
   "source": [
    "input_dim = x.size(-1)\n",
    "query = ref_points\n",
    "\n",
    "\n",
    "enc = enc_mtan_rnn(input_dim, query, latent_dim=2, nhidden=16, \n",
    "                 embed_time=16, num_heads=2, learn_emb=True, device='cpu')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 20,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "2it [00:00, 1668.05it/s]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "torch.Size([20, 5, 34])\n",
      "ModuleList(\n",
      "  (0-1): 2 x Linear(in_features=16, out_features=16, bias=True)\n",
      "  (2): Linear(in_features=34, out_features=16, bias=True)\n",
      ")\n",
      "torch.Size([16, 34])\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\n"
     ]
    }
   ],
   "source": [
    "out = enc(x.float(), time_steps.float())"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 21,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "tensor([-0.0151,  0.0093, -0.0105,  0.0625], grad_fn=<SelectBackward0>)"
      ]
     },
     "execution_count": 21,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "out[0][0]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {},
   "outputs": [],
   "source": [
    "class FeatureProcessor(nn.Module):\n",
    "\n",
    "    def __init__(self, model_conf, data_conf):\n",
    "        super(FeatureProcessor, self).__init__()\n",
    "        self.model_conf = model_conf\n",
    "        self.data_conf = data_conf\n",
    "\n",
    "        self.emb_names = list(self.data_conf.features.embeddings.keys())\n",
    "        self.init_embed_layers()\n",
    "\n",
    "    def init_embed_layers(self):\n",
    "        self.embed_layers = nn.ModuleDict()\n",
    "        \n",
    "        for name in self.emb_names:\n",
    "            vocab_size = self.data_conf.features.embeddings[name]['max_value']\n",
    "            self.embed_layers[name] = nn.Embedding(vocab_size, self.model_conf.features_emb_dim)\n",
    "\n",
    "    def forward(self, padded_batch):\n",
    "        numeric_values = []\n",
    "\n",
    "        for key, values in padded_batch.payload.items():\n",
    "            if key in self.emb_names:\n",
    "                numeric_values.append(self.embed_layers[key](values))\n",
    "            else:\n",
    "                if key == 'event_time':\n",
    "                    time_steps = values\n",
    "                else:\n",
    "                    numeric_values.append(values.unsqueeze(-1).float())\n",
    "        \n",
    "        x = torch.cat(numeric_values, dim=-1)\n",
    "        return x, time_steps\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "metadata": {},
   "outputs": [],
   "source": [
    "processor = FeatureProcessor(model_conf=model_conf, data_conf=conf)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "metadata": {},
   "outputs": [],
   "source": [
    "x, t = processor(features)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "torch.Size([20, 100, 65])"
      ]
     },
     "execution_count": 16,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "x.size()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 34,
   "metadata": {},
   "outputs": [],
   "source": [
    "class MegaEncoder(nn.Module):\n",
    "\n",
    "    def __init__(self, model_conf, data_conf):\n",
    "        super(MegaEncoder, self).__init__()\n",
    "        self.model_conf = model_conf\n",
    "        self.data_conf = data_conf\n",
    "\n",
    "        all_emb_size = self.model_conf.features_emb_dim * len(self.data_conf.features.embeddings)\n",
    "        all_numeric_size = len(self.data_conf.features.numeric_values)\n",
    "        self.input_dim = all_emb_size + all_numeric_size\n",
    "\n",
    "        self.preprocessor = FeatureProcessor(model_conf=self.model_conf, data_conf=self.data_conf)\n",
    "\n",
    "        self.ref_points = torch.linspace(0., 1., self.model_conf.num_ref_points)\n",
    "        self.encoder = enc_mtan_rnn(\n",
    "                        self.input_dim,\n",
    "                        self.ref_points,\n",
    "                        latent_dim=self.model_conf.latent_dim,\n",
    "                        nhidden=self.model_conf.ref_point_dim, \n",
    "                        embed_time=self.model_conf.time_emb_dim,\n",
    "                        num_heads=self.model_conf.num_heads_enc,\n",
    "                        learn_emb=True,\n",
    "                        device=self.model_conf.device)\n",
    "\n",
    "    def forward(self, padded_batch):\n",
    "        x, time_steps = self.preprocessor(padded_batch)\n",
    "        out = self.encoder(x, time_steps.float())\n",
    "\n",
    "        return out"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 35,
   "metadata": {},
   "outputs": [],
   "source": [
    "enc = MegaEncoder(model_conf=model_conf, data_conf=conf)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 36,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "<src.data_load.dataloader.PaddedBatch at 0x7fed44502440>"
      ]
     },
     "execution_count": 36,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "batch[0]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 37,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "2it [00:00, 1010.31it/s]"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "torch.Size([20, 64, 130])\n",
      "ModuleList(\n",
      "  (0-1): 2 x Linear(in_features=16, out_features=16, bias=True)\n",
      "  (2): Linear(in_features=130, out_features=16, bias=True)\n",
      ")\n",
      "torch.Size([16, 130])\n"
     ]
    }
   ],
   "source": [
    "out = enc(batch[0])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 38,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "torch.Size([20, 64, 4])"
      ]
     },
     "execution_count": 38,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "out.size()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 76,
   "metadata": {},
   "outputs": [],
   "source": [
    "import math\n",
    "def learn_time_embedding(tt):\n",
    "    tt = tt.unsqueeze(-1)\n",
    "    out2 = torch.sin(periodic(tt))\n",
    "    out1 = linear(tt)\n",
    "    return torch.cat([out1, out2], -1)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 62,
   "metadata": {},
   "outputs": [],
   "source": [
    "ref_points = torch.linspace(0, 1., 5)\n",
    "periodic = nn.Linear(1, 16-1)\n",
    "linear = nn.Linear(1, 1)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 70,
   "metadata": {},
   "outputs": [],
   "source": [
    "key = learn_time_embedding(time_steps.float())\n",
    "query = learn_time_embedding(ref_points.unsqueeze(0))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 73,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "(torch.Size([20, 100, 16]), torch.Size([1, 5, 16]))"
      ]
     },
     "execution_count": 73,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "key.size(), query.size()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 116,
   "metadata": {},
   "outputs": [],
   "source": [
    "class multiTimeAttention(nn.Module):\n",
    "    \n",
    "    def __init__(self, input_dim, nhidden=16, \n",
    "                 embed_time=16, num_heads=1):\n",
    "        super(multiTimeAttention, self).__init__()\n",
    "        assert embed_time % num_heads == 0\n",
    "        self.embed_time = embed_time\n",
    "        self.embed_time_k = embed_time // num_heads\n",
    "        self.h = num_heads\n",
    "        self.dim = input_dim\n",
    "        self.nhidden = nhidden\n",
    "        self.linears = nn.ModuleList([nn.Linear(embed_time, embed_time), \n",
    "                                      nn.Linear(embed_time, embed_time),\n",
    "                                      nn.Linear(input_dim*num_heads, nhidden)])\n",
    "        \n",
    "    def attention(self, query, key, value, mask=None, dropout=None):\n",
    "        \"Compute 'Scaled Dot Product Attention'\"\n",
    "        dim = value.size(-1)\n",
    "        d_k = query.size(-1)\n",
    "        scores = torch.matmul(query, key.transpose(-2, -1)) \\\n",
    "                 / math.sqrt(d_k)\n",
    "        scores = scores.unsqueeze(-1).repeat_interleave(dim, dim=-1)\n",
    "        if mask is not None:\n",
    "            scores = scores.masked_fill(mask.unsqueeze(-3) == 0, -1e9)\n",
    "        p_attn = F.softmax(scores, dim = -2)\n",
    "        if dropout is not None:\n",
    "            p_attn = dropout(p_attn)\n",
    "        return torch.sum(p_attn*value.unsqueeze(-3), -2), p_attn\n",
    "    \n",
    "    \n",
    "    def forward(self, query, key, value, mask=None, dropout=None):\n",
    "        \"Compute 'Scaled Dot Product Attention'\"\n",
    "        batch, seq_len, dim = value.size()\n",
    "        if mask is not None:\n",
    "            # Same mask applied to all h heads.\n",
    "            mask = mask.unsqueeze(1)\n",
    "        value = value.unsqueeze(1)\n",
    "        \n",
    "        query, key = [l(x).view(x.size(0), -1, self.h, self.embed_time_k).transpose(1, 2)\n",
    "                      for l, x in tqdm(zip(self.linears, (query, key)))]\n",
    "        x, _ = self.attention(query, key, value, mask, dropout)\n",
    "        x = x.transpose(1, 2).contiguous() \\\n",
    "             .view(batch, -1, self.h * dim)\n",
    "\n",
    "        print(x.size())\n",
    "        print(self.linears)\n",
    "        print(self.linears[-1].weight.size())\n",
    "        return self.linears[-1](x)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 117,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "34"
      ]
     },
     "execution_count": 117,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "2*dim"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 118,
   "metadata": {},
   "outputs": [],
   "source": [
    "att = multiTimeAttention(dim, 16, 16, 2)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 119,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "2it [00:00, 2166.48it/s]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "torch.Size([20, 5, 34])\n",
      "ModuleList(\n",
      "  (0-1): 2 x Linear(in_features=16, out_features=16, bias=True)\n",
      "  (2): Linear(in_features=34, out_features=16, bias=True)\n",
      ")\n",
      "torch.Size([16, 34])\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\n"
     ]
    }
   ],
   "source": [
    "out = att(query, key, x.float())"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 107,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "torch.Size([20, 5, 16])"
      ]
     },
     "execution_count": 107,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "out.size()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.10.12"
  },
  "orig_nbformat": 4
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
