{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "import torch\n",
    "import torch.nn as nn\n",
    "\n",
    "import numpy as np\n",
    "import pandas as pd\n",
    "import os\n",
    "import math\n",
    "import warnings\n",
    "import itertools\n",
    "import numbers\n",
    "import torch.utils.data as utils\n",
    "\n",
    "import sys\n",
    "sys.path.append('../../../')\n",
    "from configs.data_configs.physionet import data_configs\n",
    "from configs.model_configs.supervised.physionet import model_configs\n",
    "from src.data_load.dataloader import create_data_loaders, create_test_loader\n",
    "from src.models.grud import GRUDClassifier"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "path = '../data/train_trx.parquet'\n",
    "df = pd.read_parquet(path)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [],
   "source": [
    "data_conf = data_configs()\n",
    "model_conf = model_configs()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Data shapes: train 2870, val 318, test 0\n"
     ]
    }
   ],
   "source": [
    "train_loader, val_loader = create_data_loaders(data_conf, supervised=True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {},
   "outputs": [],
   "source": [
    "for batch in train_loader:\n",
    "    break"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/home/event_seq/experiments/physionet/notebooks/../../../src/models/grud.py:212: UserWarning: To copy construct from a tensor, it is recommended to use sourceTensor.clone().detach() or sourceTensor.clone().detach().requires_grad_(True), rather than torch.tensor(sourceTensor).\n",
      "  x_mean = torch.tensor(x_mean, requires_grad = True)\n",
      "/home/event_seq/experiments/physionet/notebooks/../../../src/models/grud.py:227: UserWarning: dropout option adds dropout after all but last recurrent layer, so non-zero dropout expects num_layers greater than 1, but got dropout=0.2 and num_layers=1\n",
      "  warnings.warn(\"dropout option adds dropout after all but last \"\n",
      "/usr/local/lib/python3.10/dist-packages/torch/nn/modules/rnn.py:71: UserWarning: dropout option adds dropout after all but last recurrent layer, so non-zero dropout expects num_layers greater than 1, but got dropout=0.2 and num_layers=1\n",
      "  warnings.warn(\"dropout option adds dropout after all but last \"\n"
     ]
    }
   ],
   "source": [
    "grud = GRUDClassifier(model_conf=model_conf, data_conf=data_conf)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "metadata": {},
   "outputs": [],
   "source": [
    "out = grud(batch[0])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "tensor([[1588, 2762, 2536, 1370, 2282, 2052, 1689, 1590, 1036, 1352,  552, 1028,\n",
       "         2515, 1761, 1663, 2650, 2685, 1740, 2395,  862, 1779,  568, 2537,  777,\n",
       "          672, 1156,  212, 2535,  723, 2387,  570, 1218,  168, 2574,  438, 1802,\n",
       "          508, 2702, 1472,  318, 1197,  120, 2113,  332, 2838,  205, 1427, 2035,\n",
       "         2463, 1782,  689, 1844,  381, 2054, 1945, 2336, 1793, 1419,  720, 2100,\n",
       "         1553, 2603,  660, 2013, 1606, 2796, 2223, 1735, 2217, 2314,  399,  122,\n",
       "         2075, 1823, 1481, 1301, 1865, 1876,  123, 1070, 2361, 2860, 1487, 2805,\n",
       "          806, 1582, 1980, 2789,  153, 1213, 2760, 1939, 1579,  760,   98, 2810,\n",
       "          424, 1400,  232, 1551,  657, 1355, 1279, 2849, 2478, 2244, 1319, 2572,\n",
       "          226, 2065,  383, 2709,  352,  421, 1055, 1867, 1812, 1654, 2494, 1687,\n",
       "         1747, 1228, 2155, 1078,  620, 2713, 2309,  150],\n",
       "        [   0,    0,    0,    0,    0,    0,    0,    0,    0,    0,    1,    0,\n",
       "            0,    0,    0,    1,    1,    0,    0,    0,    0,    0,    0,    0,\n",
       "            0,    0,    0,    0,    0,    0,    1,    0,    0,    0,    0,    0,\n",
       "            1,    0,    0,    0,    0,    0,    0,    0,    1,    0,    0,    0,\n",
       "            0,    0,    0,    0,    0,    0,    0,    0,    0,    1,    0,    0,\n",
       "            0,    0,    1,    1,    0,    0,    0,    1,    0,    1,    0,    0,\n",
       "            0,    1,    0,    0,    0,    0,    0,    0,    0,    1,    0,    0,\n",
       "            1,    0,    0,    0,    0,    0,    0,    0,    1,    0,    0,    0,\n",
       "            0,    1,    1,    0,    1,    0,    1,    0,    0,    0,    1,    0,\n",
       "            0,    0,    1,    0,    0,    0,    1,    0,    0,    0,    0,    0,\n",
       "            0,    0,    0,    0,    0,    1,    0,    0]])"
      ]
     },
     "execution_count": 15,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "batch[1]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "metadata": {},
   "outputs": [],
   "source": [
    "loss = grud.loss(out, batch[1])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "metadata": {},
   "outputs": [],
   "source": [
    "loss['total_loss'].backward()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [],
   "source": [
    "def create_beta(feature, times):\n",
    "    beta = torch.zeros_like(feature)\n",
    "\n",
    "    for i in range(1, times.size(1)):\n",
    "        mask = torch.logical_and((feature[:,i - 1] != -1), (feature[:,i - 1] != 0))\n",
    "        padding_mask = times[:, i] != -1\n",
    "        \n",
    "        beta[:, i] = (times[:,i] - times[:,i - 1] + mask * beta[:, i - 1]) * padding_mask\n",
    "\n",
    "    return beta\n",
    "\n",
    "class FeatureProcessor(nn.Module):\n",
    "    def __init__(self, model_conf, data_conf):\n",
    "        super().__init__()\n",
    "        self.model_conf = model_conf\n",
    "        self.data_conf = data_conf\n",
    "\n",
    "        self.emb_names = list(self.data_conf.features.embeddings.keys())\n",
    "        self.numeric_names = list(self.data_conf.features.numeric_values.keys())\n",
    "        self.init_embed_layers()\n",
    "\n",
    "    def init_embed_layers(self):\n",
    "        self.embed_layers = nn.ModuleDict()\n",
    "\n",
    "        for name in self.emb_names:\n",
    "            vocab_size = self.data_conf.features.embeddings[name][\"max_value\"]\n",
    "            self.embed_layers[name] = nn.Embedding(\n",
    "                vocab_size, self.model_conf.features_emb_dim\n",
    "            )\n",
    "\n",
    "        self.numeric_processor = nn.ModuleDict()\n",
    "        self.numeric_norms = nn.ModuleDict()\n",
    "        for name in self.numeric_names:\n",
    "            if self.model_conf.use_numeric_emb:\n",
    "                self.numeric_processor[name] = nn.Linear(\n",
    "                    1, self.model_conf.numeric_emb_size\n",
    "                )\n",
    "            else:\n",
    "                self.numeric_processor[name] = nn.Identity()\n",
    "\n",
    "            self.numeric_norms[name] = nn.Identity()\n",
    "\n",
    "    def forward(self, padded_batch, use_norm=True):\n",
    "        numeric_values = []\n",
    "        categoric_values = []\n",
    "\n",
    "        numeric_delta = []\n",
    "        categoric_delta = []\n",
    "\n",
    "        numeric_masking = []\n",
    "        categoric_masking = []\n",
    "\n",
    "        time_steps = padded_batch.payload.get(\"event_time\").float()\n",
    "        seq_lens = padded_batch.seq_lens\n",
    "        for key, values in padded_batch.payload.items():\n",
    "            if key in self.emb_names:\n",
    "                categoric_values.append(self.embed_layers[key](values.long()))\n",
    "                categoric_delta.append(create_beta(values, time_steps).unsqueeze(-1).repeat(1,1,self.model_conf.features_emb_dim))\n",
    "                categoric_masking.append(torch.logical_and((values != -1), (values != 0)).unsqueeze(-1).repeat(1,1,self.model_conf.features_emb_dim))\n",
    "            elif key in self.numeric_names:\n",
    "                if use_norm:\n",
    "                    cur_value = self.numeric_norms[key](values.float(), seq_lens)\n",
    "                else:  # we do not want to use normalization when applying decoder to our sequence\n",
    "                    # otherwise it would be hard to make true generation and lead to bias in forecasting\n",
    "                    cur_value = values.float().unsqueeze(-1)\n",
    "\n",
    "                numeric_values.append(self.numeric_processor[key](cur_value))\n",
    "                numeric_delta.append(create_beta(values, time_steps).unsqueeze(-1).repeat(1,1,self.model_conf.numeric_emb_size))\n",
    "                numeric_masking.append(torch.logical_and((values != -1), (values != 0)).unsqueeze(-1).repeat(1,1,self.model_conf.numeric_emb_size))\n",
    "\n",
    "\n",
    "        if len(categoric_values) == 0:\n",
    "            return torch.cat(numeric_values, dim=-1), time_steps\n",
    "        if len(numeric_values) == 0:\n",
    "            return torch.cat(categoric_values, dim=-1), time_steps\n",
    "\n",
    "        categoric_tensor = torch.cat(categoric_values, dim=-1)\n",
    "        numeric_tensor = torch.cat(numeric_values, dim=-1)\n",
    "        \n",
    "        numeric_delta_tensor = torch.cat(numeric_delta, dim=-1)\n",
    "        categoric_delta_tensor = torch.cat(categoric_delta, dim=-1)\n",
    "\n",
    "        numeric_mask_tensor = torch.cat(numeric_masking, dim=-1)\n",
    "        categoric_mask_tensor = torch.cat(categoric_masking, dim=-1)\n",
    "        return (\n",
    "                torch.cat([categoric_tensor, numeric_tensor], dim=-1),\n",
    "                time_steps,\n",
    "                torch.cat([categoric_delta_tensor, numeric_delta_tensor], dim=-1),\n",
    "                torch.cat([categoric_mask_tensor, numeric_mask_tensor], dim=-1)\n",
    "                )"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [],
   "source": [
    "fp = FeatureProcessor(data_conf=data_conf, model_conf=model_conf)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "out, ts, deltas, masks = fp(batch[0], use_norm=False)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 44,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "tensor([0.4799, 0.4850, 0.5318, 0.5077, 0.4399, 0.5266, 0.5382])"
      ]
     },
     "execution_count": 44,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "tr_data = torch.rand(8, 3, 7, 5) # B x F x D x L\n",
    "tr_data[:,0,:,:].mean(axis=0).mean(axis=1)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 37,
   "metadata": {},
   "outputs": [],
   "source": [
    "class GRUD_cell(torch.nn.Module):\n",
    "    \"\"\"\n",
    "    Implementation of GRUD.\n",
    "    Inputs: x_mean\n",
    "            n_smp x 3 x n_channels x len_seq tensor (0: data, 1: mask, 2: deltat)\n",
    "    \"\"\"\n",
    "    def __init__(self, input_size, hidden_size, output_size, num_layers=1, x_mean=0,\\\n",
    "                 bias=True, batch_first=False, bidirectional=False, dropout_type='mloss', dropout=0, return_hidden = False):\n",
    "\n",
    "        #use_cuda = torch.cuda.is_available()\n",
    "        #device = torch.device(\"cuda\" if use_cuda else \"cpu\")\n",
    "        device='cpu'\n",
    "\n",
    "        super(GRUD_cell, self).__init__()\n",
    "        self.input_size = input_size\n",
    "        self.hidden_size = hidden_size\n",
    "        self.output_size = output_size\n",
    "        self.num_layers = num_layers\n",
    "        self.return_hidden = return_hidden #controls the output, True if another GRU-D layer follows\n",
    "\n",
    "\n",
    "        x_mean = torch.tensor(x_mean.float(), requires_grad = True)\n",
    "        self.register_buffer('x_mean', x_mean)\n",
    "        self.bias = bias\n",
    "        self.batch_first = batch_first\n",
    "        self.dropout_type = dropout_type\n",
    "        self.dropout = dropout\n",
    "        self.bidirectional = bidirectional\n",
    "        num_directions = 2 if bidirectional else 1\n",
    "        \n",
    "        if not isinstance(dropout, numbers.Number) or not 0 <= dropout <= 1 or \\\n",
    "                isinstance(dropout, bool):\n",
    "            raise ValueError(\"dropout should be a number in range [0, 1] \"\n",
    "                             \"representing the probability of an element being \"\n",
    "                             \"zeroed\")\n",
    "        if dropout > 0 and num_layers == 1:\n",
    "            warnings.warn(\"dropout option adds dropout after all but last \"\n",
    "                          \"recurrent layer, so non-zero dropout expects \"\n",
    "                          \"num_layers greater than 1, but got dropout={} and \"\n",
    "                          \"num_layers={}\".format(dropout, num_layers))\n",
    "        \n",
    "        \n",
    "\n",
    "        #set up all the operations that are needed in the forward pass\n",
    "        self.w_dg_x = torch.nn.Linear(input_size,input_size, bias=True)\n",
    "        self.w_dg_h = torch.nn.Linear(input_size, hidden_size, bias = True)\n",
    "\n",
    "        self.w_xz = torch.nn.Linear(input_size, hidden_size, bias=False)\n",
    "        self.w_hz = torch.nn.Linear(hidden_size, hidden_size, bias=False)\n",
    "        self.w_mz = torch.nn.Linear(input_size, hidden_size, bias=True)\n",
    "\n",
    "        self.w_xr = torch.nn.Linear(input_size, hidden_size, bias=False)\n",
    "        self.w_hr = torch.nn.Linear(hidden_size, hidden_size, bias=False)\n",
    "        self.w_mr = torch.nn.Linear(input_size, hidden_size, bias=False)\n",
    "        self.w_xh = torch.nn.Linear(input_size, hidden_size, bias=False)\n",
    "        self.w_hh = torch.nn.Linear(hidden_size, hidden_size, bias=False)\n",
    "        self.w_mh = torch.nn.Linear(input_size, hidden_size, bias=True)\n",
    "\n",
    "        self.w_hy = torch.nn.Linear(hidden_size, output_size, bias=True)\n",
    "        \n",
    "    \n",
    "\n",
    "\n",
    "        Hidden_State = torch.zeros(self.hidden_size, requires_grad = True)\n",
    "        #we use buffers because pytorch will take care of pushing them to GPU for us\n",
    "        self.register_buffer('Hidden_State', Hidden_State)\n",
    "        self.register_buffer('X_last_obs', torch.zeros(input_size)) #torch.tensor(x_mean) #TODO: what to initialize last observed values with?, also check broadcasting behaviour\n",
    "\n",
    "    \n",
    "    #TODO: check usefulness of everything below here, just copied skeleton\n",
    "\n",
    "\n",
    "        self.reset_parameters()\n",
    "        \n",
    "    \n",
    "\n",
    "\n",
    "    def reset_parameters(self):\n",
    "        stdv = 1.0 / math.sqrt(self.hidden_size)\n",
    "        for weight in self.parameters():\n",
    "            torch.nn.init.uniform_(weight, -stdv, stdv)\n",
    "\n",
    "    def check_forward_args(self, input, hidden, batch_sizes):\n",
    "        is_input_packed = batch_sizes is not None\n",
    "        expected_input_dim = 2 if is_input_packed else 3\n",
    "        if input.dim() != expected_input_dim:\n",
    "            raise RuntimeError(\n",
    "                'input must have {} dimensions, got {}'.format(\n",
    "                    expected_input_dim, input.dim()))\n",
    "        if self.input_size != input.size(-1):\n",
    "            raise RuntimeError(\n",
    "                'input.size(-1) must be equal to input_size. Expected {}, got {}'.format(\n",
    "                    self.input_size, input.size(-1)))\n",
    "\n",
    "        if is_input_packed:\n",
    "            mini_batch = int(batch_sizes[0])\n",
    "        else:\n",
    "            mini_batch = input.size(0) if self.batch_first else input.size(1)\n",
    "\n",
    "        num_directions = 2 if self.bidirectional else 1\n",
    "        expected_hidden_size = (self.num_layers * num_directions,\n",
    "                                mini_batch, self.hidden_size)\n",
    "        \n",
    "        def check_hidden_size(hx, expected_hidden_size, msg='Expected hidden size {}, got {}'):\n",
    "            if tuple(hx.size()) != expected_hidden_size:\n",
    "                raise RuntimeError(msg.format(expected_hidden_size, tuple(hx.size())))\n",
    "\n",
    "        if self.mode == 'LSTM':\n",
    "            check_hidden_size(hidden[0], expected_hidden_size,\n",
    "                              'Expected hidden[0] size {}, got {}')\n",
    "            check_hidden_size(hidden[1], expected_hidden_size,\n",
    "                              'Expected hidden[1] size {}, got {}')\n",
    "        else:\n",
    "            check_hidden_size(hidden, expected_hidden_size)\n",
    "    \n",
    "    def extra_repr(self):\n",
    "        s = '{input_size}, {hidden_size}'\n",
    "        if self.num_layers != 1:\n",
    "            s += ', num_layers={num_layers}'\n",
    "        if self.bias is not True:\n",
    "            s += ', bias={bias}'\n",
    "        if self.batch_first is not False:\n",
    "            s += ', batch_first={batch_first}'\n",
    "        if self.dropout != 0:\n",
    "            s += ', dropout={dropout}'\n",
    "        if self.bidirectional is not False:\n",
    "            s += ', bidirectional={bidirectional}'\n",
    "        return s.format(**self.__dict__)\n",
    "    \n",
    "    \n",
    "\n",
    "\n",
    "\n",
    "    @property\n",
    "    def _flat_weights(self):\n",
    "        return list(self._parameters.values())\n",
    "\n",
    "\n",
    "    def forward(self, X, Mask, Delta):\n",
    "        # input.size = (3, 33,49) : num_input or num_hidden, num_layer or step\n",
    "        #X = torch.squeeze(input[0]) # .size = (33,49)\n",
    "        #Mask = torch.squeeze(input[1]) # .size = (33,49)\n",
    "        #Delta = torch.squeeze(input[2]) # .size = (33,49)\n",
    "        # X = input[:,0,:,:]\n",
    "        # Mask = input[:,1,:,:]\n",
    "        # Delta = input[:,2,:,:]\n",
    "        \n",
    "\n",
    "        step_size = X.size(1) # 49\n",
    "        #print('step size : ', step_size)\n",
    "        \n",
    "        output = None\n",
    "        #h = Hidden_State\n",
    "        h = getattr(self, 'Hidden_State')\n",
    "        #felix - buffer system from newer pytorch version\n",
    "        x_mean = getattr(self, 'x_mean')\n",
    "        x_last_obsv = getattr(self, 'X_last_obs')\n",
    "        \n",
    "\n",
    "        device = next(self.parameters()).device\n",
    "        output_tensor = torch.empty([X.size()[0], X.size()[2], self.output_size], dtype=X.dtype, device= device)\n",
    "        hidden_tensor = torch.empty(X.size()[0], X.size()[2], self.hidden_size, dtype=X.dtype, device = device)\n",
    "\n",
    "        #iterate over seq\n",
    "        for timestep in range(X.size()[2]):\n",
    "            \n",
    "            #x = torch.squeeze(X[:,layer:layer+1])\n",
    "            #m = torch.squeeze(Mask[:,layer:layer+1])\n",
    "            #d = torch.squeeze(Delta[:,layer:layer+1])\n",
    "            x = torch.squeeze(X[:,:,timestep])\n",
    "            m = torch.squeeze(Mask[:,:,timestep])\n",
    "            d = torch.squeeze(Delta[:,:,timestep])\n",
    "            \n",
    "\n",
    "            #(4)\n",
    "            gamma_x = torch.exp(-1* torch.nn.functional.relu( self.w_dg_x(d) ))\n",
    "            gamma_h = torch.exp(-1* torch.nn.functional.relu( self.w_dg_h(d) ))\n",
    "\n",
    "\n",
    "            #(5)\n",
    "            #standard mult handles case correctly, this should work - maybe broadcast x_mean, seems to be taking care of that anyway\n",
    "            \n",
    "            #update x_last_obsv\n",
    "            #print(x.size())\n",
    "            #print(x_last_obsv.size())\n",
    "            x_last_obsv = torch.where(m>0,x,x_last_obsv)\n",
    "            #print('after update')\n",
    "            #print(x_last_obsv)\n",
    "            x = m * x + (1 - m) * (gamma_x * x + (1 - gamma_x) * x_mean)\n",
    "            x = m * x + (1 - m) * (gamma_x * x_last_obsv + (1 - gamma_x) * x_mean)\n",
    "\n",
    "            #(6)\n",
    "            if self.dropout == 0:\n",
    "\n",
    "                h = gamma_h*h\n",
    "                z = torch.sigmoid( self.w_xz(x) + self.w_hz(h) + self.w_mz(m))\n",
    "                r = torch.sigmoid( self.w_xr(x) + self.w_hr(h) + self.w_mr(m))\n",
    "\n",
    "                h_tilde = torch.tanh( self.w_xh(x) + self.w_hh( r*h ) + self.w_mh(m))\n",
    "\n",
    "\n",
    "                h = (1 - z) * h + z * h_tilde\n",
    "\n",
    "            #TODO: not adapted yet\n",
    "            elif self.dropout_type == 'Moon':\n",
    "                '''\n",
    "                RNNDROP: a novel dropout for rnn in asr(2015)\n",
    "                '''\n",
    "                h = gamma_h * h\n",
    "\n",
    "                z = torch.sigmoid((w_xz*x + w_hz*h + w_mz*m + b_z))\n",
    "                r = torch.sigmoid((w_xr*x + w_hr*h + w_mr*m + b_r))\n",
    "\n",
    "                h_tilde = torch.tanh((w_xh*x + w_hh*(r * h) + w_mh*m + b_h))\n",
    "\n",
    "                h = (1 - z) * h + z * h_tilde\n",
    "                dropout = torch.nn.Dropout(p=self.dropout)\n",
    "                h = dropout(h)\n",
    "\n",
    "            elif self.dropout_type == 'Gal':\n",
    "                '''\n",
    "                A Theoretically grounded application of dropout in recurrent neural networks(2015)\n",
    "                '''\n",
    "                dropout = torch.nn.Dropout(p=self.dropout)\n",
    "                h = dropout(h)\n",
    "\n",
    "                h = gamma_h * h\n",
    "\n",
    "                z = torch.sigmoid((w_xz*x + w_hz*h + w_mz*m + b_z))\n",
    "                r = torch.sigmoid((w_xr*x + w_hr*h + w_mr*m + b_r))\n",
    "                h_tilde = torch.tanh((w_xh*x + w_hh*(r * h) + w_mh*m + b_h))\n",
    "\n",
    "                h = (1 - z) * h + z * h_tilde\n",
    "\n",
    "            elif self.dropout_type == 'mloss':\n",
    "                '''\n",
    "                recurrent dropout without memory loss arXiv 1603.05118\n",
    "                g = h_tilde, p = the probability to not drop a neuron\n",
    "                '''\n",
    "                h = gamma_h*h\n",
    "                z = torch.sigmoid( self.w_xz(x) + self.w_hz(h) + self.w_mz(m))\n",
    "                r = torch.sigmoid( self.w_xr(x) + self.w_hr(h) + self.w_mr(m))\n",
    "\n",
    "\n",
    "                dropout = torch.nn.Dropout(p=self.dropout)\n",
    "                h_tilde = torch.tanh( self.w_xh(x) + self.w_hh( r*h ) + self.w_mh(m))\n",
    "\n",
    "\n",
    "                h = (1 - z) * h + z * h_tilde\n",
    "                #######\n",
    "\n",
    "            else:\n",
    "                h = gamma_h * h\n",
    "\n",
    "                z = torch.sigmoid((w_xz*x + w_hz*h + w_mz*m + b_z))\n",
    "                r = torch.sigmoid((w_xr*x + w_hr*h + w_mr*m + b_r))\n",
    "                h_tilde = torch.tanh((w_xh*x + w_hh*(r * h) + w_mh*m + b_h))\n",
    "\n",
    "                h = (1 - z) * h + z * h_tilde\n",
    "\n",
    "            \n",
    "\n",
    "            step_output = self.w_hy(h)\n",
    "            #step_output = torch.sigmoid(step_output)\n",
    "            output_tensor[:,timestep,:] = step_output\n",
    "            hidden_tensor[:,timestep,:] = h\n",
    "            \n",
    "        #if self.return_hidden:\n",
    "            #when i want to stack GRU-Ds, need to put the tensor back together\n",
    "            #output = torch.stack([hidden_tensor,Mask,Delta], dim=1)\n",
    "        \n",
    "        output = output_tensor, hidden_tensor\n",
    "        #else:\n",
    "        #    output = output_tensor\n",
    "        return output\n",
    "\n",
    "class GRUD(torch.nn.Module):\n",
    "    def __init__(self,input_size, hidden_size, output_size, num_layers = 1, x_mean = 0,\\\n",
    "     bias =True, batch_first = False, bidirectional = False, dropout_type ='mloss', dropout = 0):\n",
    "        super().__init__()\n",
    "\n",
    "        self.gru_d = GRUD_cell(input_size = input_size, hidden_size= hidden_size, output_size=output_size, \n",
    "                dropout=dropout, dropout_type=dropout_type, x_mean=x_mean)\n",
    "        self.hidden_to_output = torch.nn.Linear(hidden_size, output_size, bias=True)\n",
    "        self.num_layers = num_layers\n",
    "        self.hidden_size = hidden_size\n",
    "\n",
    "        if self.num_layers >1:\n",
    "            #(batch, seq, feature)\n",
    "            self.gru_layers = torch.nn.GRU(input_size = hidden_size, hidden_size = hidden_size, batch_first = True, num_layers = self.num_layers -1, dropout=dropout)\n",
    "\n",
    "    def initialize_hidden(self, batch_size):\n",
    "        device = next(self.parameters()).device\n",
    "        # The hidden state at the start are all zeros\n",
    "        return torch.zeros(self.num_layers-1, batch_size, self.hidden_size, device=device)\n",
    "\n",
    "    def forward(self, x, masks, deltas):\n",
    "\n",
    "        #pass through GRU-D\n",
    "        output, hidden = self.gru_d(x, masks, deltas)\n",
    "        #print(self.gru_d.return_hidden)\n",
    "        #output = self.gru_d(input)\n",
    "        #print(output.size())\n",
    "\n",
    "        # batch_size, n_hidden, n_timesteps\n",
    "\n",
    "        if self.num_layers >1:\n",
    "            #TODO remove init hidden, not necessary, auto init works fine\n",
    "            init_hidden = self.initialize_hidden(hidden.size()[0])\n",
    "            \n",
    "\n",
    "            output, hidden = self.gru_layers(hidden)#, init_hidden)\n",
    "  \n",
    "\n",
    "            output = self.hidden_to_output(output)\n",
    "          #  output = torch.sigmoid(output)\n",
    "\n",
    "        #print(\"final output size passed as model result\")\n",
    "        #print(output.size())\n",
    "        return output\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 38,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "torch.Size([128, 121, 328])"
      ]
     },
     "execution_count": 38,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "out.size()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 47,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/tmp/ipykernel_2307001/1770133292.py:22: UserWarning: To copy construct from a tensor, it is recommended to use sourceTensor.clone().detach() or sourceTensor.clone().detach().requires_grad_(True), rather than torch.tensor(sourceTensor).\n",
      "  x_mean = torch.tensor(x_mean.float(), requires_grad = True)\n"
     ]
    }
   ],
   "source": [
    "dgru = GRUD(input_size=328, hidden_size=16, output_size=2, num_layers=2, x_mean=torch.ones(328).float(),\n",
    "                 bias=True, batch_first=True, bidirectional=False, dropout_type='mloss', dropout=0)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 48,
   "metadata": {},
   "outputs": [],
   "source": [
    "gout = dgru(out.permute(0,2,1), masks.permute(0,2,1).float(), deltas.permute(0,2,1).float())"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 50,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "torch.Size([128, 121, 328])"
      ]
     },
     "execution_count": 50,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "out.size()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 49,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "torch.Size([128, 121, 2])"
      ]
     },
     "execution_count": 49,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "gout.size()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 39,
   "metadata": {},
   "outputs": [],
   "source": [
    "dgru = GRUD_cell(input_size=328, hidden_size=16, output_size=32, num_layers=1, x_mean=1.,\n",
    "                 bias=True, batch_first=True, bidirectional=False, dropout_type='mloss', dropout=0, return_hidden = False)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 48,
   "metadata": {},
   "outputs": [],
   "source": [
    "gout = dgru(out.permute(0,2,1), masks.permute(0,2,1).float(), deltas.permute(0,2,1).float())"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": 51,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "(torch.Size([128, 105, 32]), torch.Size([128, 105, 16]))"
      ]
     },
     "execution_count": 51,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "gout[0].size(), gout[1].size()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.10.12"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
