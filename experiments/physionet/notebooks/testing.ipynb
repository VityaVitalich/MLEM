{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {
    "tags": []
   },
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/home/event_seq/experiments/physionet/notebooks/../../../src/trainers/base_trainer.py:11: TqdmExperimentalWarning: Using `tqdm.autonotebook.tqdm` in notebook mode. Use `tqdm.tqdm` instead to force console mode (e.g. in jupyter console)\n",
      "  from tqdm.autonotebook import tqdm\n"
     ]
    }
   ],
   "source": [
    "import pandas as pd\n",
    "import numpy as np\n",
    "from tqdm import tqdm\n",
    "import pyarrow.parquet as pq\n",
    "import sys\n",
    "import torch\n",
    "import random\n",
    "import torch.nn as nn\n",
    "import torch.nn.functional as F\n",
    "import math\n",
    "from IPython.display import clear_output\n",
    "import logging\n",
    "from pathlib import Path\n",
    "\n",
    "import matplotlib.pyplot as plt\n",
    "sys.path.append('../../../')\n",
    "\n",
    "from configs.data_configs.physionet_contrastive import data_configs\n",
    "from configs.model_configs.mTAN.physionet import model_configs\n",
    "from src.models.mTAND.model_mtan import EncMtanRnnClassification, MegaNetClassifier\n",
    "from src.data_load.dataloader import create_data_loaders, create_test_loader\n",
    "from src.trainers.trainer_mTAND import MtandTrainer\n",
    "\n",
    "\n",
    "from src.create_embeddings import create_embeddings\n",
    "\n",
    "from src.data_load import split_strategy\n",
    "from src.data_load.data_utils import prepare_data\n",
    "from src.data_load.splitting_dataset import (\n",
    "    ConvertingTrxDataset,  # TargetDataset\n",
    "    DropoutTrxDataset,\n",
    "    SplittingDataset,\n",
    "    TargetEnumeratorDataset,\n",
    ")\n",
    "from src.models.preprocessors import FeatureProcessor, TimeConcater\n",
    "from src.data_load.dataloader import collate_splitted_rows, padded_collate\n",
    "from torch.utils.data import DataLoader\n",
    "\n",
    "from sklearn.linear_model import LogisticRegression\n",
    "from einops import repeat, rearrange, reduce\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "# s = \"{((16, 32), 0.0, 32): 0.7662219684385382, ((16, 32), 0.0, 64): 0.7434982350498338, ((16, 32), 0.0, 128): 0.7607843646179402, ((16, 32), 0.2, 32): 0.7754749792358805, ((16, 32), 0.2, 64): 0.7730092400332225, ((16, 32), 0.2, 128): 0.7624454941860463, ((16, 32), 0.3, 32): 0.7773177948504983, ((16, 32), 0.3, 64): 0.7958757267441862, ((16, 32), 0.3, 128): 0.7598889119601329, ((16, 32), 0.4, 32): 0.7742810423588039, ((16, 32), 0.4, 64): 0.7823401162790699, ((16, 32), 0.4, 128): 0.7746444144518272, ((16, 32, 64), 0.0, 32): 0.7508046096345515, ((16, 32, 64), 0.0, 64): 0.7575659260797343, ((16, 32, 64), 0.0, 128): 0.7360231519933556, ((16, 32, 64), 0.2, 32): 0.7214493355481727, ((16, 32, 64), 0.2, 64): 0.7737749169435216, ((16, 32, 64), 0.2, 128): 0.76671511627907, ((16, 32, 64), 0.3, 32): 0.7636783637873754, ((16, 32, 64), 0.3, 64): 0.7767078488372093, ((16, 32, 64), 0.3, 128): 0.7794331395348837, ((16, 32, 64), 0.4, 32): 0.7469892026578072, ((16, 32, 64), 0.4, 64): 0.7633409468438539, ((16, 32, 64), 0.4, 128): 0.7774605481727574, ((16, 64), 0.0, 32): 0.7501816860465116, ((16, 64), 0.0, 64): 0.7505839908637874, ((16, 64), 0.0, 128): 0.7950451619601329, ((16, 64), 0.2, 32): 0.769764846345515, ((16, 64), 0.2, 64): 0.7818339908637874, ((16, 64), 0.2, 128): 0.7795499377076412, ((16, 64), 0.3, 32): 0.7646387043189369, ((16, 64), 0.3, 64): 0.7941237541528239, ((16, 64), 0.3, 128): 0.7736581187707642, ((16, 64), 0.4, 32): 0.7553337832225914, ((16, 64), 0.4, 64): 0.7831057931893688, ((16, 64), 0.4, 128): 0.7500389327242525, ((16, 64, 128), 0.0, 32): 0.7427714908637874, ((16, 64, 128), 0.0, 64): 0.7736840739202657, ((16, 64, 128), 0.0, 128): 0.7508694975083058, ((16, 64, 128), 0.2, 32): 0.658624896179402, ((16, 64, 128), 0.2, 64): 0.7475342607973423, ((16, 64, 128), 0.2, 128): 0.773982558139535, ((16, 64, 128), 0.3, 32): 0.6219308035714287, ((16, 64, 128), 0.3, 64): 0.7642234219269103, ((16, 64, 128), 0.3, 128): 0.7825088247508306, ((16, 64, 128), 0.4, 32): 0.5932049418604651, ((16, 64, 128), 0.4, 64): 0.774968853820598, ((16, 64, 128), 0.4, 128): 0.7812110672757474}\"\n",
    "# dic = eval(s)\n",
    "# for key,v in dic.items():\n",
    "#     print(\"TC_{}_TR_1H_1L_DRP_{}_GRU_{}H\".format(key[0], key[1], key[2]))\n",
    "# for key,v in dic.items():\n",
    "#     print(v)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "conf = data_configs()\n",
    "model_conf = model_configs()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [],
   "source": [
    "from src.trainers.sampling_strategies import get_sampling_strategy"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "5"
      ]
     },
     "execution_count": 5,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "model_conf.get('loss.neg_count')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [],
   "source": [
    "ss = get_sampling_strategy(model_conf)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "1347it [00:00, 4738.32it/s]"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "3200it [00:00, 4992.14it/s]\n"
     ]
    }
   ],
   "source": [
    "train_loader, valid_loader = create_data_loaders(conf)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [],
   "source": [
    "for batch in train_loader:\n",
    "    break"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "torch.Size([640, 100])"
      ]
     },
     "execution_count": 9,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "batch[0].payload['Age'].size()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "torch.Size([640])"
      ]
     },
     "execution_count": 10,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "batch[1][0].size()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {},
   "outputs": [],
   "source": [
    "embedding = torch.rand(640, 32)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {},
   "outputs": [],
   "source": [
    "positive_pairs, negative_pairs = ss.get_pairs(embedding, batch[1][0])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "tensor([[  0, 307],\n",
       "        [  1, 333],\n",
       "        [  2, 525],\n",
       "        ...,\n",
       "        [637, 614],\n",
       "        [638,  75],\n",
       "        [639, 431]])"
      ]
     },
     "execution_count": 13,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "negative_pairs"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "metadata": {},
   "outputs": [],
   "source": [
    "from src.trainers.losses import ContrastiveLoss, get_loss"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "metadata": {},
   "outputs": [],
   "source": [
    "cl = get_loss(model_conf, ss)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "tensor(6837.2266)"
      ]
     },
     "execution_count": 16,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "cl(embedding, batch[1][0])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 33,
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "\n",
    "class B():\n",
    "    def __init__(self):\n",
    "        print('inited B')\n",
    "\n",
    "class A(B):\n",
    "    def __init__(self, a):\n",
    "        super().__init__()\n",
    "        print('A inited', a)\n",
    "\n",
    "class C(A):\n",
    "\n",
    "    def __init__(self, a):\n",
    "        super().__init__(a)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 34,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "inited B\n",
      "A inited hello\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "<__main__.C at 0x7fe75a013eb0>"
      ]
     },
     "execution_count": 34,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "C('hello')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [],
   "source": [
    "processor = FeatureProcessor(model_conf=model_conf, data_conf=conf)\n",
    "\n",
    "time_blocks = [16, 32]\n",
    "\n",
    "#mtc = MultipleTimeSum(time_blocks)\n",
    "\n",
    "tc1 = TimeConcater(time_blocks[0], 'cpu')\n",
    "\n",
    "blocks2 = torch.linspace(0., 1., 33)\n",
    "tc2 = TimeConcater(time_blocks[1], 'cpu')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [],
   "source": [
    "x, time_steps = processor(batch[0])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [],
   "source": [
    "x1, time_steps = tc1(x, time_steps)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {},
   "outputs": [],
   "source": [
    "x2, time_steps = tc2(x, time_steps)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "(torch.Size([64, 16, 62]), torch.Size([64, 32, 62]))"
      ]
     },
     "execution_count": 12,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "x1.size(), x2.size()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 26,
   "metadata": {},
   "outputs": [],
   "source": [
    "x1_repeated = torch.repeat_interleave(x1, 2, dim=1).unsqueeze(0)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 27,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "(torch.Size([1, 64, 32, 62]), torch.Size([64, 32, 62]))"
      ]
     },
     "execution_count": 27,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "x1_repeated.size(), x2.size()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 49,
   "metadata": {},
   "outputs": [],
   "source": [
    "w = torch.ones(2)\n",
    "w[0] = 2"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 28,
   "metadata": {},
   "outputs": [],
   "source": [
    "features = torch.cat([x1_repeated, x2.unsqueeze(0)], dim=0)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 50,
   "metadata": {},
   "outputs": [],
   "source": [
    "w1 = repeat(w, 'nb -> nb bs l d', bs=64, l=32, d=62)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 51,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "torch.Size([2])"
      ]
     },
     "execution_count": 51,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "w.size()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "(features * w.repeat(62, 64, 32, 1).transpose(0,3)) == (features * w1)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 53,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "torch.Size([2, 64, 32, 62])"
      ]
     },
     "execution_count": 53,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "(features * w1).size()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 88,
   "metadata": {},
   "outputs": [],
   "source": [
    "class MultiTimeSummator(nn.Module):\n",
    "\n",
    "    def __init__(self, time_blocks, device):\n",
    "        super().__init__()\n",
    "\n",
    "        assert isinstance(time_blocks, list)\n",
    "\n",
    "        self.time_ps = []\n",
    "        for num_block in time_blocks:\n",
    "            self.time_ps.append(TimeConcater(num_block, device))\n",
    "\n",
    "        self.max_len = max(time_blocks)\n",
    "\n",
    "        self.weights = nn.Parameter(torch.ones(len(time_blocks)))\n",
    "\n",
    "\n",
    "    def forward(self, x, time_steps):\n",
    "        new_xs = self.collect_new_x(x, time_steps)\n",
    "\n",
    "        nb, bs, l, d = new_xs.size()\n",
    "        \n",
    "        cur_w = repeat(self.weights, 'nb -> nb bs l d', bs=bs, l=l, d=d)\n",
    "        \n",
    "        out = (new_xs * cur_w).sum(dim=0)\n",
    "\n",
    "        return out\n",
    "\n",
    "\n",
    "    def collect_new_x(self, x, time_steps):\n",
    "\n",
    "        new_xs = []\n",
    "\n",
    "        for tc in self.time_ps:\n",
    "            cur_multiplier = self.max_len // tc.num_points\n",
    "            out_x, time_steps = tc(x, time_steps)\n",
    "            new_x = torch.repeat_interleave(out_x, cur_multiplier, dim=1).unsqueeze(0)\n",
    "\n",
    "            new_xs.append(new_x)\n",
    "\n",
    "        return torch.cat(new_xs, dim=0)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 89,
   "metadata": {},
   "outputs": [],
   "source": [
    "mts = MultiTimeSummator([16, 32, 64], 'cpu')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 90,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "Parameter containing:\n",
       "tensor([1., 1., 1.], requires_grad=True)"
      ]
     },
     "execution_count": 90,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "mts.weights"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 74,
   "metadata": {},
   "outputs": [
    {
     "ename": "RuntimeError",
     "evalue": "Tensor type unknown to einops <class 'NoneType'>",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mRuntimeError\u001b[0m                              Traceback (most recent call last)",
      "\u001b[1;32m/home/event_seq/experiments/physionet/notebooks/testing.ipynb Cell 27\u001b[0m line \u001b[0;36m1\n\u001b[0;32m----> <a href='vscode-notebook-cell://attached-container%2B7b22636f6e7461696e65724e616d65223a222f6d7573696e675f72696465222c2273657474696e6773223a7b22686f7374223a227373683a2f2f6d696461732e736b6f6c746563682e7275227d7d/home/event_seq/experiments/physionet/notebooks/testing.ipynb#Y253sdnNjb2RlLXJlbW90ZQ%3D%3D?line=0'>1</a>\u001b[0m new_x \u001b[39m=\u001b[39m mts(x, time_steps)\n",
      "File \u001b[0;32m/usr/local/lib/python3.10/dist-packages/torch/nn/modules/module.py:1501\u001b[0m, in \u001b[0;36mModule._call_impl\u001b[0;34m(self, *args, **kwargs)\u001b[0m\n\u001b[1;32m   1496\u001b[0m \u001b[39m# If we don't have any hooks, we want to skip the rest of the logic in\u001b[39;00m\n\u001b[1;32m   1497\u001b[0m \u001b[39m# this function, and just call forward.\u001b[39;00m\n\u001b[1;32m   1498\u001b[0m \u001b[39mif\u001b[39;00m \u001b[39mnot\u001b[39;00m (\u001b[39mself\u001b[39m\u001b[39m.\u001b[39m_backward_hooks \u001b[39mor\u001b[39;00m \u001b[39mself\u001b[39m\u001b[39m.\u001b[39m_backward_pre_hooks \u001b[39mor\u001b[39;00m \u001b[39mself\u001b[39m\u001b[39m.\u001b[39m_forward_hooks \u001b[39mor\u001b[39;00m \u001b[39mself\u001b[39m\u001b[39m.\u001b[39m_forward_pre_hooks\n\u001b[1;32m   1499\u001b[0m         \u001b[39mor\u001b[39;00m _global_backward_pre_hooks \u001b[39mor\u001b[39;00m _global_backward_hooks\n\u001b[1;32m   1500\u001b[0m         \u001b[39mor\u001b[39;00m _global_forward_hooks \u001b[39mor\u001b[39;00m _global_forward_pre_hooks):\n\u001b[0;32m-> 1501\u001b[0m     \u001b[39mreturn\u001b[39;00m forward_call(\u001b[39m*\u001b[39;49margs, \u001b[39m*\u001b[39;49m\u001b[39m*\u001b[39;49mkwargs)\n\u001b[1;32m   1502\u001b[0m \u001b[39m# Do not call functions when jit is used\u001b[39;00m\n\u001b[1;32m   1503\u001b[0m full_backward_hooks, non_full_backward_hooks \u001b[39m=\u001b[39m [], []\n",
      "\u001b[1;32m/home/event_seq/experiments/physionet/notebooks/testing.ipynb Cell 27\u001b[0m line \u001b[0;36m2\n\u001b[1;32m     <a href='vscode-notebook-cell://attached-container%2B7b22636f6e7461696e65724e616d65223a222f6d7573696e675f72696465222c2273657474696e6773223a7b22686f7374223a227373683a2f2f6d696461732e736b6f6c746563682e7275227d7d/home/event_seq/experiments/physionet/notebooks/testing.ipynb#Y253sdnNjb2RlLXJlbW90ZQ%3D%3D?line=17'>18</a>\u001b[0m new_xs \u001b[39m=\u001b[39m \u001b[39mself\u001b[39m\u001b[39m.\u001b[39mcollect_new_x(x, time_steps)\n\u001b[1;32m     <a href='vscode-notebook-cell://attached-container%2B7b22636f6e7461696e65724e616d65223a222f6d7573696e675f72696465222c2273657474696e6773223a7b22686f7374223a227373683a2f2f6d696461732e736b6f6c746563682e7275227d7d/home/event_seq/experiments/physionet/notebooks/testing.ipynb#Y253sdnNjb2RlLXJlbW90ZQ%3D%3D?line=19'>20</a>\u001b[0m nb, bs, l, d \u001b[39m=\u001b[39m new_xs\u001b[39m.\u001b[39msize()\n\u001b[0;32m---> <a href='vscode-notebook-cell://attached-container%2B7b22636f6e7461696e65724e616d65223a222f6d7573696e675f72696465222c2273657474696e6773223a7b22686f7374223a227373683a2f2f6d696461732e736b6f6c746563682e7275227d7d/home/event_seq/experiments/physionet/notebooks/testing.ipynb#Y253sdnNjb2RlLXJlbW90ZQ%3D%3D?line=21'>22</a>\u001b[0m cur_w \u001b[39m=\u001b[39m repeat(\u001b[39mself\u001b[39;49m\u001b[39m.\u001b[39;49mweights, \u001b[39m'\u001b[39;49m\u001b[39mnb -> nb bs l d\u001b[39;49m\u001b[39m'\u001b[39;49m, bs\u001b[39m=\u001b[39;49mbs, l\u001b[39m=\u001b[39;49ml, d\u001b[39m=\u001b[39;49md)\n\u001b[1;32m     <a href='vscode-notebook-cell://attached-container%2B7b22636f6e7461696e65724e616d65223a222f6d7573696e675f72696465222c2273657474696e6773223a7b22686f7374223a227373683a2f2f6d696461732e736b6f6c746563682e7275227d7d/home/event_seq/experiments/physionet/notebooks/testing.ipynb#Y253sdnNjb2RlLXJlbW90ZQ%3D%3D?line=23'>24</a>\u001b[0m out \u001b[39m=\u001b[39m (new_xs \u001b[39m*\u001b[39m cur_w)\u001b[39m.\u001b[39msum(dim\u001b[39m=\u001b[39m\u001b[39m0\u001b[39m)\n\u001b[1;32m     <a href='vscode-notebook-cell://attached-container%2B7b22636f6e7461696e65724e616d65223a222f6d7573696e675f72696465222c2273657474696e6773223a7b22686f7374223a227373683a2f2f6d696461732e736b6f6c746563682e7275227d7d/home/event_seq/experiments/physionet/notebooks/testing.ipynb#Y253sdnNjb2RlLXJlbW90ZQ%3D%3D?line=25'>26</a>\u001b[0m \u001b[39mreturn\u001b[39;00m out\n",
      "File \u001b[0;32m/usr/local/lib/python3.10/dist-packages/einops/einops.py:533\u001b[0m, in \u001b[0;36mrepeat\u001b[0;34m(tensor, pattern, **axes_lengths)\u001b[0m\n\u001b[1;32m    486\u001b[0m \u001b[39mdef\u001b[39;00m \u001b[39mrepeat\u001b[39m(tensor: Tensor, pattern: \u001b[39mstr\u001b[39m, \u001b[39m*\u001b[39m\u001b[39m*\u001b[39maxes_lengths) \u001b[39m-\u001b[39m\u001b[39m>\u001b[39m Tensor:\n\u001b[1;32m    487\u001b[0m \u001b[39m    \u001b[39m\u001b[39m\"\"\"\u001b[39;00m\n\u001b[1;32m    488\u001b[0m \u001b[39m    einops.repeat allows reordering elements and repeating them in arbitrary combinations.\u001b[39;00m\n\u001b[1;32m    489\u001b[0m \u001b[39m    This operation includes functionality of repeat, tile, broadcast functions.\u001b[39;00m\n\u001b[0;32m   (...)\u001b[0m\n\u001b[1;32m    531\u001b[0m \n\u001b[1;32m    532\u001b[0m \u001b[39m    \"\"\"\u001b[39;00m\n\u001b[0;32m--> 533\u001b[0m     \u001b[39mreturn\u001b[39;00m reduce(tensor, pattern, reduction\u001b[39m=\u001b[39;49m\u001b[39m'\u001b[39;49m\u001b[39mrepeat\u001b[39;49m\u001b[39m'\u001b[39;49m, \u001b[39m*\u001b[39;49m\u001b[39m*\u001b[39;49maxes_lengths)\n",
      "File \u001b[0;32m/usr/local/lib/python3.10/dist-packages/einops/einops.py:412\u001b[0m, in \u001b[0;36mreduce\u001b[0;34m(tensor, pattern, reduction, **axes_lengths)\u001b[0m\n\u001b[1;32m    410\u001b[0m     hashable_axes_lengths \u001b[39m=\u001b[39m \u001b[39mtuple\u001b[39m(\u001b[39msorted\u001b[39m(axes_lengths\u001b[39m.\u001b[39mitems()))\n\u001b[1;32m    411\u001b[0m     recipe \u001b[39m=\u001b[39m _prepare_transformation_recipe(pattern, reduction, axes_lengths\u001b[39m=\u001b[39mhashable_axes_lengths)\n\u001b[0;32m--> 412\u001b[0m     \u001b[39mreturn\u001b[39;00m _apply_recipe(recipe, tensor, reduction_type\u001b[39m=\u001b[39;49mreduction)\n\u001b[1;32m    413\u001b[0m \u001b[39mexcept\u001b[39;00m EinopsError \u001b[39mas\u001b[39;00m e:\n\u001b[1;32m    414\u001b[0m     message \u001b[39m=\u001b[39m \u001b[39m'\u001b[39m\u001b[39m Error while processing \u001b[39m\u001b[39m{}\u001b[39;00m\u001b[39m-reduction pattern \u001b[39m\u001b[39m\"\u001b[39m\u001b[39m{}\u001b[39;00m\u001b[39m\"\u001b[39m\u001b[39m.\u001b[39m\u001b[39m'\u001b[39m\u001b[39m.\u001b[39mformat(reduction, pattern)\n",
      "File \u001b[0;32m/usr/local/lib/python3.10/dist-packages/einops/einops.py:233\u001b[0m, in \u001b[0;36m_apply_recipe\u001b[0;34m(recipe, tensor, reduction_type)\u001b[0m\n\u001b[1;32m    231\u001b[0m \u001b[39mdef\u001b[39;00m \u001b[39m_apply_recipe\u001b[39m(recipe: TransformRecipe, tensor: Tensor, reduction_type: Reduction) \u001b[39m-\u001b[39m\u001b[39m>\u001b[39m Tensor:\n\u001b[1;32m    232\u001b[0m     \u001b[39m# this method works for all backends but not compilable with\u001b[39;00m\n\u001b[0;32m--> 233\u001b[0m     backend \u001b[39m=\u001b[39m get_backend(tensor)\n\u001b[1;32m    234\u001b[0m     init_shapes, reduced_axes, axes_reordering, added_axes, final_shapes \u001b[39m=\u001b[39m \\\n\u001b[1;32m    235\u001b[0m         _reconstruct_from_shape(recipe, backend\u001b[39m.\u001b[39mshape(tensor))\n\u001b[1;32m    236\u001b[0m     tensor \u001b[39m=\u001b[39m backend\u001b[39m.\u001b[39mreshape(tensor, init_shapes)\n",
      "File \u001b[0;32m/usr/local/lib/python3.10/dist-packages/einops/_backends.py:52\u001b[0m, in \u001b[0;36mget_backend\u001b[0;34m(tensor)\u001b[0m\n\u001b[1;32m     49\u001b[0m             \u001b[39mif\u001b[39;00m backend\u001b[39m.\u001b[39mis_appropriate_type(tensor):\n\u001b[1;32m     50\u001b[0m                 \u001b[39mreturn\u001b[39;00m backend\n\u001b[0;32m---> 52\u001b[0m \u001b[39mraise\u001b[39;00m \u001b[39mRuntimeError\u001b[39;00m(\u001b[39m'\u001b[39m\u001b[39mTensor type unknown to einops \u001b[39m\u001b[39m{}\u001b[39;00m\u001b[39m'\u001b[39m\u001b[39m.\u001b[39mformat(\u001b[39mtype\u001b[39m(tensor)))\n",
      "\u001b[0;31mRuntimeError\u001b[0m: Tensor type unknown to einops <class 'NoneType'>"
     ]
    }
   ],
   "source": [
    "new_x = mts(x, time_steps)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 71,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "[Parameter containing:\n",
       " tensor([1., 1., 1.], requires_grad=True)]"
      ]
     },
     "execution_count": 71,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "list(mts.parameters())"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 67,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "torch.Size([64, 64, 62])"
      ]
     },
     "execution_count": 67,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "new_x.size()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 141,
   "metadata": {},
   "outputs": [],
   "source": [
    "seq_proc = nn.Sequential(FeatureProcessor(model_conf=model_conf, data_conf=conf), \n",
    "                         TimeConcater(blocks))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 142,
   "metadata": {},
   "outputs": [
    {
     "ename": "TypeError",
     "evalue": "TimeConcater.forward() missing 1 required positional argument: 'time_steps'",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mTypeError\u001b[0m                                 Traceback (most recent call last)",
      "\u001b[1;32m/home/event_seq/experiments/physionet/notebooks/testing.ipynb Cell 12\u001b[0m line \u001b[0;36m1\n\u001b[0;32m----> <a href='vscode-notebook-cell://attached-container%2B7b22636f6e7461696e65724e616d65223a222f6d7573696e675f72696465222c2273657474696e6773223a7b22686f7374223a227373683a2f2f6d696461732e736b6f6c746563682e7275227d7d/home/event_seq/experiments/physionet/notebooks/testing.ipynb#Y303sdnNjb2RlLXJlbW90ZQ%3D%3D?line=0'>1</a>\u001b[0m x, t \u001b[39m=\u001b[39m seq_proc(batch[\u001b[39m0\u001b[39;49m])\n",
      "File \u001b[0;32m/usr/local/lib/python3.10/dist-packages/torch/nn/modules/module.py:1501\u001b[0m, in \u001b[0;36mModule._call_impl\u001b[0;34m(self, *args, **kwargs)\u001b[0m\n\u001b[1;32m   1496\u001b[0m \u001b[39m# If we don't have any hooks, we want to skip the rest of the logic in\u001b[39;00m\n\u001b[1;32m   1497\u001b[0m \u001b[39m# this function, and just call forward.\u001b[39;00m\n\u001b[1;32m   1498\u001b[0m \u001b[39mif\u001b[39;00m \u001b[39mnot\u001b[39;00m (\u001b[39mself\u001b[39m\u001b[39m.\u001b[39m_backward_hooks \u001b[39mor\u001b[39;00m \u001b[39mself\u001b[39m\u001b[39m.\u001b[39m_backward_pre_hooks \u001b[39mor\u001b[39;00m \u001b[39mself\u001b[39m\u001b[39m.\u001b[39m_forward_hooks \u001b[39mor\u001b[39;00m \u001b[39mself\u001b[39m\u001b[39m.\u001b[39m_forward_pre_hooks\n\u001b[1;32m   1499\u001b[0m         \u001b[39mor\u001b[39;00m _global_backward_pre_hooks \u001b[39mor\u001b[39;00m _global_backward_hooks\n\u001b[1;32m   1500\u001b[0m         \u001b[39mor\u001b[39;00m _global_forward_hooks \u001b[39mor\u001b[39;00m _global_forward_pre_hooks):\n\u001b[0;32m-> 1501\u001b[0m     \u001b[39mreturn\u001b[39;00m forward_call(\u001b[39m*\u001b[39;49margs, \u001b[39m*\u001b[39;49m\u001b[39m*\u001b[39;49mkwargs)\n\u001b[1;32m   1502\u001b[0m \u001b[39m# Do not call functions when jit is used\u001b[39;00m\n\u001b[1;32m   1503\u001b[0m full_backward_hooks, non_full_backward_hooks \u001b[39m=\u001b[39m [], []\n",
      "File \u001b[0;32m/usr/local/lib/python3.10/dist-packages/torch/nn/modules/container.py:217\u001b[0m, in \u001b[0;36mSequential.forward\u001b[0;34m(self, input)\u001b[0m\n\u001b[1;32m    215\u001b[0m \u001b[39mdef\u001b[39;00m \u001b[39mforward\u001b[39m(\u001b[39mself\u001b[39m, \u001b[39minput\u001b[39m):\n\u001b[1;32m    216\u001b[0m     \u001b[39mfor\u001b[39;00m module \u001b[39min\u001b[39;00m \u001b[39mself\u001b[39m:\n\u001b[0;32m--> 217\u001b[0m         \u001b[39minput\u001b[39m \u001b[39m=\u001b[39m module(\u001b[39minput\u001b[39;49m)\n\u001b[1;32m    218\u001b[0m     \u001b[39mreturn\u001b[39;00m \u001b[39minput\u001b[39m\n",
      "File \u001b[0;32m/usr/local/lib/python3.10/dist-packages/torch/nn/modules/module.py:1501\u001b[0m, in \u001b[0;36mModule._call_impl\u001b[0;34m(self, *args, **kwargs)\u001b[0m\n\u001b[1;32m   1496\u001b[0m \u001b[39m# If we don't have any hooks, we want to skip the rest of the logic in\u001b[39;00m\n\u001b[1;32m   1497\u001b[0m \u001b[39m# this function, and just call forward.\u001b[39;00m\n\u001b[1;32m   1498\u001b[0m \u001b[39mif\u001b[39;00m \u001b[39mnot\u001b[39;00m (\u001b[39mself\u001b[39m\u001b[39m.\u001b[39m_backward_hooks \u001b[39mor\u001b[39;00m \u001b[39mself\u001b[39m\u001b[39m.\u001b[39m_backward_pre_hooks \u001b[39mor\u001b[39;00m \u001b[39mself\u001b[39m\u001b[39m.\u001b[39m_forward_hooks \u001b[39mor\u001b[39;00m \u001b[39mself\u001b[39m\u001b[39m.\u001b[39m_forward_pre_hooks\n\u001b[1;32m   1499\u001b[0m         \u001b[39mor\u001b[39;00m _global_backward_pre_hooks \u001b[39mor\u001b[39;00m _global_backward_hooks\n\u001b[1;32m   1500\u001b[0m         \u001b[39mor\u001b[39;00m _global_forward_hooks \u001b[39mor\u001b[39;00m _global_forward_pre_hooks):\n\u001b[0;32m-> 1501\u001b[0m     \u001b[39mreturn\u001b[39;00m forward_call(\u001b[39m*\u001b[39;49margs, \u001b[39m*\u001b[39;49m\u001b[39m*\u001b[39;49mkwargs)\n\u001b[1;32m   1502\u001b[0m \u001b[39m# Do not call functions when jit is used\u001b[39;00m\n\u001b[1;32m   1503\u001b[0m full_backward_hooks, non_full_backward_hooks \u001b[39m=\u001b[39m [], []\n",
      "\u001b[0;31mTypeError\u001b[0m: TimeConcater.forward() missing 1 required positional argument: 'time_steps'"
     ]
    }
   ],
   "source": [
    "x, t = seq_proc(batch[0])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 132,
   "metadata": {},
   "outputs": [],
   "source": [
    "tc = TimeConcater(blocks)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 133,
   "metadata": {},
   "outputs": [],
   "source": [
    "new_x = tc(x, time_steps)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 134,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "torch.Size([128, 9, 62])"
      ]
     },
     "execution_count": 134,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "new_x.size()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 61,
   "metadata": {},
   "outputs": [],
   "source": [
    "k = 10\n",
    "d = 10\n",
    "\n",
    "p= d*(k-1)//2\n",
    "conv = nn.Conv1d(62, 3, kernel_size=k, dilation=d, padding=p)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 62,
   "metadata": {},
   "outputs": [],
   "source": [
    "out = conv(x.transpose(1,2))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 63,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "torch.Size([128, 3, 128])"
      ]
     },
     "execution_count": 63,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "out.size()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 77,
   "metadata": {},
   "outputs": [],
   "source": [
    "norm = getattr(nn, 'Identity')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 80,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "Identity()"
      ]
     },
     "execution_count": 80,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "norm(64)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 81,
   "metadata": {},
   "outputs": [],
   "source": [
    "class Net(nn.Module):\n",
    "\n",
    "    def __init__(self):\n",
    "        super().__init__()\n",
    "\n",
    "        in_dim = 62\n",
    "        out_dim = 3\n",
    "        hid_gru = 7\n",
    "        kernels = [3,5,9]\n",
    "        dilations = [1,5,9]\n",
    "\n",
    "        res_dim = out_dim * (len(kernels) * len(dilations))\n",
    "\n",
    "        self.convs = nn.ModuleList()\n",
    "        for k in kernels:\n",
    "            for d in dilations:\n",
    "                p = d * (k - 1) // 2\n",
    "                self.convs.append(nn.Conv1d(in_dim, out_dim, kernel_size=k, padding=p, dilation=d))\n",
    "\n",
    "        self.gru = nn.GRU(\n",
    "            res_dim,\n",
    "            hid_gru,\n",
    "            batch_first=True\n",
    "        )\n",
    "        self.out_linear = nn.Linear(hid_gru, 2)\n",
    "    def forward(self, x):\n",
    "        out = torch.concat([conv(x.transpose(1,2)) for conv in self.convs], dim=1)\n",
    "        print(out.size())\n",
    "        all_hiddens, _ = self.gru(out.transpose(1,2))\n",
    "        out = self.out_linear(all_hiddens[:,-1])\n",
    "        return out"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 82,
   "metadata": {},
   "outputs": [],
   "source": [
    "net = Net()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 83,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "torch.Size([128, 27, 128])\n"
     ]
    }
   ],
   "source": [
    "out = net(x)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 74,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "torch.Size([128, 2])"
      ]
     },
     "execution_count": 74,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "out.size()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "metadata": {},
   "outputs": [],
   "source": [
    "encoder_layer = nn.TransformerEncoderLayer(d_model=62, nhead=2)\n",
    "transformer_encoder = nn.TransformerEncoder(encoder_layer, num_layers=3)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "metadata": {},
   "outputs": [],
   "source": [
    "out = transformer_encoder(x)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "metadata": {},
   "outputs": [],
   "source": [
    "gru = nn.GRU(\n",
    "    62,\n",
    "    33, \n",
    "    batch_first=True\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "metadata": {},
   "outputs": [],
   "source": [
    "gru_out = gru(out)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 21,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "torch.Size([128, 128, 33])"
      ]
     },
     "execution_count": 21,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "gru_out[0].size()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 22,
   "metadata": {},
   "outputs": [],
   "source": [
    "lens = batch[0].seq_lens - 1\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 25,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "torch.Size([128, 62])"
      ]
     },
     "execution_count": 25,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "out[:, lens, :].diagonal().T.size()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 27,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "torch.Size([128, 33])"
      ]
     },
     "execution_count": 27,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "gru_out[0][:, lens, :].diagonal().T.size()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [],
   "source": [
    "test = pd.read_parquet(conf.test_path)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "(target\n",
       " 0.0    2758\n",
       " 1.0     442\n",
       " Name: count, dtype: int64,\n",
       " target\n",
       " 0.0    688\n",
       " 1.0    112\n",
       " Name: count, dtype: int64)"
      ]
     },
     "execution_count": 9,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "df['target'].value_counts(), test['target'].value_counts()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "array([], dtype=int32)"
      ]
     },
     "execution_count": 12,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "np.intersect1d(test['user'].values, df['user'].values)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "<Axes: >"
      ]
     },
     "execution_count": 19,
     "metadata": {},
     "output_type": "execute_result"
    },
    {
     "data": {
      "image/png": "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",
      "text/plain": [
       "<Figure size 640x480 with 1 Axes>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "test['Age'].apply(lambda x: x[0]).hist()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 20,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "<Axes: >"
      ]
     },
     "execution_count": 20,
     "metadata": {},
     "output_type": "execute_result"
    },
    {
     "data": {
      "image/png": "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",
      "text/plain": [
       "<Figure size 640x480 with 1 Axes>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "df['Age'].apply(lambda x: x[0]).hist()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "0it [00:00, ?it/s]"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "3200it [00:00, 3209.23it/s]\n"
     ]
    }
   ],
   "source": [
    "train_data, valid_data = prepare_data(conf)\n",
    "\n",
    "train_dataset = SplittingDataset(\n",
    "    train_data,\n",
    "    split_strategy.create(**conf.train.split_strategy),\n",
    "    conf.features.target_col,\n",
    ")\n",
    "train_dataset = TargetEnumeratorDataset(train_dataset)\n",
    "# train_dataset = TargetDataset(train_dataset)\n",
    "train_dataset = ConvertingTrxDataset(train_dataset)\n",
    "# не уверен что нам нужна история с дропаутом точек.\n",
    "# Но это выглядит неплохой аугментацией в целом\n",
    "train_dataset = DropoutTrxDataset(\n",
    "    train_dataset, trx_dropout=conf.train.dropout, seq_len=conf.train.max_seq_len\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [],
   "source": [
    "batch = [train_dataset[0], train_dataset[14]]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 32,
   "metadata": {},
   "outputs": [],
   "source": [
    "for i in range(50):\n",
    "    if train_dataset[i][0][1][1].item() == 1:\n",
    "        break"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [
    {
     "ename": "NameError",
     "evalue": "name 'batch' is not defined",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mNameError\u001b[0m                                 Traceback (most recent call last)",
      "\u001b[1;32m/home/event_seq/experiments/physionet/notebooks/testing.ipynb Cell 7\u001b[0m line \u001b[0;36m1\n\u001b[0;32m----> <a href='vscode-notebook-cell://attached-container%2B7b22636f6e7461696e65724e616d65223a222f6d7573696e675f72696465222c2273657474696e6773223a7b22686f7374223a227373683a2f2f6d696461732e736b6f6c746563682e7275227d7d/home/event_seq/experiments/physionet/notebooks/testing.ipynb#W6sdnNjb2RlLXJlbW90ZQ%3D%3D?line=0'>1</a>\u001b[0m batch\n",
      "\u001b[0;31mNameError\u001b[0m: name 'batch' is not defined"
     ]
    }
   ],
   "source": [
    "batch"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 45,
   "metadata": {},
   "outputs": [],
   "source": [
    "import functools\n",
    "import operator\n",
    "from collections import defaultdict\n",
    "from typing import Dict\n",
    "\n",
    "import torch\n",
    "from torch.utils.data import DataLoader\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 46,
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "class PaddedBatch:\n",
    "    def __init__(self, payload: Dict[str, torch.Tensor], length: torch.LongTensor):\n",
    "        self._payload = payload\n",
    "        self._length = length\n",
    "\n",
    "    @property\n",
    "    def payload(self):\n",
    "        return self._payload\n",
    "\n",
    "    @property\n",
    "    def seq_lens(self):\n",
    "        return self._length\n",
    "\n",
    "    def __len__(self):\n",
    "        return len(self._length)\n",
    "\n",
    "    def to(self, device, non_blocking=False):\n",
    "        length = self._length.to(device=device, non_blocking=non_blocking)\n",
    "        payload = {\n",
    "            k: v.to(device=device, non_blocking=non_blocking)\n",
    "            for k, v in self._payload.items()\n",
    "        }\n",
    "        return PaddedBatch(payload, length)  # type: ignore\n",
    "\n",
    "\n",
    "def padded_collate(batch):\n",
    "    new_x_ = defaultdict(list)\n",
    "    for x, _ in batch:\n",
    "        for k, v in x.items():\n",
    "            new_x_[k].append(v)\n",
    "\n",
    "    lengths = torch.LongTensor([len(e) for e in next(iter(new_x_.values()))])\n",
    "\n",
    "    new_x = {\n",
    "        k: torch.nn.utils.rnn.pad_sequence(v, batch_first=True)\n",
    "        for k, v in new_x_.items()\n",
    "    }\n",
    "    new_idx = torch.tensor([y[0] for _, y in batch])\n",
    "    new_y = torch.tensor([y[1] for _, y in batch])\n",
    "\n",
    "    return PaddedBatch(new_x, lengths), torch.cat(\n",
    "        [new_idx.unsqueeze(0), new_y.unsqueeze(0)], dim=0\n",
    "    )\n",
    "\n",
    "\n",
    "def collate_splitted_rows(batch):\n",
    "    # flattens samples in list of lists to samples in list\n",
    "    batch = functools.reduce(operator.iadd, batch)\n",
    "    return padded_collate(batch)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 47,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "tensor([0, 1])"
      ]
     },
     "execution_count": 47,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "collate_splitted_rows(batch)[1][1]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": 21,
   "metadata": {
    "tags": []
   },
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "346it [00:00, 3459.24it/s]"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "3200it [00:00, 5060.75it/s]\n"
     ]
    }
   ],
   "source": [
    "train_loader, valid_loader = create_data_loaders(conf)\n",
    "#test_loader = create_test_loader(conf)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 22,
   "metadata": {},
   "outputs": [],
   "source": [
    "for batch in train_loader:\n",
    "    break"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 25,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "torch.Size([128, 128])"
      ]
     },
     "execution_count": 25,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "batch[0].payload['Albumin'].size()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 28,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "tensor([0.0000, 0.0000, 0.0000, 0.0000, 0.0000, 0.0000, 0.0000, 0.0000, 0.0000,\n",
       "        0.0000, 0.0000, 0.0000, 0.0000, 0.0000, 0.0000, 0.0000, 0.0000, 0.0000,\n",
       "        0.0000, 0.0000, 0.0000, 0.0000, 0.0000, 0.0000, 0.0000, 0.0000, 0.0000,\n",
       "        0.0000, 0.0000, 0.0000, 0.0000, 0.0000, 0.0000, 0.0000, 0.0000, 0.0000,\n",
       "        0.0000, 0.0000, 0.0000, 0.0000, 0.0000, 0.0000, 0.0000, 0.0000, 0.0000,\n",
       "        0.0000, 0.0000, 0.0000, 0.0000, 0.0000, 0.0000, 0.0000, 0.0000, 0.0000,\n",
       "        0.0000, 0.0000, 0.0000, 0.0000, 0.0000, 0.0000, 0.0000, 0.0000, 0.0000,\n",
       "        0.0000, 0.0000, 0.0000, 0.0000, 0.0000, 0.0000, 0.0000, 0.0000, 0.0000,\n",
       "        0.0000, 0.0000, 0.0000, 0.0000, 0.0000, 0.0000, 0.0000, 0.0000, 0.0000,\n",
       "        0.0000, 0.0000, 0.0000, 0.0000, 0.0000, 0.0000, 0.0000, 0.0000, 0.0000,\n",
       "        0.0000, 0.0000, 0.0000, 0.0000, 0.0000, 0.0000, 0.0000, 0.0000, 0.0000,\n",
       "        0.0000, 0.0000, 0.0000, 0.9979, 0.0000, 0.0000, 0.0000, 0.0000, 0.0000,\n",
       "        0.0000, 0.0000, 0.0000, 0.0000, 0.0000, 0.0000, 0.0000, 0.0000, 0.0000,\n",
       "        0.0000, 0.0000, 0.0000, 0.0000, 0.0000, 0.0000, 0.0000, 0.0000, 0.0000,\n",
       "        0.0000, 0.0000], dtype=torch.float64)"
      ]
     },
     "execution_count": 28,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "batch[0].payload['event_time'][:,-1]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 33,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "tensor(0.)"
      ]
     },
     "execution_count": 33,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "torch.sin(torch.tensor(0))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 30,
   "metadata": {},
   "outputs": [],
   "source": [
    "lens = batch[0].seq_lens - 1"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 38,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "128"
      ]
     },
     "execution_count": 38,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "len(lens)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": 23,
   "metadata": {},
   "outputs": [],
   "source": [
    "x = torch.rand(128, 121, 32)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 62,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "tensor([64, 64, 64, 64, 64, 64, 64, 64, 64, 64, 64, 64, 64, 64, 64, 64, 64, 64,\n",
       "        64, 64, 64, 64, 64, 64, 64, 64, 64, 64, 64, 64, 64, 64])"
      ]
     },
     "execution_count": 62,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "lens.unsqueeze(0).unsqueeze(2).repeat(1, 1, 32)[0,0,:]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 53,
   "metadata": {},
   "outputs": [],
   "source": [
    "gathered = torch.gather(x, 1, lens.unsqueeze(0).unsqueeze(2).repeat(1, 1, 32))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 55,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "torch.Size([1, 128, 32])"
      ]
     },
     "execution_count": 55,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "gathered.size()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 54,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "tensor([0.1384, 0.8533, 0.3297, 0.7942, 0.0402, 0.5267, 0.6776, 0.4334, 0.2273,\n",
       "        0.7756, 0.1255, 0.2925, 0.0079, 0.8984, 0.2342, 0.3943, 0.6500, 0.5439,\n",
       "        0.5401, 0.4058, 0.8818, 0.3804, 0.2495, 0.0518, 0.5949, 0.9953, 0.1699,\n",
       "        0.1753, 0.4296, 0.2453, 0.8269, 0.8425])"
      ]
     },
     "execution_count": 54,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "gathered[0,2,:]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 52,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "tensor([0.7498, 0.4410, 0.3366, 0.5427, 0.2153, 0.9919, 0.6954, 0.9410, 0.5704,\n",
       "        0.5550, 0.9509, 0.9612, 0.5235, 0.2092, 0.9685, 0.8754, 0.2760, 0.7941,\n",
       "        0.1731, 0.4050, 0.4290, 0.7468, 0.6349, 0.4570, 0.9743, 0.3449, 0.1059,\n",
       "        0.8301, 0.5587, 0.4942, 0.7697, 0.7323])"
      ]
     },
     "execution_count": 52,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "x[2,lens[2], :]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 73,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "torch.Size([128, 32])"
      ]
     },
     "execution_count": 73,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "x[:,lens,:].diagonal().T"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 75,
   "metadata": {},
   "outputs": [],
   "source": [
    "lstm = torch.nn.LSTM(input_size = 32, hidden_size=43)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 77,
   "metadata": {},
   "outputs": [],
   "source": [
    "x = torch.rand(16, 5, 32)\n",
    "ah, _ = lstm(x)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 80,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "(tensor([[[ 0.0732,  0.0801,  0.1020, -0.1204,  0.1263, -0.0383, -0.0513,\n",
       "            0.1629, -0.0234, -0.0594,  0.1264,  0.0807,  0.0056,  0.3168,\n",
       "            0.0027,  0.0530, -0.0199,  0.1594, -0.1713,  0.0037, -0.2926,\n",
       "           -0.0428, -0.1936, -0.0696,  0.0179, -0.1299, -0.1307, -0.1598,\n",
       "            0.1620, -0.0957, -0.0266, -0.1717, -0.0192, -0.0705, -0.1929,\n",
       "           -0.2732,  0.1136, -0.1260, -0.0935,  0.4427,  0.0504, -0.0657,\n",
       "           -0.1135],\n",
       "          [ 0.0098,  0.0720,  0.1105, -0.1589,  0.0432, -0.1394,  0.0343,\n",
       "            0.2303,  0.0189, -0.1476,  0.1567,  0.0541,  0.0124,  0.3682,\n",
       "            0.0339,  0.0963, -0.0829,  0.2176, -0.1656,  0.0037, -0.3031,\n",
       "           -0.1526, -0.2040, -0.0300, -0.0073, -0.1798, -0.1283, -0.1340,\n",
       "            0.3248, -0.1298, -0.1057, -0.1499,  0.0022, -0.0942, -0.2724,\n",
       "           -0.2618,  0.0081, -0.0987, -0.0293,  0.4103,  0.0689, -0.0648,\n",
       "           -0.1318],\n",
       "          [ 0.0353,  0.1024,  0.1028, -0.1193,  0.0437, -0.0645, -0.1658,\n",
       "            0.2347, -0.0266, -0.0614,  0.1465,  0.0804,  0.0181,  0.2810,\n",
       "            0.0256,  0.0886, -0.0572,  0.2189, -0.1797,  0.0256, -0.2557,\n",
       "           -0.0553, -0.1595, -0.0510, -0.0777, -0.1013, -0.1107, -0.0720,\n",
       "            0.2251, -0.1085, -0.0667, -0.1216,  0.0378, -0.0268, -0.2504,\n",
       "           -0.2531,  0.0387, -0.0594, -0.0353,  0.3807,  0.1325, -0.0364,\n",
       "           -0.1430],\n",
       "          [-0.0176,  0.1021,  0.1729, -0.1410,  0.0647, -0.0372, -0.0640,\n",
       "            0.0822, -0.0575, -0.1233,  0.1534,  0.0007,  0.0775,  0.3807,\n",
       "           -0.0290,  0.0841, -0.0944,  0.2084, -0.1734,  0.0613, -0.2957,\n",
       "           -0.0569, -0.2064, -0.0553, -0.0015, -0.1751, -0.1134, -0.0369,\n",
       "            0.2300, -0.1619, -0.0180, -0.1468,  0.0644, -0.0950, -0.1680,\n",
       "           -0.2897,  0.0815, -0.0547, -0.0932,  0.3516,  0.0736, -0.0557,\n",
       "           -0.0843],\n",
       "          [ 0.0599,  0.0760,  0.1292, -0.1631,  0.0608, -0.1240, -0.0863,\n",
       "            0.0346,  0.0130, -0.0462,  0.1653,  0.0597,  0.0338,  0.3518,\n",
       "            0.0356,  0.1011, -0.1634,  0.1638, -0.2182,  0.0301, -0.2551,\n",
       "           -0.1376, -0.2204, -0.0719, -0.0631, -0.1269, -0.1685, -0.0908,\n",
       "            0.2460, -0.1327, -0.1229, -0.2107,  0.0530, -0.0457, -0.1790,\n",
       "           -0.3183,  0.0606, -0.0367,  0.0379,  0.3971,  0.0418, -0.0355,\n",
       "           -0.1273]]], grad_fn=<StackBackward0>),\n",
       " tensor([[[ 0.1536,  0.1352,  0.2318, -0.2992,  0.2223, -0.0889, -0.0945,\n",
       "            0.2931, -0.0396, -0.1302,  0.2705,  0.1938,  0.0142,  0.6272,\n",
       "            0.0062,  0.1322, -0.0380,  0.3046, -0.3948,  0.0078, -0.7398,\n",
       "           -0.0743, -0.3681, -0.1242,  0.0387, -0.2455, -0.2469, -0.3765,\n",
       "            0.2328, -0.1448, -0.0546, -0.4188, -0.0350, -0.1285, -0.3900,\n",
       "           -0.6022,  0.1935, -0.2120, -0.1736,  0.7521,  0.1107, -0.1492,\n",
       "           -0.2088],\n",
       "          [ 0.0194,  0.1326,  0.2606, -0.4361,  0.0790, -0.3512,  0.0661,\n",
       "            0.4202,  0.0351, -0.2716,  0.3946,  0.1095,  0.0346,  0.7745,\n",
       "            0.0913,  0.2367, -0.1552,  0.4042, -0.4878,  0.0084, -0.9239,\n",
       "           -0.2594, -0.3935, -0.0527, -0.0149, -0.3165, -0.2916, -0.3763,\n",
       "            0.4497, -0.1976, -0.2146, -0.4170,  0.0038, -0.1810, -0.5801,\n",
       "           -0.6268,  0.0130, -0.1848, -0.0551,  0.7508,  0.1381, -0.1652,\n",
       "           -0.2871],\n",
       "          [ 0.0678,  0.1816,  0.2635, -0.2955,  0.0801, -0.1551, -0.3302,\n",
       "            0.4041, -0.0546, -0.1548,  0.3145,  0.1812,  0.0530,  0.5365,\n",
       "            0.0640,  0.2408, -0.1082,  0.4317, -0.4167,  0.0564, -0.6731,\n",
       "           -0.1079, -0.2880, -0.0945, -0.1523, -0.2131, -0.2104, -0.1648,\n",
       "            0.3656, -0.1728, -0.1306, -0.3252,  0.0728, -0.0467, -0.5418,\n",
       "           -0.5848,  0.0669, -0.0991, -0.0616,  0.6155,  0.2611, -0.0753,\n",
       "           -0.2557],\n",
       "          [-0.0339,  0.1963,  0.3790, -0.3302,  0.1103, -0.0864, -0.1299,\n",
       "            0.1459, -0.1095, -0.2806,  0.3277,  0.0015,  0.2107,  0.8233,\n",
       "           -0.0724,  0.2005, -0.1897,  0.3670, -0.4494,  0.1095, -0.7099,\n",
       "           -0.0956, -0.4593, -0.1026, -0.0031, -0.3202, -0.2164, -0.0929,\n",
       "            0.3287, -0.2821, -0.0355, -0.3536,  0.1208, -0.1973, -0.3362,\n",
       "           -0.5752,  0.1410, -0.0985, -0.1873,  0.5822,  0.1357, -0.1129,\n",
       "           -0.1510],\n",
       "          [ 0.1410,  0.1292,  0.2932, -0.3936,  0.1212, -0.3025, -0.1647,\n",
       "            0.0593,  0.0269, -0.0965,  0.3655,  0.1410,  0.0824,  0.6190,\n",
       "            0.0921,  0.2556, -0.3314,  0.3527, -0.5478,  0.0577, -0.5674,\n",
       "           -0.2336, -0.4187, -0.1419, -0.1234, -0.2468, -0.3537, -0.2565,\n",
       "            0.3621, -0.2053, -0.2523, -0.4910,  0.1003, -0.0785, -0.3578,\n",
       "           -0.6772,  0.1036, -0.0599,  0.0712,  0.6889,  0.0814, -0.0805,\n",
       "           -0.2069]]], grad_fn=<StackBackward0>))"
      ]
     },
     "execution_count": 80,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "_"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "The autoreload extension is already loaded. To reload it, use:\n",
      "  %reload_ext autoreload\n"
     ]
    }
   ],
   "source": [
    "%load_ext autoreload\n",
    "%autoreload 2\n",
    "\n",
    "from src.models.mTAND.base_models import SimpleClassifier, FeatureProcessor"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [],
   "source": [
    "fp = FeatureProcessor(model_conf=model_conf, data_conf=conf)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "torch.Size([256, 122, 38]) torch.Size([256, 122, 24])\n"
     ]
    }
   ],
   "source": [
    "f = fp(batch[0])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "torch.Size([256, 122, 62])"
      ]
     },
     "execution_count": 11,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "f[0].size()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [],
   "source": [
    "net = SimpleClassifier(model_conf=model_conf, data_conf=conf)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [],
   "source": [
    "net = net.to(model_conf.device)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "16"
      ]
     },
     "execution_count": 16,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "def perform_epoch(model, batch, optimizer):\n",
    "    optimizer.zero_grad()\n",
    "    out = model(batch[0])\n",
    "    loss = model.loss(out, batch[1])\n",
    "    loss['total_loss'].backward()\n",
    "    optimizer.step()\n",
    "\n",
    "    return loss\n",
    "\n",
    "def train(model, loader, val_loader, optimizer, num_epochs):\n",
    "\n",
    "    model.train()\n",
    "    for epoch in range(num_epochs):\n",
    "        for i, batch in tqdm(enumerate(loader), total=len(loader)):\n",
    "            loss = perform_epoch(model, batch, optimizer)\n",
    "        \n",
    "        validate(model, val_loader)\n",
    "\n",
    "def validate(model, val_loader):\n",
    "\n",
    "    model.eval()\n",
    "    pred = []\n",
    "    y_true = []\n",
    "    with torch.no_grad():\n",
    "        for batch in tqdm(val_loader):\n",
    "            out = model(batch[0])\n",
    "            pred.append(out.)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "lr = 3e-3\n",
    "net = SimpleClassifier(model_conf=model_conf, data_conf=conf)\n",
    "optimizer = torch.optim.Adam(net.parameters(), lr=lr)\n",
    "train(net, train_loader, optimizer, 10)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "torch.Size([16, 125])"
      ]
     },
     "execution_count": 19,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "batch[0].payload['Age'].size()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "torch.Size([1, 16, 32])\n"
     ]
    }
   ],
   "source": [
    "out = net(batch[0])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 20,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "torch.Size([16, 2])"
      ]
     },
     "execution_count": 20,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "out.size()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 25,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "torch.Size([16])"
      ]
     },
     "execution_count": 25,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "out.argmax(dim=1).size()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 22,
   "metadata": {},
   "outputs": [],
   "source": [
    "loss_fn = nn.CrossEntropyLoss()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 23,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "tensor(0.7712, grad_fn=<NllLossBackward0>)"
      ]
     },
     "execution_count": 23,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "loss_fn(out, batch[1][1])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": 21,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "None\n",
      "<built-in method detach of Tensor object at 0x7fd8e0bddfd0>\n",
      "None\n",
      "<built-in method detach of Tensor object at 0x7fd7d415ede0>\n",
      "None\n",
      "<built-in method detach of Tensor object at 0x7fd4d1d6e3e0>\n",
      "None\n",
      "<built-in method detach of Tensor object at 0x7fd8e0b9d8f0>\n",
      "None\n",
      "<built-in method detach of Tensor object at 0x7fd8e0b6ba10>\n",
      "None\n",
      "<built-in method detach of Tensor object at 0x7fd8e0b9e430>\n",
      "None\n",
      "<built-in method detach of Tensor object at 0x7fd8e0bde020>\n",
      "None\n",
      "<built-in method detach of Tensor object at 0x7fd8e0bde0c0>\n",
      "None\n",
      "<built-in method detach of Tensor object at 0x7fd4baa6b740>\n",
      "None\n"
     ]
    },
    {
     "ename": "AttributeError",
     "evalue": "'NoneType' object has no attribute 'detach'",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mAttributeError\u001b[0m                            Traceback (most recent call last)",
      "Cell \u001b[0;32mIn[21], line 3\u001b[0m\n\u001b[1;32m      1\u001b[0m \u001b[39mfor\u001b[39;00m param \u001b[39min\u001b[39;00m net\u001b[39m.\u001b[39mparameters():\n\u001b[1;32m      2\u001b[0m     \u001b[39mprint\u001b[39m(param\u001b[39m.\u001b[39mname)\n\u001b[0;32m----> 3\u001b[0m     \u001b[39mprint\u001b[39m(param\u001b[39m.\u001b[39;49mgrad\u001b[39m.\u001b[39;49mdetach)\n",
      "\u001b[0;31mAttributeError\u001b[0m: 'NoneType' object has no attribute 'detach'"
     ]
    }
   ],
   "source": [
    "for param in net.parameters():\n",
    "    print(param.name)\n",
    "    print(param.grad.detach)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [],
   "source": [
    "chkp_path = '../data/test_chkp.pth'\n",
    "chkp = {}\n",
    "chkp['model'] = net.state_dict()\n",
    "torch.save(chkp, chkp_path)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "metadata": {},
   "outputs": [],
   "source": [
    "out = net(batch[0])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "metadata": {},
   "outputs": [],
   "source": [
    "losses = net.loss(out)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "{'elbo_loss': tensor(1488891.7500, grad_fn=<AddBackward0>),\n",
       " 'kl_loss': tensor(4.6083, grad_fn=<MeanBackward0>),\n",
       " 'recon_loss': tensor(1488891.2500, grad_fn=<MeanBackward0>),\n",
       " 'total_CE_loss': tensor(6.5203, grad_fn=<SumBackward0>),\n",
       " 'total_loss': tensor(1488895., grad_fn=<AddBackward0>),\n",
       " 'Gender': tensor(2.2982, grad_fn=<MeanBackward0>),\n",
       " 'ICUType': tensor(2.0160, grad_fn=<MeanBackward0>),\n",
       " 'MechVent': tensor(2.2061, grad_fn=<MeanBackward0>)}"
      ]
     },
     "execution_count": 19,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "losses"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "7200it [00:02, 3347.82it/s]\n",
      "800it [00:00, 6348.51it/s]\n",
      "100%|██████████| 90/90 [00:02<00:00, 31.14it/s]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "valid embeds saved\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "100%|██████████| 200/200 [00:05<00:00, 36.33it/s]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "test embeds saved\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "100%|██████████| 1710/1710 [00:40<00:00, 41.73it/s]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "train embeds saved\n"
     ]
    }
   ],
   "source": [
    "create_embeddings(conf, model_conf)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [],
   "source": [
    "train_embeds = pd.read_csv(conf.train_embed_path, index_col=0)\n",
    "test_embeds = pd.read_csv(conf.test_embed_path, index_col=0)\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 25,
   "metadata": {},
   "outputs": [],
   "source": [
    "train_y = pd.read_parquet(conf.train_path)[conf.features.target_col]\n",
    "test_y = pd.read_parquet(conf.test_path)[conf.features.target_col].astype(int)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 26,
   "metadata": {},
   "outputs": [],
   "source": [
    "train = train_embeds.join(train_y)\n",
    "train = train.dropna()\n",
    "train[conf.features.target_col] = train[conf.features.target_col].astype(int)\n",
    "\n",
    "test = test_embeds.join(test_y)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 27,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": 37,
   "metadata": {},
   "outputs": [],
   "source": [
    "from lightgbm import LGBMClassifier"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 39,
   "metadata": {},
   "outputs": [],
   "source": [
    "params = {\n",
    "    \"n_estimators\": 500,\n",
    "    \"boosting_type\": \"gbdt\",\n",
    "    \"objective\": \"binary\",\n",
    "    \"metric\": \"auc\",\n",
    "    \"subsample\": 0.5,\n",
    "    \"subsample_freq\": 1,\n",
    "    \"learning_rate\": 0.02,\n",
    "    \"feature_fraction\": 0.75,\n",
    "    \"max_depth\": 6,\n",
    "    \"lambda_l1\": 1,\n",
    "    \"lambda_l2\": 1,\n",
    "    \"min_data_in_leaf\": 50,\n",
    "    \"random_state\": 42,\n",
    "    \"n_jobs\": 8,\n",
    "    \"reg_alpha\": None,\n",
    "    \"reg_lambda\": None,\n",
    "    \"colsample_bytree\": None,\n",
    "    \"min_child_samples\": None\n",
    "}"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 40,
   "metadata": {},
   "outputs": [],
   "source": [
    "model = LGBMClassifier(**params)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 41,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[LightGBM] [Warning] Accuracy may be bad since you didn't explicitly set num_leaves OR 2^max_depth > num_leaves. (num_leaves=31).\n",
      "[LightGBM] [Warning] Accuracy may be bad since you didn't explicitly set num_leaves OR 2^max_depth > num_leaves. (num_leaves=31).\n",
      "[LightGBM] [Info] Number of positive: 2474, number of negative: 2026\n",
      "[LightGBM] [Warning] Auto-choosing col-wise multi-threading, the overhead of testing was 0.001982 seconds.\n",
      "You can set `force_col_wise=true` to remove the overhead.\n",
      "[LightGBM] [Info] Total Bins 8160\n",
      "[LightGBM] [Info] Number of data points in the train set: 4500, number of used features: 32\n",
      "[LightGBM] [Info] [binary:BoostFromScore]: pavg=0.549778 -> initscore=0.199773\n",
      "[LightGBM] [Info] Start training from score 0.199773\n",
      "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
      "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
      "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
      "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
      "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
      "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
      "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
      "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
      "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
      "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
      "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
      "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
      "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
      "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
      "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
      "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
      "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
      "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
      "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
      "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
      "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
      "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
      "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
      "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
      "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
      "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
      "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
      "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
      "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
      "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
      "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
      "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
      "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
      "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
      "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
      "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
      "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
      "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
      "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
      "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
      "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
      "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
      "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
      "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
      "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
      "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
      "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
      "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
      "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
      "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
      "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
      "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
      "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
      "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
      "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
      "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
      "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
      "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
      "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
      "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
      "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
      "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
      "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
      "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
      "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
      "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
      "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
      "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
      "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
      "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
      "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
      "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
      "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
      "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
      "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
      "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
      "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
      "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
      "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
      "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
      "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
      "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
      "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
      "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
      "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
      "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
      "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
      "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
      "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
      "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
      "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
      "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
      "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
      "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
      "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
      "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
      "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
      "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
      "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
      "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
      "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
      "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
      "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
      "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
      "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
      "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
      "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
      "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
      "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
      "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
      "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
      "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
      "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
      "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
      "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
      "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
      "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
      "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
      "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
      "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
      "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
      "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
      "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
      "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
      "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
      "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
      "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
      "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
      "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
      "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
      "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
      "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
      "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
      "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
      "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
      "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
      "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
      "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
      "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
      "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
      "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
      "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
      "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
      "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
      "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
      "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
      "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
      "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
      "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
      "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
      "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
      "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
      "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
      "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
      "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
      "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
      "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
      "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
      "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
      "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
      "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
      "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
      "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
      "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
      "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
      "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
      "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
      "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
      "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
      "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
      "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
      "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
      "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
      "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
      "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
      "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
      "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
      "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
      "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
      "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
      "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
      "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
      "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
      "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
      "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
      "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
      "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
      "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
      "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
      "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
      "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
      "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
      "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
      "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
      "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
      "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
      "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
      "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
      "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
      "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
      "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
      "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
      "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
      "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
      "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
      "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
      "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
      "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
      "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
      "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
      "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
      "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
      "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
      "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
      "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
      "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
      "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
      "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
      "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
      "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
      "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
      "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
      "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
      "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
      "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
      "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
      "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
      "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
      "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
      "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
      "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
      "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
      "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
      "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
      "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
      "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
      "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
      "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
      "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
      "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
      "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
      "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
      "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
      "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
      "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
      "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
      "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
      "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
      "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
      "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
      "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
      "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
      "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
      "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
      "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
      "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
      "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
      "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
      "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
      "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
      "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
      "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
      "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
      "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
      "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
      "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
      "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
      "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
      "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
      "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
      "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
      "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
      "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
      "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
      "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
      "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
      "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
      "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
      "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
      "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
      "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
      "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
      "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
      "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
      "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
      "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
      "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
      "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
      "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
      "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
      "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
      "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
      "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
      "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
      "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
      "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
      "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
      "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
      "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
      "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
      "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
      "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
      "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
      "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
      "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
      "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
      "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
      "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
      "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
      "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
      "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
      "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
      "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
      "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
      "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
      "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
      "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
      "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
      "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
      "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
      "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
      "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
      "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
      "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
      "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
      "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
      "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
      "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
      "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
      "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
      "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
      "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
      "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
      "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
      "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
      "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
      "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
      "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
      "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
      "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
      "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
      "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
      "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
      "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
      "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
      "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
      "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
      "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
      "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
      "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
      "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
      "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
      "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
      "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
      "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
      "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
      "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
      "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
      "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
      "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
      "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
      "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
      "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
      "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
      "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
      "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
      "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
      "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
      "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
      "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
      "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
      "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
      "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
      "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
      "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
      "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
      "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
      "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
      "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
      "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
      "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
      "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
      "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
      "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
      "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
      "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
      "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
      "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
      "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
      "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
      "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
      "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
      "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
      "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
      "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
      "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
      "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
      "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
      "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
      "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
      "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
      "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
      "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
      "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
      "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
      "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
      "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
      "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
      "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
      "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
      "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
      "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
      "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
      "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
      "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
      "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
      "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
      "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
      "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
      "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
      "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
      "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
      "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
      "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
      "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
      "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
      "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
      "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
      "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
      "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
      "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
      "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
      "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
      "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
      "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
      "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
      "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
      "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
      "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
      "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
      "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
      "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
      "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
      "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
      "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
      "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
      "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
      "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
      "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
      "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
      "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
      "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
      "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
      "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
      "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
      "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
      "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
      "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
      "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
      "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
      "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
      "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
      "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
      "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
      "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
      "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
      "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
      "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
      "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
      "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
      "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
      "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
      "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
      "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
      "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
      "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
      "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
      "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
      "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
      "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
      "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
      "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
      "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
      "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
      "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
      "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
      "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
      "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
      "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
      "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
      "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
      "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
      "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
      "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
      "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
      "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n"
     ]
    },
    {
     "data": {
      "text/html": [
       "<style>#sk-container-id-4 {color: black;}#sk-container-id-4 pre{padding: 0;}#sk-container-id-4 div.sk-toggleable {background-color: white;}#sk-container-id-4 label.sk-toggleable__label {cursor: pointer;display: block;width: 100%;margin-bottom: 0;padding: 0.3em;box-sizing: border-box;text-align: center;}#sk-container-id-4 label.sk-toggleable__label-arrow:before {content: \"▸\";float: left;margin-right: 0.25em;color: #696969;}#sk-container-id-4 label.sk-toggleable__label-arrow:hover:before {color: black;}#sk-container-id-4 div.sk-estimator:hover label.sk-toggleable__label-arrow:before {color: black;}#sk-container-id-4 div.sk-toggleable__content {max-height: 0;max-width: 0;overflow: hidden;text-align: left;background-color: #f0f8ff;}#sk-container-id-4 div.sk-toggleable__content pre {margin: 0.2em;color: black;border-radius: 0.25em;background-color: #f0f8ff;}#sk-container-id-4 input.sk-toggleable__control:checked~div.sk-toggleable__content {max-height: 200px;max-width: 100%;overflow: auto;}#sk-container-id-4 input.sk-toggleable__control:checked~label.sk-toggleable__label-arrow:before {content: \"▾\";}#sk-container-id-4 div.sk-estimator input.sk-toggleable__control:checked~label.sk-toggleable__label {background-color: #d4ebff;}#sk-container-id-4 div.sk-label input.sk-toggleable__control:checked~label.sk-toggleable__label {background-color: #d4ebff;}#sk-container-id-4 input.sk-hidden--visually {border: 0;clip: rect(1px 1px 1px 1px);clip: rect(1px, 1px, 1px, 1px);height: 1px;margin: -1px;overflow: hidden;padding: 0;position: absolute;width: 1px;}#sk-container-id-4 div.sk-estimator {font-family: monospace;background-color: #f0f8ff;border: 1px dotted black;border-radius: 0.25em;box-sizing: border-box;margin-bottom: 0.5em;}#sk-container-id-4 div.sk-estimator:hover {background-color: #d4ebff;}#sk-container-id-4 div.sk-parallel-item::after {content: \"\";width: 100%;border-bottom: 1px solid gray;flex-grow: 1;}#sk-container-id-4 div.sk-label:hover label.sk-toggleable__label {background-color: #d4ebff;}#sk-container-id-4 div.sk-serial::before {content: \"\";position: absolute;border-left: 1px solid gray;box-sizing: border-box;top: 0;bottom: 0;left: 50%;z-index: 0;}#sk-container-id-4 div.sk-serial {display: flex;flex-direction: column;align-items: center;background-color: white;padding-right: 0.2em;padding-left: 0.2em;position: relative;}#sk-container-id-4 div.sk-item {position: relative;z-index: 1;}#sk-container-id-4 div.sk-parallel {display: flex;align-items: stretch;justify-content: center;background-color: white;position: relative;}#sk-container-id-4 div.sk-item::before, #sk-container-id-4 div.sk-parallel-item::before {content: \"\";position: absolute;border-left: 1px solid gray;box-sizing: border-box;top: 0;bottom: 0;left: 50%;z-index: -1;}#sk-container-id-4 div.sk-parallel-item {display: flex;flex-direction: column;z-index: 1;position: relative;background-color: white;}#sk-container-id-4 div.sk-parallel-item:first-child::after {align-self: flex-end;width: 50%;}#sk-container-id-4 div.sk-parallel-item:last-child::after {align-self: flex-start;width: 50%;}#sk-container-id-4 div.sk-parallel-item:only-child::after {width: 0;}#sk-container-id-4 div.sk-dashed-wrapped {border: 1px dashed gray;margin: 0 0.4em 0.5em 0.4em;box-sizing: border-box;padding-bottom: 0.4em;background-color: white;}#sk-container-id-4 div.sk-label label {font-family: monospace;font-weight: bold;display: inline-block;line-height: 1.2em;}#sk-container-id-4 div.sk-label-container {text-align: center;}#sk-container-id-4 div.sk-container {/* jupyter's `normalize.less` sets `[hidden] { display: none; }` but bootstrap.min.css set `[hidden] { display: none !important; }` so we also need the `!important` here to be able to override the default hidden behavior on the sphinx rendered scikit-learn.org. See: https://github.com/scikit-learn/scikit-learn/issues/21755 */display: inline-block !important;position: relative;}#sk-container-id-4 div.sk-text-repr-fallback {display: none;}</style><div id=\"sk-container-id-4\" class=\"sk-top-container\"><div class=\"sk-text-repr-fallback\"><pre>LGBMClassifier(colsample_bytree=None, feature_fraction=0.75, lambda_l1=1,\n",
       "               lambda_l2=1, learning_rate=0.02, max_depth=6, metric=&#x27;auc&#x27;,\n",
       "               min_child_samples=None, min_data_in_leaf=50, n_estimators=500,\n",
       "               n_jobs=8, objective=&#x27;binary&#x27;, random_state=42, reg_alpha=None,\n",
       "               reg_lambda=None, subsample=0.5, subsample_freq=1)</pre><b>In a Jupyter environment, please rerun this cell to show the HTML representation or trust the notebook. <br />On GitHub, the HTML representation is unable to render, please try loading this page with nbviewer.org.</b></div><div class=\"sk-container\" hidden><div class=\"sk-item\"><div class=\"sk-estimator sk-toggleable\"><input class=\"sk-toggleable__control sk-hidden--visually\" id=\"sk-estimator-id-4\" type=\"checkbox\" checked><label for=\"sk-estimator-id-4\" class=\"sk-toggleable__label sk-toggleable__label-arrow\">LGBMClassifier</label><div class=\"sk-toggleable__content\"><pre>LGBMClassifier(colsample_bytree=None, feature_fraction=0.75, lambda_l1=1,\n",
       "               lambda_l2=1, learning_rate=0.02, max_depth=6, metric=&#x27;auc&#x27;,\n",
       "               min_child_samples=None, min_data_in_leaf=50, n_estimators=500,\n",
       "               n_jobs=8, objective=&#x27;binary&#x27;, random_state=42, reg_alpha=None,\n",
       "               reg_lambda=None, subsample=0.5, subsample_freq=1)</pre></div></div></div></div></div>"
      ],
      "text/plain": [
       "LGBMClassifier(colsample_bytree=None, feature_fraction=0.75, lambda_l1=1,\n",
       "               lambda_l2=1, learning_rate=0.02, max_depth=6, metric='auc',\n",
       "               min_child_samples=None, min_data_in_leaf=50, n_estimators=500,\n",
       "               n_jobs=8, objective='binary', random_state=42, reg_alpha=None,\n",
       "               reg_lambda=None, subsample=0.5, subsample_freq=1)"
      ]
     },
     "execution_count": 41,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "model.fit(train.drop(columns=[conf.features.target_col]), train[conf.features.target_col])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 42,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[LightGBM] [Warning] Accuracy may be bad since you didn't explicitly set num_leaves OR 2^max_depth > num_leaves. (num_leaves=31).\n"
     ]
    }
   ],
   "source": [
    "y_pred = model.predict(test.drop(columns=[conf.features.target_col]))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 43,
   "metadata": {},
   "outputs": [],
   "source": [
    "from sklearn.metrics import accuracy_score, roc_auc_score"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 44,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "0.49788132100047816"
      ]
     },
     "execution_count": 44,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "roc_auc_score(test_y, y_pred)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 27,
   "metadata": {},
   "outputs": [],
   "source": [
    "ckpt = torch.load(chkp_path)\n",
    "net.load_state_dict(ckpt[\"model\"])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 36,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "tensor([0, 0, 0, 0, 0, 1, 1, 1, 1, 1, 2, 2, 2, 2, 2, 3, 3, 3, 3, 3])"
      ]
     },
     "execution_count": 36,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "batch[1]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 49,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "0it [00:00, ?it/s]"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "9717it [00:00, 13852.25it/s]\n"
     ]
    }
   ],
   "source": [
    "split_strategy_dict = {\n",
    "    \"split_strategy\": \"NoSplit\"\n",
    "}\n",
    "train_data, valid_data = prepare_data(conf)\n",
    "\n",
    "train_dataset = SplittingDataset(\n",
    "    train_data,\n",
    "    split_strategy.create(**split_strategy_dict),\n",
    "    # conf.features.target_col,\n",
    ")\n",
    "train_dataset = TargetEnumeratorDataset(train_dataset)\n",
    "# train_dataset = TargetDataset(train_dataset)\n",
    "train_dataset = ConvertingTrxDataset(train_dataset)\n",
    "# не уверен что нам нужна история с дропаутом точек.\n",
    "# Но это выглядит неплохой аугментацией в целом\n",
    "\n",
    "\n",
    "train_loader = DataLoader(\n",
    "    dataset=train_dataset,\n",
    "    shuffle=True,\n",
    "    collate_fn=collate_splitted_rows,\n",
    "    num_workers=conf.train.num_workers,\n",
    "    batch_size=conf.train.batch_size,\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "100%|██████████| 2308/2308 [00:31<00:00, 74.11it/s]\n"
     ]
    }
   ],
   "source": [
    "with torch.no_grad():\n",
    "    train_embeddings = []\n",
    "    ids = []\n",
    "    for batch in tqdm(train_loader):\n",
    "        out = net(batch[0])\n",
    "        embeddings = out['z'].view(4, -1)\n",
    "        train_embeddings.append(embeddings)\n",
    "        ids.append(batch[1])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "metadata": {},
   "outputs": [],
   "source": [
    "all_embeds = torch.cat(train_embeddings)\n",
    "all_indices = torch.cat(ids)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 21,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "torch.Size([9232, 32])"
      ]
     },
     "execution_count": 21,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "all_embeds.size()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 22,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>0</th>\n",
       "      <th>1</th>\n",
       "      <th>2</th>\n",
       "      <th>3</th>\n",
       "      <th>4</th>\n",
       "      <th>5</th>\n",
       "      <th>6</th>\n",
       "      <th>7</th>\n",
       "      <th>8</th>\n",
       "      <th>9</th>\n",
       "      <th>...</th>\n",
       "      <th>22</th>\n",
       "      <th>23</th>\n",
       "      <th>24</th>\n",
       "      <th>25</th>\n",
       "      <th>26</th>\n",
       "      <th>27</th>\n",
       "      <th>28</th>\n",
       "      <th>29</th>\n",
       "      <th>30</th>\n",
       "      <th>31</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>7443</th>\n",
       "      <td>0.102797</td>\n",
       "      <td>1.169541</td>\n",
       "      <td>0.019089</td>\n",
       "      <td>-0.514786</td>\n",
       "      <td>-1.388183</td>\n",
       "      <td>0.961653</td>\n",
       "      <td>-0.590597</td>\n",
       "      <td>-0.262099</td>\n",
       "      <td>0.144197</td>\n",
       "      <td>1.172947</td>\n",
       "      <td>...</td>\n",
       "      <td>1.550775</td>\n",
       "      <td>1.656553</td>\n",
       "      <td>-1.211777</td>\n",
       "      <td>0.713178</td>\n",
       "      <td>-0.422202</td>\n",
       "      <td>0.057362</td>\n",
       "      <td>-0.811645</td>\n",
       "      <td>-0.603588</td>\n",
       "      <td>-0.488893</td>\n",
       "      <td>0.160870</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>8096</th>\n",
       "      <td>0.803449</td>\n",
       "      <td>1.956381</td>\n",
       "      <td>0.596296</td>\n",
       "      <td>0.496732</td>\n",
       "      <td>0.486962</td>\n",
       "      <td>-1.346716</td>\n",
       "      <td>-1.141745</td>\n",
       "      <td>0.070024</td>\n",
       "      <td>-1.569932</td>\n",
       "      <td>-0.472612</td>\n",
       "      <td>...</td>\n",
       "      <td>-0.723514</td>\n",
       "      <td>0.423374</td>\n",
       "      <td>-1.114043</td>\n",
       "      <td>2.086276</td>\n",
       "      <td>-2.363746</td>\n",
       "      <td>1.266546</td>\n",
       "      <td>-0.149737</td>\n",
       "      <td>-0.264849</td>\n",
       "      <td>-0.322092</td>\n",
       "      <td>-1.082480</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3679</th>\n",
       "      <td>-0.002727</td>\n",
       "      <td>-1.473100</td>\n",
       "      <td>1.137815</td>\n",
       "      <td>0.073783</td>\n",
       "      <td>0.528109</td>\n",
       "      <td>-0.423857</td>\n",
       "      <td>-0.033820</td>\n",
       "      <td>0.668729</td>\n",
       "      <td>1.464864</td>\n",
       "      <td>-1.710192</td>\n",
       "      <td>...</td>\n",
       "      <td>-0.659925</td>\n",
       "      <td>-0.128479</td>\n",
       "      <td>-0.790339</td>\n",
       "      <td>1.412809</td>\n",
       "      <td>0.014140</td>\n",
       "      <td>0.823128</td>\n",
       "      <td>-0.271114</td>\n",
       "      <td>0.360713</td>\n",
       "      <td>0.073781</td>\n",
       "      <td>1.201015</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>7751</th>\n",
       "      <td>0.523437</td>\n",
       "      <td>-1.091199</td>\n",
       "      <td>0.383193</td>\n",
       "      <td>-0.536097</td>\n",
       "      <td>0.407316</td>\n",
       "      <td>0.998033</td>\n",
       "      <td>-1.337681</td>\n",
       "      <td>-0.492494</td>\n",
       "      <td>-0.177824</td>\n",
       "      <td>0.827958</td>\n",
       "      <td>...</td>\n",
       "      <td>0.050278</td>\n",
       "      <td>-0.186300</td>\n",
       "      <td>-0.929536</td>\n",
       "      <td>0.382475</td>\n",
       "      <td>0.188110</td>\n",
       "      <td>-0.039815</td>\n",
       "      <td>-0.135864</td>\n",
       "      <td>0.371854</td>\n",
       "      <td>-0.226536</td>\n",
       "      <td>-1.075974</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1058</th>\n",
       "      <td>-0.090836</td>\n",
       "      <td>0.931548</td>\n",
       "      <td>0.487700</td>\n",
       "      <td>1.544706</td>\n",
       "      <td>0.134991</td>\n",
       "      <td>-0.388189</td>\n",
       "      <td>-0.351403</td>\n",
       "      <td>0.690511</td>\n",
       "      <td>0.369822</td>\n",
       "      <td>-0.996384</td>\n",
       "      <td>...</td>\n",
       "      <td>0.037646</td>\n",
       "      <td>0.474755</td>\n",
       "      <td>0.105620</td>\n",
       "      <td>1.116618</td>\n",
       "      <td>0.331259</td>\n",
       "      <td>-0.752344</td>\n",
       "      <td>-1.333730</td>\n",
       "      <td>0.134372</td>\n",
       "      <td>-1.005091</td>\n",
       "      <td>-0.208797</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>...</th>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>5050</th>\n",
       "      <td>0.647142</td>\n",
       "      <td>0.396011</td>\n",
       "      <td>-1.058875</td>\n",
       "      <td>1.620824</td>\n",
       "      <td>-2.104278</td>\n",
       "      <td>0.130711</td>\n",
       "      <td>0.205906</td>\n",
       "      <td>1.588393</td>\n",
       "      <td>-0.809435</td>\n",
       "      <td>0.058372</td>\n",
       "      <td>...</td>\n",
       "      <td>0.256889</td>\n",
       "      <td>-0.805244</td>\n",
       "      <td>-1.666883</td>\n",
       "      <td>-0.457891</td>\n",
       "      <td>-0.153310</td>\n",
       "      <td>1.100958</td>\n",
       "      <td>-0.033816</td>\n",
       "      <td>-0.539116</td>\n",
       "      <td>-0.516178</td>\n",
       "      <td>0.900175</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>7619</th>\n",
       "      <td>-0.387352</td>\n",
       "      <td>0.407266</td>\n",
       "      <td>0.045567</td>\n",
       "      <td>1.079472</td>\n",
       "      <td>0.276017</td>\n",
       "      <td>0.598434</td>\n",
       "      <td>0.116289</td>\n",
       "      <td>-0.153847</td>\n",
       "      <td>-0.532574</td>\n",
       "      <td>-0.304695</td>\n",
       "      <td>...</td>\n",
       "      <td>-0.291467</td>\n",
       "      <td>0.979066</td>\n",
       "      <td>-2.529730</td>\n",
       "      <td>-0.883053</td>\n",
       "      <td>-0.234192</td>\n",
       "      <td>1.234930</td>\n",
       "      <td>-0.504991</td>\n",
       "      <td>0.397301</td>\n",
       "      <td>-1.507193</td>\n",
       "      <td>-1.275558</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4744</th>\n",
       "      <td>-0.665543</td>\n",
       "      <td>1.066480</td>\n",
       "      <td>0.286753</td>\n",
       "      <td>0.817286</td>\n",
       "      <td>-1.457128</td>\n",
       "      <td>0.637894</td>\n",
       "      <td>0.323675</td>\n",
       "      <td>1.191625</td>\n",
       "      <td>-0.373805</td>\n",
       "      <td>-0.026182</td>\n",
       "      <td>...</td>\n",
       "      <td>-0.252915</td>\n",
       "      <td>0.829856</td>\n",
       "      <td>2.367053</td>\n",
       "      <td>1.139469</td>\n",
       "      <td>-0.526248</td>\n",
       "      <td>0.331766</td>\n",
       "      <td>-0.604189</td>\n",
       "      <td>0.403450</td>\n",
       "      <td>-2.052348</td>\n",
       "      <td>-1.497805</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>8663</th>\n",
       "      <td>0.188203</td>\n",
       "      <td>-0.145553</td>\n",
       "      <td>0.806009</td>\n",
       "      <td>-0.384668</td>\n",
       "      <td>-0.735343</td>\n",
       "      <td>-0.895407</td>\n",
       "      <td>-1.491430</td>\n",
       "      <td>1.100353</td>\n",
       "      <td>1.275650</td>\n",
       "      <td>0.737131</td>\n",
       "      <td>...</td>\n",
       "      <td>-0.490947</td>\n",
       "      <td>-1.864704</td>\n",
       "      <td>-0.011014</td>\n",
       "      <td>-0.212244</td>\n",
       "      <td>0.133844</td>\n",
       "      <td>0.649861</td>\n",
       "      <td>0.691730</td>\n",
       "      <td>-0.337050</td>\n",
       "      <td>-0.022557</td>\n",
       "      <td>0.339424</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>8239</th>\n",
       "      <td>-0.282619</td>\n",
       "      <td>1.473131</td>\n",
       "      <td>0.222512</td>\n",
       "      <td>-0.791756</td>\n",
       "      <td>-0.829103</td>\n",
       "      <td>1.993044</td>\n",
       "      <td>1.280572</td>\n",
       "      <td>2.522489</td>\n",
       "      <td>1.288986</td>\n",
       "      <td>0.759570</td>\n",
       "      <td>...</td>\n",
       "      <td>1.366453</td>\n",
       "      <td>-0.027310</td>\n",
       "      <td>0.828379</td>\n",
       "      <td>1.281615</td>\n",
       "      <td>0.028337</td>\n",
       "      <td>-1.453597</td>\n",
       "      <td>-1.242622</td>\n",
       "      <td>0.122499</td>\n",
       "      <td>0.269273</td>\n",
       "      <td>-0.909372</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "<p>9232 rows × 32 columns</p>\n",
       "</div>"
      ],
      "text/plain": [
       "            0         1         2         3         4         5         6   \\\n",
       "7443  0.102797  1.169541  0.019089 -0.514786 -1.388183  0.961653 -0.590597   \n",
       "8096  0.803449  1.956381  0.596296  0.496732  0.486962 -1.346716 -1.141745   \n",
       "3679 -0.002727 -1.473100  1.137815  0.073783  0.528109 -0.423857 -0.033820   \n",
       "7751  0.523437 -1.091199  0.383193 -0.536097  0.407316  0.998033 -1.337681   \n",
       "1058 -0.090836  0.931548  0.487700  1.544706  0.134991 -0.388189 -0.351403   \n",
       "...        ...       ...       ...       ...       ...       ...       ...   \n",
       "5050  0.647142  0.396011 -1.058875  1.620824 -2.104278  0.130711  0.205906   \n",
       "7619 -0.387352  0.407266  0.045567  1.079472  0.276017  0.598434  0.116289   \n",
       "4744 -0.665543  1.066480  0.286753  0.817286 -1.457128  0.637894  0.323675   \n",
       "8663  0.188203 -0.145553  0.806009 -0.384668 -0.735343 -0.895407 -1.491430   \n",
       "8239 -0.282619  1.473131  0.222512 -0.791756 -0.829103  1.993044  1.280572   \n",
       "\n",
       "            7         8         9   ...        22        23        24  \\\n",
       "7443 -0.262099  0.144197  1.172947  ...  1.550775  1.656553 -1.211777   \n",
       "8096  0.070024 -1.569932 -0.472612  ... -0.723514  0.423374 -1.114043   \n",
       "3679  0.668729  1.464864 -1.710192  ... -0.659925 -0.128479 -0.790339   \n",
       "7751 -0.492494 -0.177824  0.827958  ...  0.050278 -0.186300 -0.929536   \n",
       "1058  0.690511  0.369822 -0.996384  ...  0.037646  0.474755  0.105620   \n",
       "...        ...       ...       ...  ...       ...       ...       ...   \n",
       "5050  1.588393 -0.809435  0.058372  ...  0.256889 -0.805244 -1.666883   \n",
       "7619 -0.153847 -0.532574 -0.304695  ... -0.291467  0.979066 -2.529730   \n",
       "4744  1.191625 -0.373805 -0.026182  ... -0.252915  0.829856  2.367053   \n",
       "8663  1.100353  1.275650  0.737131  ... -0.490947 -1.864704 -0.011014   \n",
       "8239  2.522489  1.288986  0.759570  ...  1.366453 -0.027310  0.828379   \n",
       "\n",
       "            25        26        27        28        29        30        31  \n",
       "7443  0.713178 -0.422202  0.057362 -0.811645 -0.603588 -0.488893  0.160870  \n",
       "8096  2.086276 -2.363746  1.266546 -0.149737 -0.264849 -0.322092 -1.082480  \n",
       "3679  1.412809  0.014140  0.823128 -0.271114  0.360713  0.073781  1.201015  \n",
       "7751  0.382475  0.188110 -0.039815 -0.135864  0.371854 -0.226536 -1.075974  \n",
       "1058  1.116618  0.331259 -0.752344 -1.333730  0.134372 -1.005091 -0.208797  \n",
       "...        ...       ...       ...       ...       ...       ...       ...  \n",
       "5050 -0.457891 -0.153310  1.100958 -0.033816 -0.539116 -0.516178  0.900175  \n",
       "7619 -0.883053 -0.234192  1.234930 -0.504991  0.397301 -1.507193 -1.275558  \n",
       "4744  1.139469 -0.526248  0.331766 -0.604189  0.403450 -2.052348 -1.497805  \n",
       "8663 -0.212244  0.133844  0.649861  0.691730 -0.337050 -0.022557  0.339424  \n",
       "8239  1.281615  0.028337 -1.453597 -1.242622  0.122499  0.269273 -0.909372  \n",
       "\n",
       "[9232 rows x 32 columns]"
      ]
     },
     "execution_count": 22,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "pd.DataFrame(data=all_embeds.numpy(), index=all_indices.numpy())"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 29,
   "metadata": {},
   "outputs": [],
   "source": [
    "criterion = nn.CrossEntropyLoss(reduction='none')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 22,
   "metadata": {},
   "outputs": [],
   "source": [
    "out_dist = torch.rand(20, 4, 100)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 23,
   "metadata": {},
   "outputs": [],
   "source": [
    "target = torch.rand(20, 1, 100)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "torch.Size([2000, 4])"
      ]
     },
     "execution_count": 15,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "out_dist.view(-1, 4).size()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 33,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "tensor(141.1932)"
      ]
     },
     "execution_count": 33,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "criterion(out_dist, target.squeeze(1).long(), ).sum(dim=1).mean()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 47,
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "loss_out = net.loss(out)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "metadata": {
    "tags": []
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "{'elbo_loss': tensor(12326.9258, grad_fn=<AddBackward0>),\n",
       " 'kl_loss': tensor(0.5898, grad_fn=<MeanBackward0>),\n",
       " 'recon_loss': tensor(12326.8672, grad_fn=<MeanBackward0>)}"
      ]
     },
     "execution_count": 16,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "loss_out"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "trainer = MtandTrainer(\n",
    "    model=net,\n",
    "    optimizer=torch.optim.Adam(net.parameters(), lr=3e-4),\n",
    "    train_loader=train_loader,\n",
    "    val_loader=valid_loader,\n",
    "    total_iters=50,\n",
    "    iters_per_epoch=20,\n",
    "    ckpt_dir=\"./ckpt\",\n",
    "    ckpt_replace=True,\n",
    "    ckpt_track_metric=\"loss\",\n",
    "    metrics_on_train=True,\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "metadata": {
    "tags": []
   },
   "outputs": [
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "eb21d5f726fa43708ece01e5557e1f99",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "  0%|          | 0/20 [00:00<?, ?it/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "b98b399db3e9443395b42783a636fb68",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "  0%|          | 0/122 [00:00<?, ?it/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "182497721c094f468ef61c1bd33acb12",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "  0%|          | 0/20 [00:00<?, ?it/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "46d27938b4e245e095e8715abbb2ad70",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "  0%|          | 0/122 [00:00<?, ?it/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "eea6cd2d8dc44daa8dbd272e6327785d",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "  0%|          | 0/10 [00:00<?, ?it/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "9065eb3294be4583aa8c0f78d418c97a",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "  0%|          | 0/122 [00:00<?, ?it/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "trainer.run()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": 20,
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "def perform_epoch(model, batch, optimizer):\n",
    "    optimizer.zero_grad()\n",
    "    out = model(batch[0])\n",
    "    loss = model.loss(out)\n",
    "    loss['elbo_loss'].backward()\n",
    "    optimizer.step()\n",
    "\n",
    "    return loss\n",
    "\n",
    "def train(model, loader, optimizer, num_epochs):\n",
    "\n",
    "    model.train()\n",
    "    recon_loss = []\n",
    "    kl_loss = []\n",
    "    for epoch in range(num_epochs):\n",
    "        for i, batch in tqdm(enumerate(loader), total=len(loader)):\n",
    "            loss = perform_epoch(model, batch, optimizer)\n",
    "            recon_loss.append(loss['recon_loss'].detach().item())\n",
    "            kl_loss.append(loss['kl_loss'].detach().item())\n",
    "\n",
    "            if (i+1) % 1 == 0:\n",
    "                plot_losses(recon_loss, kl_loss)\n",
    "\n",
    "\n",
    "def plot_losses(recon_loss, kl_loss):\n",
    "    clear_output(True)\n",
    "    plt.plot(np.log(recon_loss), color='g', label='recon')\n",
    "    plt.plot(kl_loss, color='r', label='kl')\n",
    "    plt.legend()   \n",
    "    plt.show() "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 21,
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "lr = 3e-4\n",
    "net = MegaNet(model_conf=model_conf, data_conf=conf)\n",
    "optimizer = torch.optim.Adam(net.parameters(), lr=lr)\n",
    "train(net, train_loader, optimizer, 10)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 22,
   "metadata": {
    "tags": []
   },
   "outputs": [
    {
     "data": {
      "image/png": "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",
      "text/plain": [
       "<Figure size 640x480 with 1 Axes>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "  1%|          | 26/2308 [00:07<11:32,  3.30it/s]\n"
     ]
    },
    {
     "ename": "KeyboardInterrupt",
     "evalue": "",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mKeyboardInterrupt\u001b[0m                         Traceback (most recent call last)",
      "Cell \u001b[0;32mIn[22], line 1\u001b[0m\n\u001b[0;32m----> 1\u001b[0m \u001b[43mtrain\u001b[49m\u001b[43m(\u001b[49m\u001b[43mnet\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mtrain_loader\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43moptimizer\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;241;43m10\u001b[39;49m\u001b[43m)\u001b[49m\n",
      "Cell \u001b[0;32mIn[20], line 17\u001b[0m, in \u001b[0;36mtrain\u001b[0;34m(model, loader, optimizer, num_epochs)\u001b[0m\n\u001b[1;32m     15\u001b[0m \u001b[38;5;28;01mfor\u001b[39;00m epoch \u001b[38;5;129;01min\u001b[39;00m \u001b[38;5;28mrange\u001b[39m(num_epochs):\n\u001b[1;32m     16\u001b[0m     \u001b[38;5;28;01mfor\u001b[39;00m i, batch \u001b[38;5;129;01min\u001b[39;00m tqdm(\u001b[38;5;28menumerate\u001b[39m(loader), total\u001b[38;5;241m=\u001b[39m\u001b[38;5;28mlen\u001b[39m(loader)):\n\u001b[0;32m---> 17\u001b[0m         loss \u001b[38;5;241m=\u001b[39m \u001b[43mperform_epoch\u001b[49m\u001b[43m(\u001b[49m\u001b[43mmodel\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mbatch\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43moptimizer\u001b[49m\u001b[43m)\u001b[49m\n\u001b[1;32m     18\u001b[0m         recon_loss\u001b[38;5;241m.\u001b[39mappend(loss[\u001b[38;5;124m'\u001b[39m\u001b[38;5;124mrecon_loss\u001b[39m\u001b[38;5;124m'\u001b[39m]\u001b[38;5;241m.\u001b[39mdetach()\u001b[38;5;241m.\u001b[39mitem())\n\u001b[1;32m     19\u001b[0m         kl_loss\u001b[38;5;241m.\u001b[39mappend(loss[\u001b[38;5;124m'\u001b[39m\u001b[38;5;124mkl_loss\u001b[39m\u001b[38;5;124m'\u001b[39m]\u001b[38;5;241m.\u001b[39mdetach()\u001b[38;5;241m.\u001b[39mitem())\n",
      "Cell \u001b[0;32mIn[20], line 3\u001b[0m, in \u001b[0;36mperform_epoch\u001b[0;34m(model, batch, optimizer)\u001b[0m\n\u001b[1;32m      1\u001b[0m \u001b[38;5;28;01mdef\u001b[39;00m \u001b[38;5;21mperform_epoch\u001b[39m(model, batch, optimizer):\n\u001b[1;32m      2\u001b[0m     optimizer\u001b[38;5;241m.\u001b[39mzero_grad()\n\u001b[0;32m----> 3\u001b[0m     out \u001b[38;5;241m=\u001b[39m \u001b[43mmodel\u001b[49m\u001b[43m(\u001b[49m\u001b[43mbatch\u001b[49m\u001b[43m[\u001b[49m\u001b[38;5;241;43m0\u001b[39;49m\u001b[43m]\u001b[49m\u001b[43m)\u001b[49m\n\u001b[1;32m      4\u001b[0m     loss \u001b[38;5;241m=\u001b[39m model\u001b[38;5;241m.\u001b[39mloss(out)\n\u001b[1;32m      5\u001b[0m     loss[\u001b[38;5;124m'\u001b[39m\u001b[38;5;124melbo_loss\u001b[39m\u001b[38;5;124m'\u001b[39m]\u001b[38;5;241m.\u001b[39mbackward()\n",
      "File \u001b[0;32m~/anaconda3/lib/python3.9/site-packages/torch/nn/modules/module.py:1501\u001b[0m, in \u001b[0;36mModule._call_impl\u001b[0;34m(self, *args, **kwargs)\u001b[0m\n\u001b[1;32m   1496\u001b[0m \u001b[38;5;66;03m# If we don't have any hooks, we want to skip the rest of the logic in\u001b[39;00m\n\u001b[1;32m   1497\u001b[0m \u001b[38;5;66;03m# this function, and just call forward.\u001b[39;00m\n\u001b[1;32m   1498\u001b[0m \u001b[38;5;28;01mif\u001b[39;00m \u001b[38;5;129;01mnot\u001b[39;00m (\u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_backward_hooks \u001b[38;5;129;01mor\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_backward_pre_hooks \u001b[38;5;129;01mor\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_forward_hooks \u001b[38;5;129;01mor\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_forward_pre_hooks\n\u001b[1;32m   1499\u001b[0m         \u001b[38;5;129;01mor\u001b[39;00m _global_backward_pre_hooks \u001b[38;5;129;01mor\u001b[39;00m _global_backward_hooks\n\u001b[1;32m   1500\u001b[0m         \u001b[38;5;129;01mor\u001b[39;00m _global_forward_hooks \u001b[38;5;129;01mor\u001b[39;00m _global_forward_pre_hooks):\n\u001b[0;32m-> 1501\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[43mforward_call\u001b[49m\u001b[43m(\u001b[49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[43margs\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[43mkwargs\u001b[49m\u001b[43m)\u001b[49m\n\u001b[1;32m   1502\u001b[0m \u001b[38;5;66;03m# Do not call functions when jit is used\u001b[39;00m\n\u001b[1;32m   1503\u001b[0m full_backward_hooks, non_full_backward_hooks \u001b[38;5;241m=\u001b[39m [], []\n",
      "File \u001b[0;32m~/workspace/event_seq/experiments/rosbank/notebooks/../../../src/models/mTAND/model.py:267\u001b[0m, in \u001b[0;36mMegaNet.forward\u001b[0;34m(self, padded_batch)\u001b[0m\n\u001b[1;32m    260\u001b[0m z \u001b[38;5;241m=\u001b[39m sample_z(qz_mean, qz_logstd, \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mmodel_conf\u001b[38;5;241m.\u001b[39mk_iwae)\n\u001b[1;32m    262\u001b[0m iwae_steps \u001b[38;5;241m=\u001b[39m (\n\u001b[1;32m    263\u001b[0m     time_steps[\u001b[38;5;28;01mNone\u001b[39;00m, :, :]\n\u001b[1;32m    264\u001b[0m     \u001b[38;5;241m.\u001b[39mrepeat(\u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mmodel_conf\u001b[38;5;241m.\u001b[39mk_iwae, \u001b[38;5;241m1\u001b[39m, \u001b[38;5;241m1\u001b[39m)\n\u001b[1;32m    265\u001b[0m     \u001b[38;5;241m.\u001b[39mview(\u001b[38;5;241m-\u001b[39m\u001b[38;5;241m1\u001b[39m, time_steps\u001b[38;5;241m.\u001b[39mshape[\u001b[38;5;241m1\u001b[39m])\n\u001b[1;32m    266\u001b[0m )\n\u001b[0;32m--> 267\u001b[0m dec_out \u001b[38;5;241m=\u001b[39m \u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mdecoder\u001b[49m\u001b[43m(\u001b[49m\u001b[43mz\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43miwae_steps\u001b[49m\u001b[43m)\u001b[49m\n\u001b[1;32m    268\u001b[0m dec_out \u001b[38;5;241m=\u001b[39m dec_out\u001b[38;5;241m.\u001b[39mview(\n\u001b[1;32m    269\u001b[0m     \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mmodel_conf\u001b[38;5;241m.\u001b[39mk_iwae,\n\u001b[1;32m    270\u001b[0m     time_steps\u001b[38;5;241m.\u001b[39mshape[\u001b[38;5;241m0\u001b[39m],\n\u001b[1;32m    271\u001b[0m     dec_out\u001b[38;5;241m.\u001b[39mshape[\u001b[38;5;241m1\u001b[39m],\n\u001b[1;32m    272\u001b[0m     dec_out\u001b[38;5;241m.\u001b[39mshape[\u001b[38;5;241m2\u001b[39m],\n\u001b[1;32m    273\u001b[0m )\n\u001b[1;32m    275\u001b[0m \u001b[38;5;28;01mreturn\u001b[39;00m {\n\u001b[1;32m    276\u001b[0m     \u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mx_recon\u001b[39m\u001b[38;5;124m\"\u001b[39m: dec_out,\n\u001b[1;32m    277\u001b[0m     \u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mz\u001b[39m\u001b[38;5;124m\"\u001b[39m: z,\n\u001b[0;32m   (...)\u001b[0m\n\u001b[1;32m    281\u001b[0m     \u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mlog_std\u001b[39m\u001b[38;5;124m\"\u001b[39m: qz_logstd,\n\u001b[1;32m    282\u001b[0m }\n",
      "File \u001b[0;32m~/anaconda3/lib/python3.9/site-packages/torch/nn/modules/module.py:1501\u001b[0m, in \u001b[0;36mModule._call_impl\u001b[0;34m(self, *args, **kwargs)\u001b[0m\n\u001b[1;32m   1496\u001b[0m \u001b[38;5;66;03m# If we don't have any hooks, we want to skip the rest of the logic in\u001b[39;00m\n\u001b[1;32m   1497\u001b[0m \u001b[38;5;66;03m# this function, and just call forward.\u001b[39;00m\n\u001b[1;32m   1498\u001b[0m \u001b[38;5;28;01mif\u001b[39;00m \u001b[38;5;129;01mnot\u001b[39;00m (\u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_backward_hooks \u001b[38;5;129;01mor\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_backward_pre_hooks \u001b[38;5;129;01mor\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_forward_hooks \u001b[38;5;129;01mor\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_forward_pre_hooks\n\u001b[1;32m   1499\u001b[0m         \u001b[38;5;129;01mor\u001b[39;00m _global_backward_pre_hooks \u001b[38;5;129;01mor\u001b[39;00m _global_backward_hooks\n\u001b[1;32m   1500\u001b[0m         \u001b[38;5;129;01mor\u001b[39;00m _global_forward_hooks \u001b[38;5;129;01mor\u001b[39;00m _global_forward_pre_hooks):\n\u001b[0;32m-> 1501\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[43mforward_call\u001b[49m\u001b[43m(\u001b[49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[43margs\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[43mkwargs\u001b[49m\u001b[43m)\u001b[49m\n\u001b[1;32m   1502\u001b[0m \u001b[38;5;66;03m# Do not call functions when jit is used\u001b[39;00m\n\u001b[1;32m   1503\u001b[0m full_backward_hooks, non_full_backward_hooks \u001b[38;5;241m=\u001b[39m [], []\n",
      "File \u001b[0;32m~/workspace/event_seq/experiments/rosbank/notebooks/../../../src/models/mTAND/model.py:231\u001b[0m, in \u001b[0;36mMegaDecoder.forward\u001b[0;34m(self, z, time_steps)\u001b[0m\n\u001b[1;32m    230\u001b[0m \u001b[38;5;28;01mdef\u001b[39;00m \u001b[38;5;21mforward\u001b[39m(\u001b[38;5;28mself\u001b[39m, z, time_steps):\n\u001b[0;32m--> 231\u001b[0m     out \u001b[38;5;241m=\u001b[39m \u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mdecoder\u001b[49m\u001b[43m(\u001b[49m\u001b[43mz\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mtime_steps\u001b[49m\u001b[43m)\u001b[49m\n\u001b[1;32m    233\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m out\n",
      "File \u001b[0;32m~/anaconda3/lib/python3.9/site-packages/torch/nn/modules/module.py:1501\u001b[0m, in \u001b[0;36mModule._call_impl\u001b[0;34m(self, *args, **kwargs)\u001b[0m\n\u001b[1;32m   1496\u001b[0m \u001b[38;5;66;03m# If we don't have any hooks, we want to skip the rest of the logic in\u001b[39;00m\n\u001b[1;32m   1497\u001b[0m \u001b[38;5;66;03m# this function, and just call forward.\u001b[39;00m\n\u001b[1;32m   1498\u001b[0m \u001b[38;5;28;01mif\u001b[39;00m \u001b[38;5;129;01mnot\u001b[39;00m (\u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_backward_hooks \u001b[38;5;129;01mor\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_backward_pre_hooks \u001b[38;5;129;01mor\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_forward_hooks \u001b[38;5;129;01mor\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_forward_pre_hooks\n\u001b[1;32m   1499\u001b[0m         \u001b[38;5;129;01mor\u001b[39;00m _global_backward_pre_hooks \u001b[38;5;129;01mor\u001b[39;00m _global_backward_hooks\n\u001b[1;32m   1500\u001b[0m         \u001b[38;5;129;01mor\u001b[39;00m _global_forward_hooks \u001b[38;5;129;01mor\u001b[39;00m _global_forward_pre_hooks):\n\u001b[0;32m-> 1501\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[43mforward_call\u001b[49m\u001b[43m(\u001b[49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[43margs\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[43mkwargs\u001b[49m\u001b[43m)\u001b[49m\n\u001b[1;32m   1502\u001b[0m \u001b[38;5;66;03m# Do not call functions when jit is used\u001b[39;00m\n\u001b[1;32m   1503\u001b[0m full_backward_hooks, non_full_backward_hooks \u001b[38;5;241m=\u001b[39m [], []\n",
      "File \u001b[0;32m~/workspace/event_seq/experiments/rosbank/notebooks/../../../src/models/mTAND/model.py:137\u001b[0m, in \u001b[0;36mDecMtanRnn.forward\u001b[0;34m(self, z, time_steps)\u001b[0m\n\u001b[1;32m    134\u001b[0m keys \u001b[38;5;241m=\u001b[39m torch\u001b[38;5;241m.\u001b[39mcat(keys, dim\u001b[38;5;241m=\u001b[39m\u001b[38;5;241m1\u001b[39m)\u001b[38;5;241m.\u001b[39mto(\u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mdevice)\n\u001b[1;32m    135\u001b[0m querys \u001b[38;5;241m=\u001b[39m torch\u001b[38;5;241m.\u001b[39mcat(querys, dim\u001b[38;5;241m=\u001b[39m\u001b[38;5;241m1\u001b[39m)\u001b[38;5;241m.\u001b[39mto(\u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mdevice)\n\u001b[0;32m--> 137\u001b[0m out \u001b[38;5;241m=\u001b[39m \u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43matt\u001b[49m\u001b[43m(\u001b[49m\u001b[43mquerys\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mkeys\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mout\u001b[49m\u001b[43m)\u001b[49m\n\u001b[1;32m    138\u001b[0m out \u001b[38;5;241m=\u001b[39m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mz0_to_obs(out)\n\u001b[1;32m    139\u001b[0m \u001b[38;5;28;01mreturn\u001b[39;00m out\n",
      "File \u001b[0;32m~/anaconda3/lib/python3.9/site-packages/torch/nn/modules/module.py:1501\u001b[0m, in \u001b[0;36mModule._call_impl\u001b[0;34m(self, *args, **kwargs)\u001b[0m\n\u001b[1;32m   1496\u001b[0m \u001b[38;5;66;03m# If we don't have any hooks, we want to skip the rest of the logic in\u001b[39;00m\n\u001b[1;32m   1497\u001b[0m \u001b[38;5;66;03m# this function, and just call forward.\u001b[39;00m\n\u001b[1;32m   1498\u001b[0m \u001b[38;5;28;01mif\u001b[39;00m \u001b[38;5;129;01mnot\u001b[39;00m (\u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_backward_hooks \u001b[38;5;129;01mor\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_backward_pre_hooks \u001b[38;5;129;01mor\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_forward_hooks \u001b[38;5;129;01mor\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_forward_pre_hooks\n\u001b[1;32m   1499\u001b[0m         \u001b[38;5;129;01mor\u001b[39;00m _global_backward_pre_hooks \u001b[38;5;129;01mor\u001b[39;00m _global_backward_hooks\n\u001b[1;32m   1500\u001b[0m         \u001b[38;5;129;01mor\u001b[39;00m _global_forward_hooks \u001b[38;5;129;01mor\u001b[39;00m _global_forward_pre_hooks):\n\u001b[0;32m-> 1501\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[43mforward_call\u001b[49m\u001b[43m(\u001b[49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[43margs\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[43mkwargs\u001b[49m\u001b[43m)\u001b[49m\n\u001b[1;32m   1502\u001b[0m \u001b[38;5;66;03m# Do not call functions when jit is used\u001b[39;00m\n\u001b[1;32m   1503\u001b[0m full_backward_hooks, non_full_backward_hooks \u001b[38;5;241m=\u001b[39m [], []\n",
      "File \u001b[0;32m~/workspace/event_seq/experiments/rosbank/notebooks/../../../src/models/mTAND/model_utils.py:114\u001b[0m, in \u001b[0;36mMultiTimeAttention.forward\u001b[0;34m(self, query, key, value, mask, dropout)\u001b[0m\n\u001b[1;32m    105\u001b[0m value \u001b[38;5;241m=\u001b[39m value\u001b[38;5;241m.\u001b[39munsqueeze(\u001b[38;5;241m1\u001b[39m)\n\u001b[1;32m    107\u001b[0m query, key \u001b[38;5;241m=\u001b[39m [\n\u001b[1;32m    108\u001b[0m     \u001b[38;5;66;03m# divided to  bs, num_time_emb, num_heads, L, dim_head after transpose\u001b[39;00m\n\u001b[1;32m    109\u001b[0m     linear(x)\n\u001b[0;32m   (...)\u001b[0m\n\u001b[1;32m    112\u001b[0m     \u001b[38;5;28;01mfor\u001b[39;00m linear, x \u001b[38;5;129;01min\u001b[39;00m (\u001b[38;5;28mzip\u001b[39m(\u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mlinears, (query, key)))\n\u001b[1;32m    113\u001b[0m ]\n\u001b[0;32m--> 114\u001b[0m x, _ \u001b[38;5;241m=\u001b[39m \u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mattention\u001b[49m\u001b[43m(\u001b[49m\u001b[43mquery\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mkey\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mvalue\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mmask\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mdropout\u001b[49m\u001b[43m)\u001b[49m\n\u001b[1;32m    115\u001b[0m \u001b[38;5;66;03m# dimensions are bs, num_time_emb, num_ref_points, num_head * input_dim\u001b[39;00m\n\u001b[1;32m    116\u001b[0m \u001b[38;5;66;03m# transpose applied to flatten last 2 dimensions\u001b[39;00m\n\u001b[1;32m    117\u001b[0m x \u001b[38;5;241m=\u001b[39m x\u001b[38;5;241m.\u001b[39mtranspose(\u001b[38;5;241m2\u001b[39m, \u001b[38;5;241m3\u001b[39m)\u001b[38;5;241m.\u001b[39mcontiguous()\u001b[38;5;241m.\u001b[39mview(batch, \u001b[38;5;241m-\u001b[39m\u001b[38;5;241m1\u001b[39m, num_ref_points, \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mh \u001b[38;5;241m*\u001b[39m dim)\n",
      "File \u001b[0;32m~/workspace/event_seq/experiments/rosbank/notebooks/../../../src/models/mTAND/model_utils.py:85\u001b[0m, in \u001b[0;36mMultiTimeAttention.attention\u001b[0;34m(self, query, key, value, mask, dropout)\u001b[0m\n\u001b[1;32m     83\u001b[0m scores \u001b[38;5;241m=\u001b[39m torch\u001b[38;5;241m.\u001b[39mmatmul(query, key\u001b[38;5;241m.\u001b[39mtranspose(\u001b[38;5;241m-\u001b[39m\u001b[38;5;241m2\u001b[39m, \u001b[38;5;241m-\u001b[39m\u001b[38;5;241m1\u001b[39m)) \u001b[38;5;241m/\u001b[39m math\u001b[38;5;241m.\u001b[39msqrt(d_k)\n\u001b[1;32m     84\u001b[0m \u001b[38;5;66;03m# repeated to match dimension of input values\u001b[39;00m\n\u001b[0;32m---> 85\u001b[0m scores \u001b[38;5;241m=\u001b[39m \u001b[43mscores\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43munsqueeze\u001b[49m\u001b[43m(\u001b[49m\u001b[38;5;241;43m-\u001b[39;49m\u001b[38;5;241;43m1\u001b[39;49m\u001b[43m)\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mrepeat_interleave\u001b[49m\u001b[43m(\u001b[49m\u001b[43mdim\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mdim\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[38;5;241;43m-\u001b[39;49m\u001b[38;5;241;43m1\u001b[39;49m\u001b[43m)\u001b[49m\n\u001b[1;32m     86\u001b[0m \u001b[38;5;28;01mif\u001b[39;00m mask \u001b[38;5;129;01mis\u001b[39;00m \u001b[38;5;129;01mnot\u001b[39;00m \u001b[38;5;28;01mNone\u001b[39;00m:\n\u001b[1;32m     87\u001b[0m     scores \u001b[38;5;241m=\u001b[39m scores\u001b[38;5;241m.\u001b[39mmasked_fill(mask\u001b[38;5;241m.\u001b[39munsqueeze(\u001b[38;5;241m-\u001b[39m\u001b[38;5;241m3\u001b[39m) \u001b[38;5;241m==\u001b[39m \u001b[38;5;241m0\u001b[39m, \u001b[38;5;241m-\u001b[39m\u001b[38;5;241m1e9\u001b[39m)\n",
      "\u001b[0;31mKeyboardInterrupt\u001b[0m: "
     ]
    }
   ],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "hw1",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.10.12"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
